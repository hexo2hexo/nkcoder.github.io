{"meta":{"title":"Daniel's Blog","subtitle":"Salvation lies within！","description":"个人博客，分享技术经验和总结","author":"Daniel","url":"http://nkcoder.github.io"},"pages":[{"title":"","date":"2017-09-17T03:02:20.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"404.html","permalink":"http://nkcoder.github.io/404.html","excerpt":"","text":""},{"title":"","date":"2017-09-25T14:31:25.000Z","updated":"2017-09-25T14:30:54.000Z","comments":true,"path":"googlee225e753c07ab44e.html","permalink":"http://nkcoder.github.io/googlee225e753c07ab44e.html","excerpt":"","text":"google-site-verification: googlee225e753c07ab44e.html"},{"title":"About-Me","date":"2015-11-17T09:40:07.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"about-me/index.html","permalink":"http://nkcoder.github.io/about-me/index.html","excerpt":"","text":"我是一个程序员, 出身后端， 但是对前端、客户端也甚感兴趣，在学习的路上。 我是一个程序员， 不善言辞，不怎么幽默，更不浪漫， 有女朋友，所以也并不孤单. 我是一个程序员， 目前在创业的路上，坐标成都， 寻志同道合的你为友! Email: nkcoder at icloud.com 南开大学，计算机"},{"title":"分类","date":"2016-01-24T13:53:18.000Z","updated":"2017-09-17T03:02:20.000Z","comments":false,"path":"categories/index.html","permalink":"http://nkcoder.github.io/categories/index.html","excerpt":"","text":""},{"title":"读书使人明智","date":"2016-01-16T13:18:19.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"reading/index.html","permalink":"http://nkcoder.github.io/reading/index.html","excerpt":"","text":"2016 正在读的书 Java SE 8 for the Really Impatient Dive Into Python 3 2016 读过的书 Producter - 让产品从 0 到 1"},{"title":"TagCloud","date":"2016-01-24T13:51:57.000Z","updated":"2017-09-17T03:02:20.000Z","comments":false,"path":"tags/index.html","permalink":"http://nkcoder.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Junit入门指南","slug":"junit-tutorial-1","date":"2017-09-17T02:43:09.000Z","updated":"2017-09-26T00:03:33.000Z","comments":true,"path":"2017/09/17/junit-tutorial-1/","link":"","permalink":"http://nkcoder.github.io/2017/09/17/junit-tutorial-1/","excerpt":"Junit版本：4.12 本文简单介绍Junit在实际工作中的常见的一些用法和API。 添加gradle依赖： 1234testCompile(\"junit:junit:4.12\") &#123; exclude group: 'org.hamcrest'&#125;testCompile \"org.hamcrest:hamcrest-library:1.3\" 1. 基本注解12345@BeforeClass: 在class中的所有测试方法执行之前运行一次@AfterClass：在class中的所有测试方法执行之后运行一次@Before：在每一个测试方法执行之前运行一次@After：在每一个测试方法执行之后运行一次@Test：表示一个测试方法","text":"Junit版本：4.12 本文简单介绍Junit在实际工作中的常见的一些用法和API。 添加gradle依赖： 1234testCompile(\"junit:junit:4.12\") &#123; exclude group: 'org.hamcrest'&#125;testCompile \"org.hamcrest:hamcrest-library:1.3\" 1. 基本注解12345@BeforeClass: 在class中的所有测试方法执行之前运行一次@AfterClass：在class中的所有测试方法执行之后运行一次@Before：在每一个测试方法执行之前运行一次@After：在每一个测试方法执行之后运行一次@Test：表示一个测试方法 其中@BeforeClass和@AfterClass修饰的方法签名为public static void，@Before、@After和@Test修饰的方法的签名为public void。 1234567891011121314151617181920212223242526272829@BeforeClasspublic static void run_before_class() &#123; log.info(\"run once before class.\");&#125;@AfterClasspublic static void run_after_class() &#123; log.info(\"run once after class.\");&#125;@Beforepublic void run_before_test_method() &#123; log.info(\"run before every test method.\");&#125;@Afterpublic void run_after_test_method() &#123; log.info(\"run before every test method.\");&#125;@Testpublic void test_method1() &#123; log.info(\"test method 1\");&#125;@Testpublic void test_method2() &#123; log.info(\"test method 2\");&#125; 2. 测试异常测试异常主要有三种方式： 通过@Test的expect属性 通过try...catch...搭配fail()方法使用，使用fail()的原因是，如果测试的方法没有抛出指定的异常，则该单元测试就会通过 通过@Rule注解 12345678910111213141516171819202122232425@Test(expected = IndexOutOfBoundsException.class)public void test_exception_with_expect_attribute() &#123; new ArrayList&lt;&gt;().get(0);&#125;@Testpublic void test_exception_with_try_catch_fail() &#123; try &#123; new ArrayList&lt;&gt;().get(0); fail(); &#125; catch (IndexOutOfBoundsException e) &#123; assertThat(e.getMessage(), is(\"Index: 0, Size: 0\")); &#125;&#125;@Rulepublic ExpectedException expectedException = ExpectedException.none();@Testpublic void test_exception_with_rule() &#123; expectedException.expect(IndexOutOfBoundsException.class); expectedException.expectMessage(is(\"Index: 0, Size: 0\")); expectedException.expectMessage(containsString(\"Index: 0, Size: 0\")); new ArrayList&lt;&gt;().get(0);&#125; 3. 忽略测试通过@Ignore可以忽略单元测试，如果用在方法上，表示该方法不作为单元测试被执行，如果用在类上，表示该类中的所有单元测试方法都不被执行。 为什么要用@Ignore忽略单元测试，而不是直接注释掉单元测试或者注释掉@Test注解呢？因为被@Ignore的单元测试会显示在最后的测试结果中，另外，在多人协作的多模块项目中，ignore掉别的模块中执行不通过的单元测试，可以避免整个项目都无法运行。 12345678@Ignore(\"will be add later!\")public class JunitException &#123; @Test(expected = IndexOutOfBoundsException.class) @Ignore public void test_exception_with_expect_attribute() &#123; new ArrayList&lt;&gt;().get(0); &#125;&#125; 4. 设置超时@Test的tiimeout属性可以设置超时，单位是毫秒。 12345678@Test(timeout = 1000)public void test_timeout() &#123; try &#123; TimeUnit.SECONDS.sleep(2000); &#125; catch (InterruptedException e) &#123; log.error(e.getMessage(), e); &#125;&#125; 5. 测试list的常用方法12345678910111213List&lt;String&gt; actual = Arrays.asList(\"a\", \"b\", \"c\");List&lt;String&gt; expected = Arrays.asList(\"a\", \"b\", \"c\");List&lt;Integer&gt; numList = Arrays.asList®(1, 2, 3);assertThat(actual, is(expected));assertThat(actual, hasItem(\"a\"));assertThat(actual, hasItems(\"c\", \"b\"));assertThat(actual, containsInAnyOrder(\"c\", \"b\", \"a\"));assertThat(actual.size(), is(3));assertThat(actual, hasSize(3));assertThat(numList, everyItem(greaterThanOrEqualTo(1))); 6. 测试map的常用方法123456789101112131415Map&lt;String, Integer&gt; actual = new HashMap&lt;&gt;();actual.put(\"00001\", 1);actual.put(\"00002\", 2);actual.put(\"00003\", 3);Map&lt;String, Integer&gt; expected = new HashMap&lt;&gt;();expected.put(\"00001\", 1);expected.put(\"00002\", 2);expected.put(\"00003\", 3);assertThat(actual, is(expected));assertThat(actual, hasEntry(\"00001\", 1));assertThat(actual, not(hasEntry(\"00004\", 4)));assertThat(actual, hasKey(\"00002\"));assertThat(actual, hasValue(\"00003\")); 参考 junit4 JUnit Tutorial","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"单元测试","slug":"单元测试","permalink":"http://nkcoder.github.io/tags/单元测试/"},{"name":"Junit","slug":"Junit","permalink":"http://nkcoder.github.io/tags/Junit/"}]},{"title":"Express框架入门：初级篇","slug":"express-tutorial-basic","date":"2016-08-19T22:59:43.000Z","updated":"2017-09-26T00:07:01.000Z","comments":true,"path":"2016/08/20/express-tutorial-basic/","link":"","permalink":"http://nkcoder.github.io/2016/08/20/express-tutorial-basic/","excerpt":"express是nodejs的一个流行的web框架。本文主要介绍将express作为服务端对外提供API接口时，需要了解的入门知识。 1. Hello World首先安装node（如果已安装，则略过）： 1$ brew install node 创建一个项目，然后安装express： 123$ mkdir express-hello-world &amp;&amp; cd express-hello-world$ npm init$ npm install express --save npm init会提示输入一些配置信息，回车使用默认值即可，执行完后，当前目录下会自动创建package.json文件。 npm install express --save表示为当前项目安装express依赖，该依赖信息会保存在package.json文件中。","text":"express是nodejs的一个流行的web框架。本文主要介绍将express作为服务端对外提供API接口时，需要了解的入门知识。 1. Hello World首先安装node（如果已安装，则略过）： 1$ brew install node 创建一个项目，然后安装express： 123$ mkdir express-hello-world &amp;&amp; cd express-hello-world$ npm init$ npm install express --save npm init会提示输入一些配置信息，回车使用默认值即可，执行完后，当前目录下会自动创建package.json文件。 npm install express --save表示为当前项目安装express依赖，该依赖信息会保存在package.json文件中。 新建文件index.js文件， 输入以下内容： 12345678var express = require('express');var app = express();app.get('/', function(req, res) &#123; res.send('hello, world!');&#125;);app.listen(3000); 运行： 1$ node index.js 浏览器访问：http://localhost:3000/，会输出”hello, world!”。 2. 中间件middleware在express中，中间件（middleware）函数是一种特殊的函数，它可以访问一个http请求周期中的request对象、response对象，以及表示调用栈中的下一个中间件函数的引用，如： 123function (req, res, next) &#123; next();&#125; 其中，req, res和next三个参数名是约定的，不要使用其它的变量名。中间件函数可以修改request和response，或者提前结束response，也可以调用next()表示将执行传递给调用栈中的下一个中间件函数。 如果当前中间件函数没有结束HTTP请求，则必须调用next()将执行传递给下一个中间件函数，否则请求会挂起。 使用app.use()加载中间件函数，中间件函数加载的顺序决定了它的执行顺序，即先加载，先执行。 在上面hello-world的例子中，我们增加两个简单的中间件函数，分别打印两条日志信息。在var app = express();的后面增加以下代码： 123456789app.use(function (req, res, next) &#123; console.log('in middleware one...'); next();&#125;);app.use(function (req, res, next) &#123; console.log('in middleware two...'); next();&#125;); 执行后，终端会依次输出两个中间件函数中的日志信息。 express中的中间件可以分为以下几类： app级中间件 router级中间件 错误处理中间件 内置中间件 第三方中间件 这里仅简要介绍一下主要的app级中间件和router级中间件。 app级中间件，即将中间件函数绑定到app对象（即使用express()得到的对象），通过app.use()或者app.METHOD()方法来加载中间件函数，其中METHOD()表示HTTP请求中的GET/POST/PUT等方法。上面的hello-world示例中就是app级中间件： 123app.get('/', function(req, res) &#123; res.send('hello, world!');&#125;); router级中间件与app级中间件的用法基本一致，不同的是，它将中间件函数绑定到express.Router()对象，通过router.use()或者router.METHOD()方法来加载。比如上面的app级中间件，用router级中间件的方法改写如下： 123456var router = express.Router();router.get('/', function(req, res) &#123; res.send('hello, world!');&#125;);app.use('/', router); 引入router级中间件的好处之一是解耦，通过router去分层，然后app加载router。 3. HTTP的req对象3.1 req.params取路径参数express中，路径参数使用命名参数的方式，比如路径是/user/:id，则使用req.params.id取参数:id的值，如： /user/:id GET /user/15 req.params.id =&gt; 15 3.2 req.query取查询参数取查询参数，只需要通过req.query根据key取值即可，如： GET /search?name=Ketty&amp;gender=male req.query.name =&gt; Ketty req.query.gender =&gt; male GET /search?user[name]=Ketty&amp;user[age]=30 req.query.user.name =&gt; Ketty req.query.user.age =&gt; 30 3.3 req.body要取HTTP请求中的body内容，使用req.body，但是需要借助第三方module，如body-parser和multer，以下示例来自express文档： 123456789101112var app = require('express')();var bodyParser = require('body-parser');var multer = require('multer'); // v1.0.5var upload = multer(); // for parsing multipart/form-dataapp.use(bodyParser.json()); // for parsing application/jsonapp.use(bodyParser.urlencoded(&#123; extended: true &#125;)); // for parsing application/x-www-form-urlencodedapp.post('/profile', upload.array(), function (req, res, next) &#123; console.log(req.body); res.json(req.body);&#125;); 3.4 req.get()提取HTTP header中的信息，其中，key是不区分大小写的，如： 12req.get('Content-Type'); // text/htmlreq.get('content-type'); // text/html 4. HTTP的res对象4.1 res.status()该方法仅仅是设置状态码，返回response还需调用send()/end()等方法，如： 12res.status(200).end();res.status(401).send(\"error: unauthorized!\"); 4.2 res.json()返回json格式的信息，res会自动设置response的Content-Type为application/json，如： 1res.status(401).json(&#123;\"error\": \"unauthorized\"&#125;); 4.3 res.send()发送HTTP响应信息，参数可以是字符串、数组、Buffer对象等，会根据参数的类型自动设置header的Content-Type，如： 123res.send(new Buffer(\"buffer info\")); // Content-Type: application/octet-streamres.send(\"&lt;small&gt;small text&lt;/small&gt;\"); // Content-Type: text/htmlres.send(&#123;message: \"Welcome\"&#125;); // Content-Type: application/json 4.4. res.set()设置HTTP的header信息，如： 12res.set('Content-Type', 'application/pdf');res.setHeader('Content-Disposition', 'attachment; filename=cnb.pdf'); 4.5 res.render()使用模板引擎渲染页面，然后作为response返回。如果参数表示的文件名不带后缀，则会根据模板引擎的设置，自动推断后缀；如果文件名带后缀，则会加载该后缀对应的模板引擎模块。如 1res.render('index'); 如果默认的模板引擎是jade，则express会从对应的路径下查找index.jade文件并渲染。 参考 4.x API","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://nkcoder.github.io/tags/nodejs/"},{"name":"express","slug":"express","permalink":"http://nkcoder.github.io/tags/express/"}]},{"title":"Sed命令用法简介","slug":"linux-sed-note","date":"2016-08-06T06:23:42.000Z","updated":"2017-09-26T00:16:19.000Z","comments":true,"path":"2016/08/06/linux-sed-note/","link":"","permalink":"http://nkcoder.github.io/2016/08/06/linux-sed-note/","excerpt":"sed是一个面向行的流处理工具，主要用于对文件的增加、删除、替换等操作。 sed命令的语法： 1sed [options] commands [file-to-edit] 下面以annoying.txt文件作为示例数据源， 文件内容为： 123456root@a01:~/junk# echo \"this is the song that never ends&gt; yes, it goes on and on, my friend&gt; some people started singing it&gt; not knowing what it was&gt; and they'll continue singing it forever&gt; just because...\" &gt; annoying.txt","text":"sed是一个面向行的流处理工具，主要用于对文件的增加、删除、替换等操作。 sed命令的语法： 1sed [options] commands [file-to-edit] 下面以annoying.txt文件作为示例数据源， 文件内容为： 123456root@a01:~/junk# echo \"this is the song that never ends&gt; yes, it goes on and on, my friend&gt; some people started singing it&gt; not knowing what it was&gt; and they'll continue singing it forever&gt; just because...\" &gt; annoying.txt 1. 命令及参数含义1.1 读取文件内容如果commands为空，表示对每一行没有做任何处理，相当于读取文件的内容（因为sed默认会将读到的内容打印出来），如： 1234567root@a01:~/junk# sed '' annoying.txtthis is the song that never endsyes, it goes on and on, my friendsome people started singing itnot knowing what it wasand they'll continue singing it foreverjust because... 这与执行：# cat annoying.txt | sed &#39;&#39;的输出是一样的。 1.2 打印命令pp命令将读到的内容直接打印出来， 如： 12345678910111213root@a01:~/junk# sed 'p' annoying.txtthis is the song that never endsthis is the song that never endsyes, it goes on and on, my friendyes, it goes on and on, my friendsome people started singing itsome people started singing itnot knowing what it wasnot knowing what it wasand they'll continue singing it foreverand they'll continue singing it foreverjust because...just because... 上面每一行都会打印两遍，是因为sed默认会将读到的内容输出，使用-n选项可以禁用默认的输出，如： 1234567root@a01:~/junk# sed -n 'p' annoying.txtthis is the song that never endsyes, it goes on and on, my friendsome people started singing itnot knowing what it wasand they'll continue singing it foreverjust because... 范围限制：可以在p命令前增加范围限制，数字表示具体的行数，+数字表示增量，$表示最后一行，~表示间隔的行数，如： 打印第1行： 12root@a01:~/junk# sed -n '1p' annoying.txtthis is the song that never ends 打印第1行到第2行： 123root@a01:~/junk# sed -n '1,2p' annoying.txtthis is the song that never endsyes, it goes on and on, my friend 打印第3行到最后一行： 12345root@a01:~/junk# sed -n '3,$p' annoying.txtsome people started singing itnot knowing what it wasand they'll continue singing it foreverjust because... 打印第1行到后面的2行，即前3行: 1234root@a01:~/junk# sed -n '1,+2p' annoying.txtthis is the song that never endsyes, it goes on and on, my friendsome people started singing it 每隔2行打印，即打印奇数行： 1234root@a01:~/junk# sed -n '1~2p' annoying.txtthis is the song that never endssome people started singing itand they'll continue singing it forever 仅打印pattern匹配到的行： 12root@a01:~/junk# sed -n '/this/p' annoying.txtthis is the song that never ends 1.3 删除命令d删除命令d与打印命令p的用法基本类似，只需要将p换成d即可，如： 123456789root@a01:~/junk# sed '1~2d' annoying.txtyes, it goes on and on, my friendnot knowing what it wasjust because...root@a01:~/junk# sed '1,3d' annoying.txtnot knowing what it wasand they'll continue singing it foreverjust because... 使用删除命令，没有被删的行会被打印出来，但是原文件是不被影响的。可以使用-i选项，表示在原文件上直接修改，但是这样是比较危险的，最好先备份，而-i选项支持备份，-i后面的参数值表示备份文件的后缀，如： 12345678910111213141516171819root@a01:~/junk# sed -i '1d' annoying.txtroot@a01:~/junk# sed -i.bak '1d' annoying.txtroot@a01:~/junk# lsannoying.txt annoying.txt.bakroot@a01:~/junk# cat annoying.txtsome people started singing itnot knowing what it wasand they'll continue singing it foreverjust because...root@a01:~/junk# cat annoying.txt.bakthis is the song that never endsyes, it goes on and on, my friendsome people started singing itnot knowing what it wasand they'll continue singing it foreverjust because... 1.4 替换命令s1's/old_word/new_word/' s是替换命令，/是默认的分隔符，也可以使用其它字符作为分隔符（紧跟s之后），常见的有：|, :, _等，注意末尾的分隔符不能省略。old_word为正则表达式，用于匹配，new_word是要替换的字符串。如： 12root@a01:~/junk# echo http://www.thegeekstuff.com/2009/09/unix-sed | sed 's_2009/09_2016/07_'http://www.thegeekstuff.com/2016/07/unix-sed s命令默认仅替换每一行出现的第1个匹配，如： 1234567root@a01:~/junk# sed 's/on/forward/' annoying.txtthis is the sforwardg that never endsyes, it goes forward and on, my friendsome people started singing itnot knowing what it wasand they'll cforwardtinue singing it foreverjust because... 也可以仅替换每行的第n个匹配，如： 1234567root@a01:~/junk# sed 's/on/forward/2' annoying.txtthis is the song that never endsyes, it goes on and forward, my friendsome people started singing itnot knowing what it wasand they'll continue singing it foreverjust because... 全局替换使用g参数： 1234567root@a01:~/junk# sed 's/on/forward/g' annoying.txtthis is the sforwardg that never endsyes, it goes forward and forward, my friendsome people started singing itnot knowing what it wasand they'll cforwardtinue singing it foreverjust because... 如果仅替换独立的单词，不替换单词的部分，使用\\b限制边界，如： 1234567root@a01:~/junk# sed 's/\\bon\\b/forward/g' annoying.txtthis is the song that never endsyes, it goes forward and forward, my friendsome people started singing itnot knowing what it wasand they'll continue singing it foreverjust because... 限制操作的行数，如仅全局替换前3行（&amp;表示引用匹配到的部分）： 1234567root@a01:~/junk# sed &apos;1,2s/^.*/ &amp;/&apos; annoying.txt this is the song that never ends yes, it goes on and on, my friendsome people started singing itnot knowing what it wasand they&apos;ll continue singing it foreverjust because... 可以使用-n选项和p参数，打印仅发生替换的内容，如： 1234root@a01:~/junk# sed -n 's/on/forward/gp' annoying.txtthis is the sforwardg that never endsyes, it goes forward and forward, my friendand they'll cforwardtinue singing it forever 可见，在s命令的最后，可以通过参数影响替换的行为，如i表示忽略大小写： 1234root@a01:~/junk# sed -n 's/on/forward/gpi' annoying.txtthis is the sforwardg that never endsyes, it goes forward and forward, my friendand they'll cforwardtinue singing it forever 引用被匹配的部分：如果只有一个分组，则使用&amp;比较方便，表示引用被匹配的部分，如： 1234567root@a01:~/junk# sed 's/.*/| &amp;/' annoying.txt| this is the song that never ends| yes, it goes on and on, my friend| some people started singing it| not knowing what it was| and they'll continue singing it forever| just because... 如果有多个分组，首先在old_word里通过括号()分组，然后在new_word里通过数字序号去引用（分组的括号以及引用的数字都需要通过\\转义）,数字和前面的分组一一对应，最多可以使用9个数字。比如我们要反转每一行的前两个单词： 1234567root@a01:~/junk# sed 's/\\([0-9a-zA-Z][0-9a-zA-Z]*\\) \\([0-9a-zA-Z][0-9a-zA-Z]*\\)/\\2 \\1/' annoying.txtis this the song that never endsyes, goes it on and on, my friendpeople some started singing itknowing not what it wasthey and'll continue singing it foreverbecause just... \\1可以用于用在new_word，也可以用在old_word中，如打印具有连续重复词的行： 12root@a01:~/junk# sed -n '/\\([a-z][a-z]*\\) \\1/p' annoying.txtthis is the song that never ends 1.5 行之前插入命令i在文件中插入内容也是sed常见用法之一。i命令表示在匹配的行前插入内容，插入的内容作为一行显示，如： 123456789root@a01:~/junk# sed '/singing/i before every line' annoying.txtthis is the song that never endsyes, it goes on and on, my friendbefore every linesome people started singing itnot knowing what it wasbefore every lineand they'll continue singing it foreverjust because... 1.6 行之后插入命令a表示在匹配的行之后插入内容： 1234567891011root@a01:~/junk# sed '/it/a after it' annoying.txtthis is the song that never endsyes, it goes on and on, my friendafter itsome people started singing itafter itnot knowing what it wasafter itand they'll continue singing it foreverafter itjust because... 1.7 行修改（替换）命令c1234567root@a01:~/junk# sed '/it/c change it' annoying.txtthis is the song that never endschange itchange itchange itchange itjust because... 1.8 执行多条处理命令：-e选项可以通过管道|，如： 1234567root@a01:~/junk# sed 's/and/\\&amp;/' annoying.txt | sed 's/people/horses/'this is the song that never endsyes, it goes on &amp; on, my friendsome horses started singing itnot knowing what it was&amp; they'll continue singing it foreverjust because... 但是因为需要多次调用sed命令，因此效率不好。推荐使用sed的-e选项来执行多条命令（只有一条命令时，-e选项不是必须的），如： 1234567root@a01:~/junk# sed -e 's/and/\\&amp;/' -e 's/people/horses/' annoying.txtthis is the song that never endsyes, it goes on &amp; on, my friendsome horses started singing itnot knowing what it was&amp; they'll continue singing it foreverjust because... 还有一种方式，通过分号(;)将命令分割，如： 1234567root@a01:~/junk# sed 's/and/\\&amp;/;s/people/horses/' annoying.txtthis is the song that never endsyes, it goes on &amp; on, my friendsome horses started singing itnot knowing what it was&amp; they'll continue singing it foreverjust because... 1.9 sed脚本文件：-f选项可以将sed命令放到文件里，然后执行这个脚本文件，语法为： 1$ sed -f script-file file-to-edit 比如： 12345678910root@a01:~/junk# echo 's/and/\\&amp;/&gt; s/people/horses/' &gt;&gt; sed-demoroot@a01:~/junk# sed -f sed-demo annoying.txtthis is the song that never endsyes, it goes on &amp; on, my friendsome horses started singing itnot knowing what it was&amp; they'll continue singing it foreverjust because... 1.10 pattern命令前面都可以通过pattern去匹配，pattern为正则表达式。 比如仅处理包含singing这个词的行： 1234567root@a01:~/junk# sed '/singing/s/it/&amp; loudly/' annoying.txtthis is the song that never endsyes, it goes on and on, my friendsome people started singing it loudlynot knowing what it wasand they'll continue singing it loudly foreverjust because... pattern默认的分隔符也是/，如果第一个字符为\\，则使用后面的字符作为分隔符，一般用于匹配本身带有/的值，如： 12root@a01:~/junk# echo http://www.grymoire.com/Unix/sed.html | sed '\\_/Unix/sed_s_/sed_/awk_'http://www.grymoire.com/Unix/awk.html pattern除了直接匹配，还支持范围匹配，语法为： sed &apos;/start/,/stop/ s/#.*//&apos; start表示匹配开始，stop表示匹配结束，可以看作开关；如果是正则表达式，需要使用//，如果只是数字或者^$，则不需要//。 删除从some开头到not开头的中间所有行： 12345root@a01:~/junk# sed '/^some/,/^not/d' annoying.txtthis is the song that never endsyes, it goes on and on, my friendand they'll continue singing it foreverjust because... 删除第1行到包含yes的那一行： 12345root@a01:~/junk# sed '1,/yes/d' annoying.txtsome people started singing itnot knowing what it wasand they'll continue singing it foreverjust because... 删除包含yes那行到最后一行（命令d前面允许有空格的）： 12root@a01:~/junk# sed '/yes/,$ d' annoying.txtthis is the song that never ends 命令前面使用!表示对命令取反，比如对上一条命令取反： 123root@a01:~/junk# sed '/^some/,/^not/!d' annoying.txtsome people started singing itnot knowing what it was 2. 应用示例2.1 保留每一行的第一个单词12345678root@a01:~/junk# sed 's/\\([a-zA-Z]*\\).*/\\1/' &lt; annoying.txt &gt; first_word.txtroot@a01:~/junk# cat first_word.txtthisyessomenotandjust 2.2 Here is输入1234567891011root@a01:~/junk# cat sed-here-is.sh#!/bin/shecho -n \"what is the value?\\n\"read valuesed \"s/XYZ/$value/\" &lt;&lt;EOFThe value is XYZEOFroot@a01:~/junk# sh sed-here-is.shwhat is the value?testThe value is test 参考 Sed - An Introduction and Tutorial by Bruce Barnett The Basics of Using the Sed Stream Editor to Manipulate Text in Linux Intermediate Sed: Manipulating Streams of Text in a Linux Environment","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://nkcoder.github.io/tags/linux/"},{"name":"sed","slug":"sed","permalink":"http://nkcoder.github.io/tags/sed/"}]},{"title":"使用LiquiBase管理数据库的迁移","slug":"liquibase-in-maven-and-gradle","date":"2016-04-10T08:06:32.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2016/04/10/liquibase-in-maven-and-gradle/","link":"","permalink":"http://nkcoder.github.io/2016/04/10/liquibase-in-maven-and-gradle/","excerpt":"本文链接为：http://nkcoder.github.io/2016/04/10/liquibase-in-maven-and-gradle/ ，转载请注明出处，谢谢！ LiquiBase是一个用于数据库重构和迁移的开源工具，通过日志文件的形式记录数据库的变更，然后执行日志文件中的修改，将数据库更新或回滚到一致的状态。LiquiBase的主要特点有： 支持几乎所有主流的数据库，如MySQL, PostgreSQL, Oracle, Sql Server, DB2等； 支持多开发者的协作维护； 日志文件支持多种格式，如XML, YAML, JSON, SQL等； 支持多种运行方式，如命令行、Spring集成、Maven插件、Gradle插件等；","text":"本文链接为：http://nkcoder.github.io/2016/04/10/liquibase-in-maven-and-gradle/ ，转载请注明出处，谢谢！ LiquiBase是一个用于数据库重构和迁移的开源工具，通过日志文件的形式记录数据库的变更，然后执行日志文件中的修改，将数据库更新或回滚到一致的状态。LiquiBase的主要特点有： 支持几乎所有主流的数据库，如MySQL, PostgreSQL, Oracle, Sql Server, DB2等； 支持多开发者的协作维护； 日志文件支持多种格式，如XML, YAML, JSON, SQL等； 支持多种运行方式，如命令行、Spring集成、Maven插件、Gradle插件等； 本文首先简单介绍一下LiquiBase的changelog文件的常用标签配置，然后介绍在Maven和Gradle中集成并运行LiquiBase。 1. changelog文件格式changelog是LiquiBase用来记录数据库的变更，一般放在CLASSPATH下，然后配置到执行路径中。 changelog支持多种格式，主要有XML/JSON/YAML/SQL，其中XML/JSON/YAML除了具体格式语法不同，节点配置很类似，SQL格式中主要记录SQL语句，这里仅给出XML格式和SQL格式的示例，更多的格式示例请参考文档 changelog.xml &lt;changeSet id=&quot;2&quot; author=&quot;daniel&quot; runOnChange=&quot;true&quot;&gt; &lt;insert tableName=&quot;contest_info&quot;&gt; &lt;column name=&quot;id&quot;&gt;3&lt;/column&gt; &lt;column name=&quot;title&quot;&gt;title 3&lt;/column&gt; &lt;column name=&quot;content&quot;&gt;content 3&lt;/column&gt; &lt;/insert&gt; &lt;/changeSet&gt; changelog.sql --liquibase formatted sql --changeset daniel:16040707 CREATE TABLE `role_authority_sum` ( `row_id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT &apos;自增id&apos;, `role_id` int(11) unsigned NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;关联role的role_id&apos;, `authority_sum` int(11) unsigned NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;perms的值的和&apos;, `data_type_id` int(11) unsigned NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;关联data_type的id&apos;, PRIMARY KEY (`row_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&apos;角色的权限值的和，如角色有RD权限，则和为2+8=10&apos;; 2. 常用的标签及命令2.1 标签一个&lt;changeSet&gt;标签对应一个变更集，由属性id、name，以及changelog的文件路径唯一标识。changelog在执行的时候并不是按照id的顺序，而是按照changeSet在changelog中出现的顺序。 LiquiBase在执行changelog时，会在数据库中插入两张表：DATABASECHANGELOG和DATABASECHANGELOGLOCK，分别记录changelog的执行日志和锁日志。 LiquiBase在执行changelog中的changeSet时，会首先查看DATABASECHANGELOG表，如果已经执行过，则会跳过（除非changeSet的runAlways属性为true，后面会介绍），如果没有执行过，则执行并记录changelog日志； changelog中的一个changeSet对应一个事务，在changeSet执行完后commit，如果出现错误则rollback； &lt;changeSet&gt;标签的主要属性有： runAlways：即使已经执行过，仍然每次都执行；注意: 由于DATABASECHANGELOG表中还记录了changeSet的MD5校验值MD5SUM，如果changeSet的id和name没变，而内容变了，则由于MD5值变了，即使runAlways的值为True，执行也是失败的，会报错。这种情况应该使用runOnChange属性。 [ERROR] Failed to execute goal org.liquibase:liquibase-maven-plugin:3.4.2:update (default-cli) on project tx_test: Error setting up or running Liquibase: Validation Failed: [ERROR] 1 change sets check sum runOnChange：第一次的时候执行以及当changeSet的内容发生变化时执行。不受MD5校验值的约束。 runInTransaction：是否作为一个事务执行，默认为true。设置为false时需要小心：如果执行过程中出错了则不会rollback，数据库很可能处于不一致的状态； &lt;changeSet&gt;下有一个重要的子标签&lt;rollback&gt;，即定义回滚的SQL语句。对于create table, rename column和add column等，LiquiBase会自动生成对应的rollback语句，而对于drop table、insert data等则需要显示定义rollback语句。 2.2 &lt;include&gt;与&lt;includeAll&gt;标签当changelog文件越来越多时，可以使用&lt;include&gt;将文件管理起来，如： &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt; &lt;databaseChangeLog xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.liquibase.org/xml/ns/dbchangelog&quot; xsi:schemaLocation=&quot;http://www.liquibase.org/xml/ns/dbchangelog http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-3.1.xsd&quot;&gt; &lt;include file=&quot;logset-20160408/0001_authorization_init.sql&quot; relativeToChangelogFile=&quot;true&quot;/&gt; &lt;/databaseChangeLog&gt; &lt;include&gt;的file属性表示要包含的changelog文件的路径，这个文件可以是LiquiBase支持的任意格式，relativeToChangelogFile如果为true，则表示file属性表示的文件路径是相对于根changelog而不是CLASSPATH的，默认为false。 &lt;includeAll&gt;指定的是changelog的目录，而不是为文件，如： &lt;includeAll path=&quot;com/example/changelogs/&quot;/&gt; 注意: 目前&lt;include&gt;没有解决重复引用和循环引用的问题，重复引用还好，LiquiBase在执行的时候可以判断重复，而循环引用会导致无限循环，需要注意！ 2.3 diff命令diff命令用于比较数据库之间的异同。比如通过命令行执行： java -jar liquibase.jar --driver=com.mysql.jdbc.Driver \\ --classpath=./mysql-connector-java-5.1.29.jar \\ --url=jdbc:mysql://127.0.0.1:3306/test \\ --username=root --password=passwd \\ diff \\ --referenceUrl=jdbc:mysql://127.0.0.1:3306/authorization \\ --referenceUsername=root --referencePassword=passwd 2.4 generateChangeLog在已有的项目上使用LiquiBase，要生成当前数据库的changeset，可以采用两种方式，一种是使用数据库工具导出SQL数据，然后changelog文件以SQL格式记录即可；另一种方式就是用generateChangeLog命令，如： liquibase --driver=com.mysql.jdbc.Driver \\ --classpath=./mysql-connector-java-5.1.29.jar \\ --changeLogFile=liquibase/db.changelog.xml \\ --url=&quot;jdbc:mysql://127.0.0.1:3306/test&quot; \\ --username=root \\ --password=yourpass \\ generateChangeLog 不过generateChangeLog不支持以下功能：存储过程、函数以及触发器； 3. Maven集成LiquiBase3.1 liquibase-maven-plugin的配置Maven中集成LiquiBase，主要是配置liquibase-maven-plugin，首先给出一个示例： &lt;plugin&gt; &lt;groupId&gt;org.liquibase&lt;/groupId&gt; &lt;artifactId&gt;liquibase-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.4.2&lt;/version&gt; &lt;configuration&gt; &lt;changeLogFile&gt;src/main/resources/liquibase/test_changelog.xml&lt;/changeLogFile&gt; &lt;driver&gt;com.mysql.jdbc.Driver&lt;/driver&gt; &lt;url&gt;jdbc:mysql://127.0.0.1:3306/test&lt;/url&gt; &lt;username&gt;root&lt;/username&gt; &lt;password&gt;passwd&lt;/password&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;process-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;update&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 其中&lt;configuration&gt;节点中的配置可以放在单独的配置文件里。 如果需要在父项目中配置子项目共享的LiquiBase配置，而各个子项目可以定义自己的配置，并覆盖父项目中的配置，则只需要在父项目的pom中将propertyFileWillOverride设置为true即可，如： &lt;plugin&gt; &lt;groupId&gt;org.liquibase&lt;/groupId&gt; &lt;artifactId&gt;liquibase-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.4.2&lt;/version&gt; &lt;configuration&gt; &lt;propertyFileWillOverride&gt;true&lt;/propertyFileWillOverride&gt; &lt;propertyFile&gt;liquibase/liquibase.properties&lt;/propertyFile&gt; &lt;/configuration&gt; &lt;/plugin&gt; 3.2 liquibase:update执行changelog中的变更： $ mvn liquibase:update 3.3 liquibase:rollbackrollback有3中形式，分别是： - rollbackCount: 表示rollback的changeset的个数； - rollbackDate：表示rollback到指定的日期； - rollbackTag：表示rollback到指定的tag，需要使用LiquiBase在具体的时间点打上tag； rollbackCount比较简单，示例如： $ mvn liquibase:rollback -Dliquibase.rollbackCount=3 rollbackDate需要注意日期的格式，必须匹配当前平台上执行DateFormat.getDateInstance()得到的格式，比如我的格式为MMM d, yyyy，示例如： $ mvn liquibase:rollback -Dliquibase.rollbackDate=&quot;Apr 10, 2016&quot; rollbackTag使用tag标识，所以需要先打tag，示例如： $ mvn liquibase:tag -Dliquibase.tag=tag20160410 然后rollback到tag20160410，如： $ mvn liquibase:rollback -Dliquibase.rollbackTag=tag20160410 4. Gradle集成LiquiBase首先在build.gradle中配置liquibase-gradle-plugin： buildscript { repositories { mavenCentral() } dependencies { classpath &quot;org.liquibase:liquibase-gradle-plugin:1.2.1&quot; classpath &quot;mysql:mysql-connector-java:5.1.38&quot; } } apply plugin: &apos;org.liquibase.gradle&apos; 然后在build.gradle中配置该plugin的activities，其中一个activity表示一种运行环境： liquibase { activities { main { changeLogFile &quot;src/main/resources/web-bundle-config/liquibase/main-changelog.xml&quot; url &quot;jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;amp;characterEncoding=utf-8&quot; username &quot;root&quot; password &quot;yourpass&quot; } test { main { changeLogFile &quot;src/main/resources/web-bundle-config/liquibase/main-test-changelog.xml&quot; url &quot;jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;amp;characterEncoding=utf-8&quot; username &quot;root&quot; password &quot;yourpass&quot; } } runList = project.ext.runList } } 比如执行main的命令为： $ gradle update -PrunList=main 参考 Building Changelogs How to tag a changeset in liquibase to rollback only buildscript {} and other plugins {} script blocks are allowed before plugins {} blocks, no other statements are allowed","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"LiquiBase","slug":"LiquiBase","permalink":"http://nkcoder.github.io/tags/LiquiBase/"},{"name":"Maven","slug":"Maven","permalink":"http://nkcoder.github.io/tags/Maven/"},{"name":"Gradle","slug":"Gradle","permalink":"http://nkcoder.github.io/tags/Gradle/"}]},{"title":"Ehcache以及与MyBatis的集成","slug":"ehcache-mybatis","date":"2016-04-09T14:23:36.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2016/04/09/ehcache-mybatis/","link":"","permalink":"http://nkcoder.github.io/2016/04/09/ehcache-mybatis/","excerpt":"本文链接为：http://nkcoder.github.io/2016/04/09/ehcache-mybatis/ ，转载请注明出处，谢谢！ Ehcache是目前使用很广泛的Java系的开源cache。可以把它当作通用的cache，或者作为Hibernate/MyBatis等的二级缓存。本文简要介绍在MyBatis中集成Ehcache，其中Ehcache的版本是2.10.1。 1. Ehcache的配置Ehcache默认使用CLASSPATH根目录下的ehcache.xml作为配置文件，如果没找到，则使用Jar包下的ehcache-failsafe.xml作为配置文件，该配置文件提供了默认的简单配置：","text":"本文链接为：http://nkcoder.github.io/2016/04/09/ehcache-mybatis/ ，转载请注明出处，谢谢！ Ehcache是目前使用很广泛的Java系的开源cache。可以把它当作通用的cache，或者作为Hibernate/MyBatis等的二级缓存。本文简要介绍在MyBatis中集成Ehcache，其中Ehcache的版本是2.10.1。 1. Ehcache的配置Ehcache默认使用CLASSPATH根目录下的ehcache.xml作为配置文件，如果没找到，则使用Jar包下的ehcache-failsafe.xml作为配置文件，该配置文件提供了默认的简单配置： &lt;ehcache xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:noNamespaceSchemaLocation=&quot;../config/ehcache.xsd&quot;&gt; &lt;diskStore path=&quot;java.io.tmpdir&quot;/&gt; &lt;defaultCache maxElementsInMemory=&quot;10000&quot; eternal=&quot;false&quot; timeToIdleSeconds=&quot;120&quot; timeToLiveSeconds=&quot;120&quot; maxElementsOnDisk=&quot;10000000&quot; diskExpiryThreadIntervalSeconds=&quot;120&quot; memoryStoreEvictionPolicy=&quot;LRU&quot;&gt; &lt;persistence strategy=&quot;localTempSwap&quot;/&gt; &lt;/defaultCache&gt; &lt;/ehcache&gt; &lt;ehcache&gt;节点对应一个CacheManager，一个CacheManager可以管理多个cache实例。ehcache节点可配置参数主要有： - updateCheck: CacheManager是否自动在线检测Ehcache的新版本，默认为true； - dynamicConfig: 是否允许cache动态配置，即运行时动态调整内存、磁盘等的容量，默认为true； 以下三个配置由CacheManager管理的所有cache共享，单位可以时B/M/G： - maxBytesLocalHeap: CacheManager可用的最大heap内存; - maxBytesLocalOffHeap: CacheManager可用的最大的off-heap内存； - maxBytesLocalDisk: CacheManager可用的最大的本地disk空间； &lt;diskStore&gt;节点表示启用磁盘cache时的文件路径，如果使用了磁盘作为cache，但是没有指定&lt;diskStore&gt;，则Ehcache默认使用java.io.tmpdir作为cache的磁盘路径； &lt;cache&gt;节点和&lt;defaultCache&gt;都用于表示一个cache实例，不同的是&lt;cache&gt;需要配置name属性，而&lt;defaultCache&gt;不需要，因为它的name默认为default，&lt;defaultCache&gt;用于未显式配置的cache。 &lt;cache&gt;节点中的主要参数有： - name：唯一标识cache实例； - maxEntriesLocalHeap：Memory中可保存的对象的最大数量，默认为0表示不限； - maxEntriesLocalDisk：Disk中可保存的对象的最大数量，默认为0表示不限； - eternal：表示cache是否过期，如果eternal为true，则对象永不过期； - maxBytesLocalHeap：该实例的最大可用Heap，不能超过`&lt;ehcache&gt;`中配置到CacheManager的最大Heap容量，如果使用了maxBytesLocalHeap，则不能使用maxBytesLocalHeap； - maxBytesLocalDisk：该实例的最大可用磁盘容量； - timeToIdleSeconds：表示对象最后一次访问到过期的时间，默认为0，表示不过期，该参数仅当eternal为false时有效； - timeToLiveSeconds：表示对象从创建到过期的时间，默认为0，表示不过期，该参数仅当eternal为false时有效； - memoryStoreEvictionPolicy：当cache的对象达到maxEntriesLocalHeap限制时使用的剔除策略，默认为`LRU`，可用值有：LRU, FIFO, LFU &lt;persistence&gt;节点的参数: - &lt;strategy&gt;表示持久化方式，值为localTempSwap表示当heap/off-heap满的时候，将缓存的对象持久化到disk，none表示不持久化chache； 2. MyBatis中配置Ehcache首先添加mybatis-ehcache依赖，当前版本是1.0.3，然后配置ehcache.xml并放在CLASSPATH的根目录下，最后在mapper文件中定义&lt;cache&gt;节点，如： &lt;mapper namespace=&quot;xxx&quot;&gt; &lt;cache type=&quot;org.mybatis.caches.ehcache.EhcacheCache&quot;/&gt; &lt;select id=&quot;xxx&quot; parameterType=&quot;map&quot; resultType=&quot;xxx&quot;&gt; ... &lt;/select&gt; &lt;/mapper&gt; 也可以在&lt;cache&gt;节点中配置ecache，就不需要额外的.ecache.xml配置了，如： &lt;cache type=&quot;org.mybatis.caches.ehcache.EhcacheCache&quot;&gt; &lt;property name=&quot;timeToIdleSeconds&quot; value=&quot;3600&quot;/&gt; &lt;property name=&quot;timeToLiveSeconds&quot; value=&quot;3600&quot;/&gt; &lt;property name=&quot;maxEntriesLocalHeap&quot; value=&quot;1000&quot;/&gt; &lt;property name=&quot;maxEntriesLocalDisk&quot; value=&quot;100000&quot;/&gt; &lt;property name=&quot;memoryStoreEvictionPolicy&quot; value=&quot;LRU&quot;/&gt; &lt;/cache&gt; 在mybatis-ehcache的1.1.0-SNAPSHOT中，cache的type，除了EhcacheCache，还可以是EhBlockingCache。EhBlockingCache的主要应用场景是要缓存的数据是动态变化的，而并发访问数据的请求非常高，此时使用阻塞cache，让第一个线程去cache，其它等待的限制只需要直接去cache中取数据即可。 参考 Configuring Cache MyBatis Ehcache Adapter - Reference Documentation ehcache.xml","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"Ehcache","slug":"Ehcache","permalink":"http://nkcoder.github.io/tags/Ehcache/"},{"name":"MyBatis","slug":"MyBatis","permalink":"http://nkcoder.github.io/tags/MyBatis/"}]},{"title":"Google的Python编码规范","slug":"google-python-style-guide","date":"2016-02-26T06:05:12.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2016/02/26/google-python-style-guide/","link":"","permalink":"http://nkcoder.github.io/2016/02/26/google-python-style-guide/","excerpt":"本文链接为：http://nkcoder.github.io/2016/02/26/google-python-style-guide/ ，转载请注明出处，谢谢！ Python Style RulesSemicolons语句结尾不要使用分号，也不要通过分号将两条语句写在同一行上。 Line length最大行宽为80个字符，除非：1. 很长的import语句；2. 注释中的URL。 不要使用反斜线(\\)做行连接。使用()/[]/{}的隐式连接方式，如果有必要，可以将表达式放在额外的括号中：","text":"本文链接为：http://nkcoder.github.io/2016/02/26/google-python-style-guide/ ，转载请注明出处，谢谢！ Python Style RulesSemicolons语句结尾不要使用分号，也不要通过分号将两条语句写在同一行上。 Line length最大行宽为80个字符，除非：1. 很长的import语句；2. 注释中的URL。 不要使用反斜线(\\)做行连接。使用()/[]/{}的隐式连接方式，如果有必要，可以将表达式放在额外的括号中： Yes: foo_bar(self, width, height, color=&apos;black&apos;, design=None, x=&apos;foo&apos;, emphasis=None, highlight=0) if (width == 0 and height == 0 and color == &apos;red&apos; and emphasis == &apos;strong&apos;): 如果字符串太长，一行容不下，使用括号进行隐式连接： x = (&apos;This will build a very long long &apos; &apos;long long long long long long string&apos;) 在注释中，URL始终在一行显示： Yes: # See details at # http://www.example.com/us/developer/documentation/api/content/v2.0/csv_file_name_extension_full_specification.html No: # See details at # http://www.example.com/us/developer/documentation/api/content/\\ # v2.0/csv_file_name_extension_full_specification.html Parentheses括号能不用则不用。 在条件判断、返回语句中不要使用括号，除非用于较长字符串的隐式连接。tuple用括号是可以的。 Yes: if foo: bar() while x: x = bar() if x and y: bar() if not x: bar() return foo for (x, y) in dict.items(): ... No: if (x): bar() if not(x): bar() return (foo) Indentation代码缩进使用4个空格。 不要使用tab或混用tab和空格。在隐式连接时，可以两行元素之间垂直对齐，也可以缩进4个空格对齐，两种方式都可以。 Yes: # Aligned with opening delimiter foo = long_function_name(var_one, var_two, var_three, var_four) # Aligned with opening delimiter in a dictionary foo = { long_dictionary_key: value1 + value2, ... } # 4-space hanging indent; nothing on first line foo = long_function_name( var_one, var_two, var_three, var_four) # 4-space hanging indent in a dictionary foo = { long_dictionary_key: long_dictionary_value, ... } No: # Stuff on first line forbidden foo = long_function_name(var_one, var_two, var_three, var_four) # 2-space hanging indent forbidden foo = long_function_name( var_one, var_two, var_three, var_four) # No hanging indent in a dictionary foo = { long_dictionary_key: long_dictionary_value, ... } Blank Lines顶层(top-level)的定义之间空两行，method之间空一行。 top-level定义，无论是function还是class，空两行。method之间，以及class与第一个method之间，空一行。 Whitespace按照标准排印规则使用空格。 ()/[]/{}里面不要使用空格： Yes: spam(ham[1], {eggs: 2}, []) No: spam( ham[ 1 ], { eggs: 2 }, [ ] ) 逗号，分号和冒号前面没有空格，除非是在一行的末尾，否则逗号，分号和冒号的后面不要使用空格： Yes: if x == 4: print x, y x, y = y, x No: if x == 4 : print x , y x , y = y , x 作为参数列表或下标索引的小括号()和中括号[]的前面不要使用空格： Yes: spam(1) No: spam (1) Yes: dict[&apos;key&apos;] = list[index] No: dict [&apos;key&apos;] = list [index] 二元操作符(==, &gt;, &lt; !=, &lt;&gt;, &lt;=, &gt;=, in, not in, is, is not, and, or, not)的前后各使用一个空格： Yes: x == 1 No: x&lt;1 =用于参数默认值时，前后不要使用空格： Yes: def complex(real, imag=0.0): return magic(r=real, i=imag) No: def complex(real, imag = 0.0): return magic(r = real, i = imag) 对于连续的行，不要通过空格去垂直对齐(主要是=和#): Yes: foo = 1000 # comment long_name = 2 # comment that should not be aligned dictionary = { &apos;foo&apos;: 1, &apos;long_name&apos;: 2, } No: foo = 1000 # comment long_name = 2 # comment that should not be aligned dictionary = { &apos;foo&apos; : 1, &apos;long_name&apos;: 2, } Shebang Line大多数的`.py`文件都不需要`#!`行，仅在main文件的第一行使用`#!/usr/bin/python`，版本号2/3后缀是可选的。 #!行被kernel用于查找Python解释器，但是在import module的时候被忽略，所以仅当文件被直接运行的时候才需要。 Comments确保正确地使用各种不同形式的注释：module/function/method/inline. Doc Strings 对于doc strings，建议总是使用三个双引号(&quot;&quot;&quot;)形式：与&quot;&quot;&quot;同一行是注释的概述，然后空一行，与&quot;&quot;&quot;缩进相同的位置开始是注释的详细说明。 Modules 每一个文件都应该包含合适的licence引用信息。 Functions and Methods function必须包含docstring，除非：1. 不被外部使用；2. 非常短；3. 非常明显； docstring中应该包含调用function的足够的信息，而不需要阅读function的代码。docstring应该描述调用函数的语法，而不是函数的实现。 function的docstring分为不同的section：Args, Returns, Raises，section名后面使用冒号，section描述缩进显示。 Args: 依次列出参数名，后跟一个冒号和空格，参数的描述应该包含需要的类型和参数的含义；参数名之间缩进对齐。Returns: （对于generator，是Yields），返回值的含义，如果返回None，则该section可以省略。Raises: 列出所有的异常； def fetch_bigtable_rows(big_table, keys, other_silly_variable=None): &quot;&quot;&quot;Fetches rows from a Bigtable. Retrieves rows pertaining to the given keys from the Table instance represented by big_table. Silly things may happen if other_silly_variable is not None. Args: big_table: An open Bigtable Table instance. keys: A sequence of strings representing the key of each table row to fetch. other_silly_variable: Another optional variable, that has a much longer name than the other args, and which does nothing. Returns: A dict mapping keys to the corresponding table row data fetched. Each row is represented as a tuple of strings. For example: {&apos;Serak&apos;: (&apos;Rigel VII&apos;, &apos;Preparer&apos;), &apos;Zim&apos;: (&apos;Irk&apos;, &apos;Invader&apos;), &apos;Lrrr&apos;: (&apos;Omicron Persei 8&apos;, &apos;Emperor&apos;)} If a key from the keys argument is missing from the dictionary, then that row was not found in the table. Raises: IOError: An error occurred accessing the bigtable.Table object. &quot;&quot;&quot; pass Classes docstring应该位于class的定义下面，如果class包含public的属性，使用Attributes依次列出。 class SampleClass(object): &quot;&quot;&quot;Summary of class here. Longer class information.... Longer class information.... Attributes: likes_spam: A boolean indicating if we like SPAM or not. eggs: An integer count of the eggs we have laid. &quot;&quot;&quot; def __init__(self, likes_spam=False): &quot;&quot;&quot;Inits SampleClass with blah.&quot;&quot;&quot; self.likes_spam = likes_spam self.eggs = 0 def public_method(self): &quot;&quot;&quot;Performs operation blah.&quot;&quot;&quot; Block and Inline Comments 对于复杂的逻辑，注释应该在逻辑的上面单独说明，对于简单但不明显的逻辑，注释放在代码的行末，但是与代码至少有2个空格的间距。 # We use a weighted dictionary search to find out where i is in # the array. We extrapolate position based on the largest num # in the array and the array size and then do binary search to # get the exact number. if i &amp; (i-1) == 0: # true iff i is a power of 2 永远不要试图描述你的代码。要假设阅读代码的人比你更懂Python： # BAD COMMENT: Now go through the b array and make sure whenever i occurs # the next element is i+1 Classes如果class没有继承别的class，则显式继承`object`，嵌套的class也是如此。 继承object可以使properties正常工作，同时也避免了与Python 3的new style class的不兼容。而且，继承object，预定义了很多默认的属性和method，如__new__, __init__, __delattr__, __getattribute__, __setattr__, __hash__, __repr__, __str__。 Yes: class SampleClass(object): pass class OuterClass(object): class InnerClass(object): pass class ChildClass(ParentClass): &quot;&quot;&quot;Explicitly inherits from another class already.&quot;&quot;&quot; No: class SampleClass: pass class OuterClass: class InnerClass: pass Strings使用`format()`或`%`格式化字符串，即使所有的参数都是string。 Yes: x = a + b x = &apos;%s, %s!&apos; % (imperative, expletive) x = &apos;{}, {}!&apos;.format(imperative, expletive) x = &apos;name: %s; score: %d&apos; % (name, n) x = &apos;name: {}; score: {}&apos;.format(name, n) No: x = &apos;%s%s&apos; % (a, b) # use + in this case x = &apos;{}{}&apos;.format(a, b) # use + in this case x = imperative + &apos;, &apos; + expletive + &apos;!&apos; x = &apos;name: &apos; + name + &apos;; score: &apos; + str(n) 不要使用+和+=在循环中拼接字符串。因为字符串是不可变的，这样会创建很多不必要的临时对象，导致运行时间是乘方级的，而不是线性的。更好地做法应该是，循环将各个字串放到list中，循环结束后通过&#39;&#39;.join()连接： Yes: items = [&apos;&lt;table&gt;&apos;] for last_name, first_name in employee_list: items.append(&apos;&lt;tr&gt;&lt;td&gt;%s, %s&lt;/td&gt;&lt;/tr&gt;&apos; % (last_name, first_name)) items.append(&apos;&lt;/table&gt;&apos;) employee_table = &apos;&apos;.join(items) No: employee_table = &apos;&lt;table&gt;&apos; for last_name, first_name in employee_list: employee_table += &apos;&lt;tr&gt;&lt;td&gt;%s, %s&lt;/td&gt;&lt;/tr&gt;&apos; % (last_name, first_name) employee_table += &apos;&lt;/table&gt;&apos; 在同一个文件中，对于字符串引号的使用要保持一致。使用&#39;&#39;或&quot;&quot;都可以，保持一致即可。两者可以同时使用，避免通过\\转义。 Yes: Python(&apos;Why are you hiding your eyes?&apos;) Gollum(&quot;I&apos;m scared of lint errors.&quot;) Narrator(&apos;&quot;Good!&quot; thought a happy Python reviewer.&apos;) No: Python(&quot;Why are you hiding your eyes?&quot;) Gollum(&apos;The lint. It burns. It burns us.&apos;) Gollum(&quot;Always the great lint. Watching. Watching.&quot;) 如果string占多行，建议使用&quot;&quot;&quot;，而不是&#39;&#39;&#39;。当且仅当字符串使用&#39;&#39;表示，多行字符串可以使用&#39;&#39;&#39;表示。docstring总是使用&quot;&quot;&quot;，无论什么情况下。通常，对于多行字符串使用隐式连接(())更清晰易读： Yes: print (&quot;This is much nicer.\\n&quot; &quot;Do it this way.\\n&quot;) No: print &quot;&quot;&quot;This is pretty ugly. Don&apos;t do this. &quot;&quot;&quot; Files and Sockets使用完之后，要显式关闭file和socket。 没有合理地关闭file或socket会带来很多问题： 会消耗很多系统资源，比如文件描述符。如果这类的对象很多，可能会耗尽系统资源。 file处于未关闭状态，可能会导致其它的操作不可用，比如移动或删除。 被共享的file和socket，可能被意外地读写，如果被显式关闭，则读写会立即产生异常。 当file对象被销毁的时候，file和socket会被自动关闭，但是将file对象的存活期与其状态绑定并不是好的实践： 无法保证runtime一定会执行file对象的析构函数。 对file对象意外的引用可能会延长其存活期。 推荐使用with语句操作file： with open(&quot;hello.txt&quot;) as hello_file: for line in hello_file: print line 对于不支持with的类似对象，可以使用contextlib.closing(): import contextlib with contextlib.closing(urllib.urlopen(&quot;http://www.python.org/&quot;)) as front_page: for line in front_page: print line TODO Comments`TODO`注释用于临时的、短期的或不够优化的解决方法。 TODO的格式：TODO大写，后跟的小括号内是用户名/邮箱等，表示应该关注该TODO的人，接下来的冒号是可选的，后面的注释表示TODO的内容。TODO的用户并不一定是fix这个问题的人，所以这里的用户几乎都是自己。 # TODO(kl@gmail.com): Use a &quot;*&quot; here for string repetition. # TODO(Zeke) Change this to use relations. 如果TODO表示的是“在将来的某个时间点修复”，则务必包含具体的日期(2009年11月前修复)或事件(当所有的客户端都可以处理XML结果时删除这段代码)。 Imports formattingimport语句应该独占一行。 Yes: import os import sys No: import os, sys import总是位于文件的顶部，在module的注释和docstring的后面，而在module的全局变量和常量的前面。import应该根据通用性进行分组： import标准库 import第三方库 import应用特定的库 在每一个分组中，import应该根据module的全包名按照字母序排列： import foo from foo import bar from foo.bar import baz from foo.bar import Quux from Foob import ar Statements通常，一行仅允许一条语句。 当if没有else分支，且一行可以容纳if的结果语句时，可以将if的结果语句和if的判断语句写在同一行。不能将try/except的语句放在同一行： Yes: if foo: bar(foo) No: if foo: bar(foo) else: baz(foo) try: bar(foo) except ValueError: baz(foo) try: bar(foo) except ValueError: baz(foo) Naming命名规范：module_name, package_name, ClassName, method_name, ExceptionName, function_name, GLOBAL_CONSTANT_NAME, global_var_name, instance_var_name, function_parameter_name, local_var_name. 避免使用的命名： 除了计数(counter)和迭代(iterator)，不要使用单字符变量名。 不要在package/module名中使用连字符(-)。 不要使用双下划线开头和双下划线结尾的变量(double_leading_and_trailing)。 命名规范： Internal表示module内部的，或者class的private或protected。 以_开头表示module内部的变量或函数(不包含在import * from)，使用__开头表示class的private变量或函数。 将相关的class和顶层的function放在一个module里。与Java不同的是，一个module中可以包含多个class。 class使用首字母大写(CapWord)的命名方式，而module名使用小写和下划线(lower_with_uder.py)形式。 Python之父Guido推荐的命名规范： Type Public Internal Packages lower_with_under Modules lower_with_under _lower_with_under Classes CapWords _CapWords Exceptions CapWords Functions lower_with_under() _lower_with_under() Global/Class Constants CAPS_WITH_UNDER _CAPS_WITH_UNDER Global/Class Variables lower_with_under _lower_with_under Instance Variables lower_with_under _lower_with_under (protected) or __lower_with_under (private) Method Names lower_with_under() _lower_with_under() (protected) or __lower_with_under() (private) Function/Method Parameters lower_with_under Local Variables lower_with_under Main即使是脚本文件，也应该是可以被其它文件import的，因此在import时，不要产生副作用，即 不要执行脚本的主功能。主功能应该被放在`main()`函数里。 pydoc和单元测试都需要import文件，所以文件中应该总是添加if __name__ == &#39;__main__&#39;确保当module被import的时候，不会执行其主功能。 def main(): ... if __name__ == &apos;__main__&apos;: main() 在module被import的时候，所有顶层(top-level)的代码都会被执行。 Python Language RulesLint使用`pylint`检查代码的bug和编码规范等问题。 pylint可以检测一些常见的错误，比如拼写错误、用var声明变量等，但是pylint并不完全准确，经常会有一些误报的warning，这些warning可以忽略。 Imports`import`仅用于package和module。 import x: import一个package或module；from x import y: x是package名，y是不带前缀的module名；from x import y as z: 如果有两个module的名称都是y，或者y的名字比较长时，使用这种形式； 在import中不要使用相对名称，应该使用package的全名。例如： from sound.effects import echo ... echo.EchoFilter(input, output, delay=0.7, atten=4) Packages在import一个module的时候，使用module的全路径名。 可以避免module名冲突。方便module的查找。 例如： # Reference in code with complete name. import sound.effects.echo # Reference in code with just module name (preferred). from sound.effects import echo ExceptionsException可以使用，但是必须谨慎。 使用Exception，需遵循以下原则： 使用raise MyException(&#39;Error message&#39;)或raise MyException，不要使用两个参数的形式(raise MyException, &#39;Error message&#39;)，也不要使用过时的String形式(raise &#39;Error message&#39;)。 module或package应该定义自己的exception基类，该基类应该继承Exception类，例如： class Error(Exception): pass; 不要使用捕获所有异常(catch-all)的形式，如except:, 或except Exception:, 以及except StandardError等，除非将异常重新抛出，或者当前处于线程的最外层。否则所有的异常（比如拼写错误、单元测试错误、Ctrl+C中断等）都会被捕获。 尽量简化try/except中的代码块，代码越多，发生错误的概率就越大，而真正的错误很可能被忽略了。 无论是否发生Exception，使用finally执行一些代码，比如关闭文件； 当捕获到Exception时，使用as而不是逗号，例如： try: raise Error except Error as error: pass Global Variables尽量避免使用全局变量。 当moduel被import后，module变量（全局变量）可以通过赋值修改。 尽量使用class变量，而不是全局变量。以下是例外情况： 脚本的默认选项； module的常量，例如：PI = 3.14159; 通过全局变量缓存值； Nested/Local/Inner Classes and Functions嵌套定义class和function是允许的 class可以定义在method/function/class中，function可以定义在method/function中。嵌套的函数对于上层的变量是只读的；适用于在一个作用域内定义工具类和函数。 List Comprehensions适用于简单的情形 例如： Yes: result = [] for x in range(10): for y in range(5): if x * y &gt; 10: result.append((x, y)) for x in xrange(5): for y in xrange(5): if x != y: for z in xrange(5): if y != z: yield (x, y, z) return ((x, complicated_transform(x)) for x in long_generator_function(parameter) if x is not None) squares = [x * x for x in range(10)] eat(jelly_bean for jelly_bean in jelly_beans if jelly_bean.color == &apos;black&apos;) No: result = [(x, y) for x in range(10) for y in range(5) if x * y &gt; 10] return ((x, y, z) for x in xrange(5) for y in xrange(5) if x != y for z in xrange(5) if y != z) Default Iterators and Operators如果类型支持（如list/dictionary/file)，使用默认的iterator和operator。 容器类型，如list/dictionary定义了默认的iterator和成员测试操作符(in和not in)，使用这些默认的iterator和operator，简单高效，没有额外的函数调用开销，例如： Yes: for key in adict: ... if key not in adict: ... if obj in alist: ... for line in afile: ... for k, v in dict.iteritems(): ... No: for key in adict.keys(): ... if not adict.has_key(key): ... for line in afile.readlines(): ... Generators在需要的时候使用generator. 代码更简单，相比函数一次创建整个list，generator消耗的内存更少。 Lambda Functions适用于`一行代码`的情形。 Lambda表达式就是匿名函数，一般作为map()/filter()等高阶函数的回调或操作符；如果lambda表达式中的代码比较长（超过60-80字符），最好定义成函数；常用的操作，比如乘法运算，推荐使用operator模块(operator.mul)，而不是lambda函数(lambda x, y: x * y)。 Conditional Expressions适用于`一行代码`的情形。 条件表达式就是if语句的简写形式，代码更短也更方便，但是如果表达式语句较长，则条件可能不容易定位。适用于只有一行代码的情形，其它情况使用if语句。 Default Argument Values大部分情况下都是可以的。 函数的参数可以使用默认值，但是不要将可变对象作为函数的默认值： Yes: def foo(a, b=None): if b is None: b = [] No: No: def foo(a, b=[]): ... No: def foo(a, b=time.time()): # The time the module was loaded??? ... No: def foo(a, b=FLAGS.my_thing): # sys.argv has not yet been parsed... ... True/False evaluations尽量使用`隐式`的false判断。 所有含义为空的值，如0/None/[]/{}/&#39;&#39;都是false； 使用if foo:，而不是if foo != []; 不要使用==或!=去比较None，应该用is或is not； 不要使用==去比较False，应该使用if not x:; 如果要区分False和None，使用if not x and x is not None:； 对于列表类型(string/list/tuple)，空值表示false，使用if not seq:或if seq:，比使用if len(seq):或if not len(seq):更好； 注意，’0’是true； 对于整数，必须额外小心，不要将None当作0处理： Yes: if not users: print &apos;no users&apos; if foo == 0: self.handle_zero() if i % 10 == 0: self.handle_multiple_of_ten() No: if len(users) == 0: print &apos;no users&apos; if foo is not None and not foo: self.handle_zero() if not i % 10: self.handle_multiple_of_ten() Threading不要依赖内置类型的原子行(atomicity) 有一些内置类型(如dictionary)的某些操作看起来是原子性的，但是它们不可靠；使用Queue模块中的Queue作为线程间交互的数据结构，或者使用threading模块。 参考 Google Python Style Guide","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"python","slug":"python","permalink":"http://nkcoder.github.io/tags/python/"}]},{"title":"Java 8新的日期和时间API","slug":"java-8-date-time-api","date":"2016-01-31T08:23:29.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2016/01/31/java-8-date-time-api/","link":"","permalink":"http://nkcoder.github.io/2016/01/31/java-8-date-time-api/","excerpt":"本文链接为：http://nkcoder.github.io/2016/01/31/java-8-date-time-api/ ，转载请注明出处，谢谢！ 1. Instant 与 Duration1) Instant表示某一个时间点的时间戳，可以类比于java.uti.Date。支持各种运算操作： Instant begin = Instant.now(); begin.plus(5, ChronoUnit.SECONDS); begin.minusMillis(50); begin.isBefore(Instant.now()); begin.toEpochMilli();","text":"本文链接为：http://nkcoder.github.io/2016/01/31/java-8-date-time-api/ ，转载请注明出处，谢谢！ 1. Instant 与 Duration1) Instant表示某一个时间点的时间戳，可以类比于java.uti.Date。支持各种运算操作： Instant begin = Instant.now(); begin.plus(5, ChronoUnit.SECONDS); begin.minusMillis(50); begin.isBefore(Instant.now()); begin.toEpochMilli(); 2) Duration表示Instant之间的时间差，可以用来统计任务的执行时间，也支持各种运算操作，比如： Instant begin = Instant.now(); // do some work Instant end = Instant.now(); Duration elapsed = Duration.between(begin, end); elapsed.toMillis() elapsed.dividedBy(10).minus(Duration.ofMillis(10)).isNegative(); elapsed.isZero(); elapsed.plusHours(3); 2. LocalDate 与 Period1) LocalDate用于表示日期，与时区(TimeZone)无关。 创建LocalDate： LocalDate now = LocalDate.now(); LocalDate today = LocalDate.of(2016, 1, 31); LocalDate today2 = LocalDate.of(2016, Month.JANUARY, 31); // JANUARY = 1, ..., DECEMBER = 12 支持的操作： today2.getDayOfWeek().getValue(); // Monday = 1, ..., Sunday = 7 LocalDate dayOfYear = Year.now().atDay(220); YearMonth april = Year.of(2016).atMonth(Month.APRIL); 注意，有些操作得到的日期可能是不存在的，比如2016-01-31增加1个月后为2016-02-31，该日期是不存在的，返回值为该月的最后一天，即2016-02-29: LocalDate nextMonth = LocalDate.of(2016, 1, 31).plusMonths(1); // 2016-02-29 2) Period用来表示两个LocalDate之间的时间差，支持各种运算操作： LocalDate fiveDaysLater = LocalDate.now().plusDays(5); Period period = LocalDate.now().until(fiveDaysLater).plusMonths(2); period.isNegative(); 3) TemporalAdjusters用于表示某个月第一天、下个周一等日期： LocalDate.now().with(TemporalAdjusters.firstDayOfMonth()); LocalDate.now().with(TemporalAdjusters.lastInMonth(DayOfWeek.SUNDAY)); LocalDate.now().with(TemporalAdjusters.nextOrSame(DayOfWeek.MONDAY)); 3. LocalTime1) LocalTime表示时间，没有日期，与时区(TimeZone)无关： LocalTime.now().isBefore(LocalTime.of(16, 2, 1)); LocalTime.now().plusHours(2).getHour(); 2) LocalDateTime表示日期和时间，适用于时区固定不变的场合(LocalDateTime使用系统默认的时区)，如果需要根据时区调整日期和时间，应该使用ZonedDateTime: LocalDateTime.now().plusDays(3).minusHours(5).isAfter(LocalDateTime.of(2016, 1, 30, 10, 20, 30)); 4. ZonedDateTime1) ZonedDateTime表示带时区的日期和时间，支持的操作与LocalDateTime非常类似： Set&lt;String&gt; zones = ZoneId.getAvailableZoneIds(); ZonedDateTime.now(ZoneId.of(&quot;Asia/Shanghai&quot;)).plusMonths(1).minusHours(3) .isBefore(ZonedDateTime.now()); 2) ZonedDateTime与LocalDateTime、Instant之间可以相互转换： ZonedDateTime nowOfShanghai = LocalDateTime.now().atZone(ZoneId.of(&quot;Asia/Shanghai&quot;)); ZonedDateTime.now(ZoneId.of(&quot;UTC&quot;)).toLocalDate(); ZonedDateTime nowOfShanghai2 = Instant.now().atZone(ZoneId.of(&quot;Asia/Shanghai&quot;)); ZonedDateTime.of(LocalDate.now(), LocalTime.now(), ZoneId.of(&quot;UTC&quot;)).toInstant(); 5. Formatting 与 Parsing1) 要格式化或者解析日期时，需要使用到DateTimeFormatter，用来定义日期或时间的格式： // 2016-01-31T15:39:31.481 DateTimeFormatter.ISO_LOCAL_DATE_TIME.format(LocalDateTime.now()); // Jan 31, 2016 3:50:04 PM DateTimeFormatter.ofLocalizedDateTime(FormatStyle.MEDIUM).format(LocalDateTime.now()); // Sun 2016-01-31 15:50:04 DateTimeFormatter.ofPattern(&quot;E yyyy-MM-dd HH:mm:ss&quot;).format(LocalDateTime.now()); LocalDateTime.parse(&quot;2016-01-31 15:51:00-0400&quot;, DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ssxx&quot;)); LocalDate.parse(&quot;2016-01-31&quot;); 2) 日期和时间格式化的常见格式： 年 yy: 16 yyyy: 2016 月 M: 1 MM: 01 日 d: 3 dd: 03 周 e: 3 E: Web 时 H: 9 HH: 09 钟 mm: 02 秒 ss: 00 纳秒 nnnnnn:000000 时区偏移 x: -04 xx:-0400 参考 Java SE8 for the Really Impatient) 示例代码","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"java","slug":"java","permalink":"http://nkcoder.github.io/tags/java/"}]},{"title":"使用Java 8中的Stream","slug":"java-8-stream-api","date":"2016-01-24T12:45:15.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2016/01/24/java-8-stream-api/","link":"","permalink":"http://nkcoder.github.io/2016/01/24/java-8-stream-api/","excerpt":"本文链接为：http://nkcoder.github.io/2016/01/24/java-8-stream-api/ ，转载请注明出处，谢谢！ Stream是Java 8 提供的高效操作集合类（Collection）数据的API。 1. 从Iterator到Stream有一个字符串的list，要统计其中长度大于7的字符串的数量，用迭代来实现： List&lt;String&gt; wordList = Arrays.asList(&quot;regular&quot;, &quot;expression&quot;, &quot;specified&quot;, &quot;as&quot;, &quot;a&quot;, &quot;string&quot;, &quot;must&quot;); int countByIterator = 0; for (String word: wordList) { if (word.length() &gt; 7) { countByIterator++; } }","text":"本文链接为：http://nkcoder.github.io/2016/01/24/java-8-stream-api/ ，转载请注明出处，谢谢！ Stream是Java 8 提供的高效操作集合类（Collection）数据的API。 1. 从Iterator到Stream有一个字符串的list，要统计其中长度大于7的字符串的数量，用迭代来实现： List&lt;String&gt; wordList = Arrays.asList(&quot;regular&quot;, &quot;expression&quot;, &quot;specified&quot;, &quot;as&quot;, &quot;a&quot;, &quot;string&quot;, &quot;must&quot;); int countByIterator = 0; for (String word: wordList) { if (word.length() &gt; 7) { countByIterator++; } } 用Stream实现： long countByStream = wordList.stream().filter(w -&gt; w.length() &gt; 7).count(); 显然，用stream实现更简洁，不仅如此，stream很容易实现并发操作，比如： long countByParallelStream = wordList.parallelStream(). filter(w -&gt; w.length() &gt; 7).count(); stream遵循的原则是：告诉我做什么，不用管我怎么做。比如上例：告诉stream通过多线程统计字符串长度，至于以什么顺序、在哪个线程中执行，由stream来负责；而在迭代实现中，由于计算的方式已确定，很难优化了。 Stream和Collection的区别主要有： stream本身并不存储数据，数据是存储在对应的collection里，或者在需要的时候才生成的； stream不会修改数据源，总是返回新的stream； stream的操作是懒执行(lazy)的：仅当最终的结果需要的时候才会执行，比如上面的例子中，结果仅需要前3个长度大于7的字符串，那么在找到前3个长度符合要求的字符串后，filter()将停止执行； 使用stream的步骤如下： 创建stream； 通过一个或多个中间操作(intermediate operations)将初始stream转换为另一个stream； 通过中止操作(terminal operation)获取结果；该操作触发之前的懒操作的执行，中止操作后，该stream关闭，不能再使用了； 在上面的例子中，wordList.stream()和wordList.parallelStream()是创建stream，filter()是中间操作，过滤后生成一个新的stream，count()是中止操作，获取结果。 2. 创建Stream的方式1) 从array或list创建stream： Stream&lt;Integer&gt; integerStream = Stream.of(10, 20, 30, 40); String[] cityArr = {&quot;Beijing&quot;, &quot;Shanghai&quot;, &quot;Chengdu&quot;}; Stream&lt;String&gt; cityStream = Stream.of(cityArr); Stream&lt;String&gt; nameStream = Arrays.asList(&quot;Daniel&quot;, &quot;Peter&quot;, &quot;Kevin&quot;). stream(); Stream&lt;String&gt; cityStream2 = Arrays.stream(cityArr, 0, 1); Stream&lt;String&gt; emptyStream = Stream.empty(); 2) 通过generate和iterate创建无穷stream： Stream&lt;String&gt; echos = Stream.generate(() -&gt; &quot;echo&quot;); Stream&lt;Integer&gt; integers = Stream.iterate(0, num -&gt; num + 1); 3) 通过其它API创建stream： Stream&lt;String&gt; lines = Files.lines(Paths.get(&quot;test.txt&quot;)) String content = &quot;AXDBDGXC&quot;; Stream&lt;String&gt; contentStream = Pattern.compile(&quot;[ABC]{1,3}&quot;). splitAsStream(content); 3. Stream转换1) filter()用于过滤，即使原stream中满足条件的元素构成新的stream： List&lt;String&gt; langList = Arrays.asList(&quot;Java&quot;, &quot;Python&quot;, &quot;Swift&quot;, &quot;HTML&quot;); Stream&lt;String&gt; filterStream = langList.stream().filter(lang -&gt; lang.equalsIgnoreCase(&quot;java&quot;)); 2) map()用于映射，遍历原stream中的元素，转换后构成新的stream： List&lt;String&gt; langList = Arrays.asList(&quot;Java&quot;, &quot;Python&quot;, &quot;Swift&quot;, &quot;HTML&quot;); Stream&lt;String&gt; mapStream = langList.stream().map(String::toUpperCase); 3) flatMap()用于将[[&quot;ABC&quot;, &quot;DEF&quot;], [&quot;FGH&quot;, &quot;IJK&quot;]]的形式转换为[&quot;ABC&quot;, &quot;DEF&quot;, &quot;FGH&quot;, &quot;IJK&quot;]： Stream&lt;String&gt; cityStream = Stream.of(&quot;Beijing&quot;, &quot;Shanghai&quot;, &quot;Shenzhen&quot;); // [[&apos;B&apos;, &apos;e&apos;, &apos;i&apos;, &apos;j&apos;, &apos;i&apos;, &apos;n&apos;, &apos;g&apos;], [&apos;S&apos;, &apos;h&apos;, &apos;a&apos;, &apos;n&apos;, &apos;g&apos;, &apos;h&apos;, &apos;a&apos;, &apos;i&apos;], ...] Stream&lt;Stream&lt;Character&gt;&gt; characterStream1 = cityStream. map(city -&gt; characterStream(city)); Stream&lt;String&gt; cityStreamCopy = Stream.of(&quot;Beijing&quot;, &quot;Shanghai&quot;, &quot;Shenzhen&quot;); // [&apos;B&apos;, &apos;e&apos;, &apos;i&apos;, &apos;j&apos;, &apos;i&apos;, &apos;n&apos;, &apos;g&apos;, &apos;S&apos;, &apos;h&apos;, &apos;a&apos;, &apos;n&apos;, &apos;g&apos;, &apos;h&apos;, &apos;a&apos;, &apos;i&apos;, ...] Stream&lt;Character&gt; characterStreamCopy = cityStreamCopy. flatMap(city -&gt; characterStream(city)); 其中，characterStream()返回有参数字符串的字符构成的Stream; 4) limit()表示限制stream中元素的数量，skip()表示跳过stream中前几个元素，concat表示将多个stream连接起来，peek()主要用于debug时查看stream中元素的值： Stream&lt;Integer&gt; limitStream = Stream.of(18, 20, 12, 35, 89).sorted().limit(3); Stream&lt;Integer&gt; skipStream = Stream.of(18, 20, 12, 35, 89).sorted(Comparator.reverseOrder()) .skip(1); Stream&lt;Integer&gt; concatStream = Stream.concat(Stream.of(1, 2, 3), Stream.of(4, 5, 6)); concatStream.peek(i -&gt; System.out.println(i)).count(); peek()是intermediate operation，所以后面需要一个terminal operation，如count()才能在输出中看到结果； 5) 有状态的(stateful)转换，即元素之间有依赖关系，如distinct()返回由唯一元素构成的stream，sorted()返回排序后的stream： Stream&lt;String&gt; distinctStream = Stream.of(&quot;Beijing&quot;, &quot;Tianjin&quot;, &quot;Beijing&quot;).distinct(); Stream&lt;String&gt; sortedStream = Stream.of(&quot;Beijing&quot;, &quot;Shanghai&quot;, &quot;Chengdu&quot;) .sorted(Comparator.comparing(String::length).reversed()); 4. Stream reductionreduction就是从stream中取出结果，是terminal operation，因此经过reduction后的stream不能再使用了。 4.1 OptionalOptional表示或者有一个T类型的对象，或者没有值； 1) 创建Optional对象： 直接通过Optional的类方法：of()/empty()/ofNullable()： Optional&lt;Integer&gt; intOpt = Optional.of(10); Optional&lt;String&gt; emptyOpt = Optional.empty(); Optional&lt;Double&gt; doubleOpt = Optional.ofNullable(5.5); 2) 使用Optional对象： 你当然可以这么使用： if (intOpt.isPresent()) { intOpt.get(); } 但是，最好这么使用： doubleOpt.orElse(0.0); doubleOpt.orElseGet(() -&gt; 1.0); doubleOpt.orElseThrow(RuntimeException::new); List&lt;Double&gt; doubleList = new ArrayList&lt;&gt;(); doubleOpt.ifPresent(doubleList::add); map()方法与ifPresent()用法相同，就是多个返回值，flatMap()用于Optional的链式表达： Optional&lt;Boolean&gt; addOk = doubleOpt.map(doubleList::add); Optional.of(4.0).flatMap(num -&gt; Optional.ofNullable(num * 100)) .flatMap(num -&gt; Optional.ofNullable(Math.sqrt(num))); 4.2 简单的reduction主要包含以下操作： findFirst()/findAny()/allMatch/anyMatch()/noneMatch，比如： Optional&lt;String&gt; firstWord = wordStream.filter(s -&gt; s.startsWith(&quot;Y&quot;)).findFirst(); Optional&lt;String&gt; anyWord = wordStream.filter(s -&gt; s.length() &gt; 3).findAny(); wordStream.allMatch(s -&gt; s.length() &gt; 3); wordStream.anyMatch(s -&gt; s.length() &gt; 3); wordStream.noneMatch(s -&gt; s.length() &gt; 3); 4.3 reduce方法1) reduce(accumulator)：参数是一个执行双目运算的Functional Interface，假如这个参数表示的操作为op，stream中的元素为x, y, z, …，则reduce()执行的就是x op y op z ...，所以要求op这个操作具有结合性(associative)，即满足：(x op y) op z = x op (y op z)，满足这个要求的操作主要有：求和、求积、求最大值、求最小值、字符串连接、集合并集和交集等。另外，该函数的返回值是Optional的： Optional&lt;Integer&gt; sum1 = numStream.reduce((x, y) -&gt; x + y); 2) reduce(identity, accumulator)：可以认为第一个参数为默认值，但需要满足identity op x = x，所以对于求和操作，identity的值为0，对于求积操作，identity的值为1。返回值类型是stream元素的类型： Integer sum2 = numStream.reduce(0, Integer::sum); 5. collect结果1) collect()方法： reduce()和collect()的区别是： reduce()的结果是一个值； collect()可以对stream中的元素进行各种处理后，得到stream中元素的值； Collectors接口提供了很方便的创建Collector对象的工厂方法： // collect to Collection Stream.of(&quot;You&quot;, &quot;may&quot;, &quot;assume&quot;).collect(Collectors.toList()); Stream.of(&quot;You&quot;, &quot;may&quot;, &quot;assume&quot;).collect(Collectors.toSet()); Stream.of(&quot;You&quot;, &quot;may&quot;, &quot;assume&quot;).collect(Collectors.toCollection(TreeSet::new)); // join element Stream.of(&quot;You&quot;, &quot;may&quot;, &quot;assume&quot;).collect(Collectors.joining()); Stream.of(&quot;You&quot;, &quot;may&quot;, &quot;assume&quot;).collect(Collectors.joining(&quot;, &quot;)); // summarize element IntSummaryStatistics summary = Stream.of(&quot;You&quot;, &quot;may&quot;, &quot;assume&quot;) .collect(Collectors.summarizingInt(String::length)); summary.getMax(); 2) foreach()方法： foreach()用于遍历stream中的元素，属于terminal operation；forEachOrdered()是按照stream中元素的顺序遍历，也就无法利用并发的优势； Stream.of(&quot;You&quot;, &quot;may&quot;, &quot;assume&quot;, &quot;you&quot;, &quot;can&quot;, &quot;fly&quot;).parallel() .forEach(w -&gt; System.out.println(w)); Stream.of(&quot;You&quot;, &quot;may&quot;, &quot;assume&quot;, &quot;you&quot;, &quot;can&quot;, &quot;fly&quot;) .forEachOrdered(w -&gt; System.out.println(w)); 3) toArray()方法： 得到由stream中的元素得到的数组，默认是Object[]，可以通过参数设置需要结果的类型： Object[] words1 = Stream.of(&quot;You&quot;, &quot;may&quot;, &quot;assume&quot;).toArray(); String[] words2 = Stream.of(&quot;You&quot;, &quot;may&quot;, &quot;assume&quot;).toArray(String[]::new); 4) toMap()方法： toMap: 将stream中的元素映射为的形式，两个参数分别用于生成对应的key和value的值。比如有一个字符串stream，将首字母作为key，字符串值作为value，得到一个map： Stream&lt;String&gt; introStream = Stream. of(&quot;Get started with UICollectionView and the photo library&quot;.split(&quot; &quot;)); Map&lt;String, String&gt; introMap = introStream.collect(Collectors.toMap(s -&gt; s.substring(0, 1), s -&gt; s)); 如果一个key对应多个value，则会抛出异常，需要使用第三个参数设置如何处理冲突，比如仅使用原来的value、使用新的value，或者合并： Stream&lt;String&gt; introStream = Stream.of(&quot;Get started with UICollectionView and the photo library&quot; .split(&quot; &quot;)); Map&lt;Integer, String&gt; introMap2 = introStream.collect(Collectors.toMap(s -&gt; s.length(), s -&gt; s, (existingValue, newValue) -&gt; existingValue)); 如果value是一个集合，即将key对应的所有value放到一个集合中，则需要使用第三个参数，将多个value合并： Stream&lt;String&gt; introStream3 = Stream.of(&quot;Get started with UICollectionView and the photo library&quot; .split(&quot; &quot;)); Map&lt;Integer, Set&lt;String&gt;&gt; introMap3 = introStream3.collect(Collectors.toMap(s -&gt; s.length(), s -&gt; Collections.singleton(s), (existingValue, newValue) -&gt; { HashSet&lt;String&gt; set = new HashSet&lt;&gt;(existingValue); set.addAll(newValue); return set; } )); introMap3.forEach((k, v) -&gt; System.out.println(k + &quot;: &quot; + v)); 如果value是对象自身，则使用Function.identity()，如： Map&lt;Integer, Person&gt; idToPerson = people.collect(Collectors .toMap(Person::getId, Function.identity())); toMap()默认返回的是HashMap，如果需要其它类型的map，比如TreeMap，则可以在第四个参数指定构造方法： Map&lt;Integer, String&gt; introMap2 = introStream.collect( Collectors.toMap(s -&gt; s.length(), s -&gt; s, (existingValue, newValue) -&gt; existingValue, TreeMap::new)); 6. Grouping和Partitioning1) groupingBy()表示根据某一个字段或条件进行分组，返回一个Map，其中key为分组的字段或条件，value默认为list，groupingByConcurrent()是其并发版本： Map&lt;String, List&lt;Locale&gt;&gt; countryToLocaleList = Stream.of(Locale.getAvailableLocales()) .collect(Collectors.groupingBy(l -&gt; l.getDisplayCountry())); 2) 如果groupingBy()分组的依据是一个bool条件，则key的值为true/false，此时与partitioningBy()等价，且partitioningBy()的效率更高： // predicate Map&lt;Boolean, List&lt;Locale&gt;&gt; englishAndOtherLocales = Stream.of(Locale.getAvailableLocales()) .collect(Collectors.groupingBy(l -&gt; l.getDisplayLanguage().equalsIgnoreCase(&quot;English&quot;))); // partitioningBy Map&lt;Boolean, List&lt;Locale&gt;&gt; englishAndOtherLocales2 = Stream.of(Locale.getAvailableLocales()) .collect(Collectors.partitioningBy(l -&gt; l.getDisplayLanguage().equalsIgnoreCase(&quot;English&quot;))); 3) groupingBy()提供第二个参数，表示downstream，即对分组后的value作进一步的处理： 返回set，而不是list： Map&lt;String, Set&lt;Locale&gt;&gt; countryToLocaleSet = Stream.of(Locale.getAvailableLocales()) .collect(Collectors.groupingBy(l -&gt; l.getDisplayCountry(), Collectors.toSet())); 返回value集合中元素的数量： Map&lt;String, Long&gt; countryToLocaleCounts = Stream.of(Locale.getAvailableLocales()) .collect(Collectors.groupingBy(l -&gt; l.getDisplayCountry(), Collectors.counting())); 对value集合中的元素求和： Map&lt;String, Integer&gt; cityToPopulationSum = Stream.of(cities) .collect(Collectors.groupingBy(City::getName, Collectors.summingInt(City::getPopulation))); 对value的某一个字段求最大值，注意value是Optional的： Map&lt;String, Optional&lt;City&gt;&gt; cityToPopulationMax = Stream.of(cities) .collect(Collectors.groupingBy(City::getName, Collectors.maxBy(Comparator.comparing(City::getPopulation)))); 使用mapping对value的字段进行map处理： Map&lt;String, Optional&lt;String&gt;&gt; stateToNameMax = Stream.of(cities) .collect(Collectors.groupingBy(City::getState, Collectors.mapping(City::getName, Collectors.maxBy(Comparator.comparing(String::length))))); Map&lt;String, Set&lt;String&gt;&gt; stateToNameSet = Stream.of(cities) .collect(Collectors.groupingBy(City::getState, Collectors.mapping(City::getName, Collectors.toSet()))); 通过summarizingXXX获取统计结果： Map&lt;String, IntSummaryStatistics&gt; stateToPopulationSummary = Stream.of(cities) .collect(Collectors.groupingBy(City::getState, Collectors.summarizingInt(City::getPopulation))); reducing()可以对结果作更复杂的处理，但是reducing()却并不常用： Map&lt;String, String&gt; stateToNameJoining = Stream.of(cities) .collect(Collectors.groupingBy(City::getState, Collectors.reducing(&quot;&quot;, City::getName, (s, t) -&gt; s.length() == 0 ? t : s + &quot;, &quot; + t))); 比如上例可以通过mapping达到同样的效果： Map&lt;String, String&gt; stateToNameJoining2 = Stream.of(cities) .collect(Collectors.groupingBy(City::getState, Collectors.mapping(City::getName, Collectors.joining(&quot;, &quot;) ))); 7. Primitive StreamStream&lt;Integer&gt;对应的Primitive Stream就是IntStream，类似的还有DoubleStream和LongStream。 1) Primitive Stream的构造：of(), range(), rangeClosed(), Arrays.stream(): IntStream intStream = IntStream.of(10, 20, 30); IntStream zeroToNintyNine = IntStream.range(0, 100); IntStream zeroToHundred = IntStream.rangeClosed(0, 100); double[] nums = {10.0, 20.0, 30.0}; DoubleStream doubleStream = Arrays.stream(nums, 0, 3); 2) Object Stream与Primitive Stream之间的相互转换，通过mapToXXX()和boxed()： // map to Stream&lt;String&gt; cityStream = Stream.of(&quot;Beijing&quot;, &quot;Tianjin&quot;, &quot;Chengdu&quot;); IntStream lengthStream = cityStream.mapToInt(String::length); // box Stream&lt;Integer&gt; oneToNine = IntStream.range(0, 10).boxed(); 3) 与Object Stream相比，Primitive Stream的特点： toArray()方法返回的是对应的Primitive类型： int[] intArr = intStream.toArray(); 自带统计类型的方法，如：max(), average(), summaryStatistics(): OptionalInt maxNum = intStream.max(); IntSummaryStatistics intSummary = intStream.summaryStatistics(); 8. Parallel Stream1) Stream支持并发操作，但需要满足以下几点： 构造一个paralle stream，默认构造的stream是顺序执行的，调用paralle()构造并行的stream： IntStream scoreStream = IntStream.rangeClosed(10, 30).parallel(); 要执行的操作必须是可并行执行的，即并行执行的结果和顺序执行的结果是一致的，而且必须保证stream中执行的操作是线程安全的： int[] wordLength = new int[12]; Stream.of(&quot;It&quot;, &quot;is&quot;, &quot;your&quot;, &quot;responsibility&quot;).parallel().forEach(s -&gt; { if (s.length() &lt; 12) wordLength[s.length()]++; }); 这段程序的问题在于，多线程访问共享数组wordLength，是非线程安全的。解决的思路有：1）构造AtomicInteger数组；2）使用groupingBy()根据length统计； 2) 可以通过并行提高效率的常见场景： 使stream无序：对于distinct()和limit()等方法，如果不关心顺序，则可以使用并行： LongStream.rangeClosed(5, 10).unordered().parallel().limit(3); IntStream.of(14, 15, 15, 14, 12, 81).unordered().parallel().distinct(); 在groupingBy()的操作中，map的合并操作是比较重的，可以通过groupingByConcurrent()来并行处理，不过前提是parallel stream： Stream.of(cities).parallel().collect(Collectors.groupingByConcurrent(City::getState)); 在执行stream操作时不能修改stream对应的collection； stream本身是不存储数据的，数据保存在对应的collection中，所以在执行stream操作的同时修改对应的collection，结果是未定义的： // ok Stream&lt;String&gt; wordStream = wordList.stream(); wordList.add(&quot;number&quot;); wordStream.distinct().count(); // ConcurrentModificationException Stream&lt;String&gt; wordStream = wordList.stream(); wordStream.forEach(s -&gt; { if (s.length() &gt;= 6) wordList.remove(s);}); 9. Functional Interface仅包含一个抽象方法的interface被成为Functional Interface，比如：Predicate, Function, Consumer等。此时我们一般传入一个lambda表达式或Method Reference。 常见的Functional Interface有： Functional Interface Parameter Return Type Description Types Supplier&lt;T&gt; None T Supplies a value of type T Consumer&lt;T&gt; T void Consumes a value of type T BiConsumer&lt;T, U&gt; T,U void Consumes values of types T and U Predicate&lt;T&gt; T boolean A Boolean-valued function ToIntFunction&lt;T&gt; T int An int-, long-, or double-valued function ToLongFunction&lt;T&gt; T long ToDoubleFunction&lt;T&gt; T double IntFunction&lt;R&gt; int R A function with argument of type int, long, or double LongFunction&lt;R&gt; long DoubleFunction&lt;R&gt; double Function&lt;T, R&gt; T R A function with argument of type T BiFunction&lt;T, U, R&gt; T,U R A function with arguments of types T and U UnaryOperator&lt;T&gt; T T A unary operator on the type T BinaryOperator&lt;T&gt; T,T T A binary operator on the type T 参考 Java SE8 for the Really Impatient Stream示例代码","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"java","slug":"java","permalink":"http://nkcoder.github.io/tags/java/"}]},{"title":"Java 8 Lambda表达式入门","slug":"java-8-lambda-expression-guide","date":"2016-01-16T10:03:04.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2016/01/16/java-8-lambda-expression-guide/","link":"","permalink":"http://nkcoder.github.io/2016/01/16/java-8-lambda-expression-guide/","excerpt":"本文链接为：http://nkcoder.github.io/2016/01/16/java-8-lambda-expression-guide/ ，转载请注明出处，谢谢！ 1.1 为什么使用Lambda表达式先看几个例子： 第一个例子，在一个独立的线程中执行某项任务，我们通常这么实现： class Worker implements Runnable { public void run() { for (int i = 0; i &lt; 100; i++) doWork(); } ... } Worker w = new Worker(); new Thread(w).start();","text":"本文链接为：http://nkcoder.github.io/2016/01/16/java-8-lambda-expression-guide/ ，转载请注明出处，谢谢！ 1.1 为什么使用Lambda表达式先看几个例子： 第一个例子，在一个独立的线程中执行某项任务，我们通常这么实现： class Worker implements Runnable { public void run() { for (int i = 0; i &lt; 100; i++) doWork(); } ... } Worker w = new Worker(); new Thread(w).start(); 第二个例子，自定义字符串比较的方法（通过字符串长度），一般这么做： class LengthComparator implements Comparator&lt;String&gt; { public int compare(String first, String second) { return Integer.compare(first.length(), second.length()); } } Arrays.sort(strings, new LengthComparator()); 第三个例子，在JavaFX中，给一个button添加一个callback： button.setOnAction(new EventHandler&lt;ActionEvent&gt;() { public void handle(ActionEvent event) { System.out.println(&quot;Thanks for clicking!&quot;); } }); 这些例子有一个共同点，就是：先定义一段代码块，传给某个对象或方法，然后被执行。在Lambda表达式之前，Java是不允许直接传递代码块的，因为Java是面向对象的，因此必须传递一个对象，将要执行的代码块封装到对象里。 1.2 Lambda表达式的语法将上面第二个例子中的LengthComparator，用Lambda表达式表示为： (String first, String second) -&gt; Integer.compare(first.length(), second.length()); -&gt;前为参数列表，其后为表达式语句体； 如果表达式语句体不止一行，则将语句体写在{}中，与普通的函数一样： (String first, String second) -&gt; { if (first.length() &gt; second.length()) { return 1; } else if (first.length() == second.length()) { return 0; } else { return -1; } }; 如果没有参数，()还是需要带上，比如上面的第一个例子，可以表示为： () -&gt; { for (int i = 0; i &lt; 1000; i ++) { doWork(); } } 如果参数的类型可以从上下文自动推断，则可以省略： Comparator&lt;String&gt; comp = (first, second) // Same as (String first, String second) -&gt; Integer.compare(first.length(), second.length()); 如果参数只有一个，且类型可以自动推断，则小括号()也可以省略： // Instead of (event) -&gt; or (ActionEvent event) -&gt; eventHandler&lt;ActionEvent&gt; listener = event -&gt; System.out.println(&quot;Thanks for clicking!&quot;); lambda表达式的返回值的类型是自动推断的，因此不需要指明；在lambda表达式，某些条件分支中有返回值，而其它分支没有返回值，是不允许的，如： (x) -&gt; { if (x &gt;= 0) { return 1; } } 另外，expression lambda和statement lambda的区别是，expression lambda不需要写return关键字，Java runtime会将表达式的结果作为返回值返回，而statement lambda是写在{}中的表达式，需要使用return关键字，比如： // expression lambda Comparator&lt;String&gt; comp1 = (first, second) -&gt; Integer.compare(first.length(), second.length()); // statement lambda Comparator&lt;String&gt; comp2 = (first, second) -&gt; { return Integer.compare(first.length(), second.length());}; 1.3 Functional Interface如果一个接口（interface）仅有一个抽象方法（abstract method），就称为Functional Interface，比如Runnable、Comparator等。在任何一个需要Functional Interface对象的地方，都可以使用lambda表达式： Arrays.sort(words, (first, second) -&gt; Integer.compare(first.length(), second.length())); 这里，sort()的第二个参数需要的是一个Comparator对象，而Comparator是Functional Interface，因此可以直接传入lambda表达式，在调用该对象的compare()方法时，就是执行该lambda表达式中的语句体； 如果lambda表达式的语句体会抛出异常，则对应的Functional Interface中的抽象方法必须抛出了该异常，否则就需要在lambda表达式中显式捕获异常： Runnable r = () -&gt; { System.out.println(&quot;------&quot;); try { Thread.sleep(10); } catch (InterruptedException e) { // catch exception } }; Callable&lt;String&gt; c = () -&gt; { System.out.println(&quot;--------&quot;); Thread.sleep(10); return &quot;&quot;; }; 1.4 Method Reference如果将lambda表达式的参数作为参数传递给一个方法，他们的执行效果是相同的，则该lambda表达式可以使用Method Reference表达，以下两种方式是等价的： (x) -&gt; System.out.println(x) System.out::println 其中System.out::println被称为Method Reference。 Method Reference主要有三种形式： object::instanceMethod Class::staticMethod Class::instanceMethod 对于前两种方式，对应的lambda表达式的参数和method的参数是一致的，比如： System.out::println (x) -&gt; System.out.println(x) Math::pow (x, y) -&gt; Math.pow(x, y) 对于第三种方式，对应的lambda表达式的语句体中，第一个参数作为对象，调用method，将其它参数作为method的参数，比如： String::compareToIgnoreCase (s1, s2) -&gt; s1.compareToIgnoreCase(s2) 1.5 Constructor ReferenceConstructor Reference与Method Reference类似，只不过是特殊的method：new，具体调用的是哪个构造函数，由上下文环境决定，比如： List&lt;String&gt; labels = ...; Stream&lt;Button&gt; stream = labels.stream().map(Button::new); Button::new等价于(x) -&gt; Button(x)，所以调用的构造函数是：Button(x); 除了创建单个对象，也可以创建对象数组，如下面两种方式等价： int[]::new (x) -&gt; new int[x] 1.6 变量作用域lambd表达式会捕获当前作用域下可用的变量，比如： public void repeatMessage(String text, int count) { Runnable r = () -&gt; { for (int i = 0; i &lt; count; i ++) { System.out.println(text); Thread.yield(); } }; new Thread(r).start(); } 但是这些变量必须是不可变的，为什么呢？看下面这个例子： int matches = 0; for (Path p : files) new Thread(() -&gt; { if (p has some property) matches++; }).start(); // Illegal to mutate matches 因为可变的变量在lambda表达式中不是线程安全的，这和内部类的要求是一致的，内部类中只能引用外部定义的final变量； lambda表达式的作用域与嵌套代码块的作用域是一样的，所以在lambd表达式中的参数名或变量名不能与局部变量冲突，如： Path first = Paths.get(&quot;/usr/bin&quot;); Comparator&lt;String&gt; comp = (first, second) -&gt; Integer.compare(first.length(), second.length()); // Error: Variable first already defined 如果在lambda表达式中引用this变量，则引用的是创建该lambda表达式的方法的this变量，如： public class Application() { public void doWork() { Runnable runner = () -&gt; { ...; System.out.println(this.toString()); ... }; } } 所以这里的this.toString()调用的是Application对象的toString()，而不是Runnable对象的。 1.7 Default Method接口中只能有抽象方法，如果在已有的接口中新增一个方法，则该接口所有的实现类都需要实现该方法。Java 8中引入了Default Method的概念，在接口中新增一个default方法，不会破坏已有的接口规则，接口的实现类可以选择重写或直接继承该default方法，比如： interface Person { long getId(); default String getName() { return &quot;John Q. Public&quot;; } } Java是允许多继承的，如果一个类的父类中定义的方法和接口中定义的default方法完全相同，或者一个类的两个接口中定义了完全相同的方法， 则如何处理这种冲突呢？处理规则如下： 如果是父类和接口的方法冲突：以父类中的方法为准，接口中的方法被忽略； 如果两个接口中的default方法冲突，则需要重写该方法解决冲突； 1.8 Static MethodJava 8之前，接口中只能定义static变量，Java 8开始，接口中可以添加static方法，比如Comparator接口新增了一系列comparingXXX的static方法，比如： public static &lt;T&gt; Comparator&lt;T&gt; comparingInt(ToIntFunction&lt;? super T&gt; keyExtractor) { Objects.requireNonNull(keyExtractor); return (Comparator&lt;T&gt; &amp; Serializable) (c1, c2) -&gt; Integer.compare(keyExtractor.applyAsInt(c1), keyExtractor.applyAsInt(c2)); } 使用这个static方法，以下两种方式也是等价的： Arrays.sort(cities, (first, second) -&gt; Integer.compare(first.length(), second.length())); Arrays.sort(cities, Comparator.comparingInt(String::length)); 所以，以后我们在设计自己的接口时，不需要再定义单独的工具类（如Collections/Collection)，在接口中使用static方法就行了。 参考 Java SE8 for the Really Impatient Lambda示例代码","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"java","slug":"java","permalink":"http://nkcoder.github.io/tags/java/"}]},{"title":"Python Tips 2: String模块与reversed关键字","slug":"python-tips2-string-and-reversed","date":"2016-01-12T11:56:14.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2016/01/12/python-tips2-string-and-reversed/","link":"","permalink":"http://nkcoder.github.io/2016/01/12/python-tips2-string-and-reversed/","excerpt":"1. 使用string模块如果需要构造这样一个dict： &apos;A&apos; -&gt; 1 &apos;B&apos; -&gt; 2 &apos;C&apos; -&gt; 3 ... &apos;Z&apos; -&gt; 26","text":"1. 使用string模块如果需要构造这样一个dict： &apos;A&apos; -&gt; 1 &apos;B&apos; -&gt; 2 &apos;C&apos; -&gt; 3 ... &apos;Z&apos; -&gt; 26 你当然不想写26遍赋值语句，我们知道，内置函数ord()和chr()可以在unicode字符和值之间进行转换，而对于ASCII字符，unicode编码和ASCII编码的值相同，所以，可以这样来实现： ascii_of_A = ord(&apos;A&apos;) ascii_dict = {} for index in range(26): c = chr(ascii_of_A + index) ascii_dict[c] = index + 1 其实，string模块有可以直接拿来用的函数： string.ascii_letters string.ascii_uppercase string.ascii_lowercase 所以，可以这样实现： ascii_dict = {} for index, c in enumerate(string.ascii_uppercase): ascii_dict[c] = index + 1 2. 使用reversed反转列表现在，有个list或一个string，我们想逆序遍历，怎么实现呢？ 最直觉简单的做法就是先拿到列表的长度，然后使用下标索引逆序遍历，像这样： num_list = [1, 2, 3, 4, 5, 6, 7, 8, 9] i = len(num_list) - 1 while i &gt;= 0: print(num_list[i]) i -= 1 python的内置函数reversed()就是用来反转一个列表的（list和string都可以）： for x in reversed(num_list): print(x) 如果在逆序遍历的同时，还希望得到对应的索引呢，使用enumerate(): for i, x in reversed(list(enumerate(num_list))): print(i, x) 其实，除了使用reversed()函数，也可以使用list和string的slicing机制，这里简单介绍一下： a_list[start:end:step] 默认是顺序遍历，且挨个元素遍历，因为step的值默认是1； 遍历的范围是list[start, end)，包含前一个索引，不包含后一个索引； 当start省略的时候，值为0，当end省略的时候，值为list的长度，所以a_list[::]表示整个列表； start和end可以为负数，比如-1表示倒数第一个元素，可以认为此时的索引值为(list长度-1)； step大于0时，表示顺序遍历，step小于0时，表示逆序遍历，所以[::-1]表示逆序遍历列表； 明白了slicing的机制，上面的问题就很容易这样来实现了： for x in num_list[::-1]: print(x) 参考 Built-in Functions¶ Traverse a list in reverse order in Python","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"python","slug":"python","permalink":"http://nkcoder.github.io/tags/python/"}]},{"title":"Python Tips 1: 循环与Enumerate","slug":"python-tips1-loop-and-enumerate","date":"2016-01-10T14:07:42.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2016/01/10/python-tips1-loop-and-enumerate/","link":"","permalink":"http://nkcoder.github.io/2016/01/10/python-tips1-loop-and-enumerate/","excerpt":"开发的时候，经常会有这样的需求：遍历一个集合（比如数组），索引不是从0开始，而是从某一个具体的值开始，在Java或Swift中，可以直接用for循环实现： Java代码： for (int i = 3; i &lt; dataList.size(); i++) { // do someting with dataList[i] }","text":"开发的时候，经常会有这样的需求：遍历一个集合（比如数组），索引不是从0开始，而是从某一个具体的值开始，在Java或Swift中，可以直接用for循环实现： Java代码： for (int i = 3; i &lt; dataList.size(); i++) { // do someting with dataList[i] } Swift代码： for var i = 3; i &lt; dataList.count; i++ { // do someting with dataList[i] } for i in 3 ..&lt; dataList.count { // do someting with dataList[i] } 但是python的for循环没有类似的形式，当然可以用while循环实现，但总感觉不够简洁： i = 3 while i &lt; len(data_list): // do someting with data_list[i] i += 1 借助range()函数，使用for...in...循环也是可以实现的： for i in range(3, len(data_list)): // do someting with data_list[i] 另外一种实现，可以使用enumerate()方法，起始索引通过参数设置： i = 3 for (i, value) in enumerate(data_list[i:], start=i): // do someting with value 这里需要说明一下的是，enumerate()的第一个参数表示的数组，必须和第二个参数表示的索引是对应的，如果是enumerate(data_list, start=i)，那么i的值从3开始，而value的值还是从data_list[0]开始的，而不是从data_list[3]开始的。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"python","slug":"python","permalink":"http://nkcoder.github.io/tags/python/"}]},{"title":"python学习笔记之Native Datatypes","slug":"python3-learn-note-datatypes","date":"2015-12-25T15:43:17.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2015/12/25/python3-learn-note-datatypes/","link":"","permalink":"http://nkcoder.github.io/2015/12/25/python3-learn-note-datatypes/","excerpt":"本文来自Dive Into Python 3-Native Datatypes的学习笔记，记录重点，用于加深印象。 所有变量都是有数据类型，但是不需要声明，python会在第一次赋值时确定变量的类型； 1.1 Boolean True/False","text":"本文来自Dive Into Python 3-Native Datatypes的学习笔记，记录重点，用于加深印象。 所有变量都是有数据类型，但是不需要声明，python会在第一次赋值时确定变量的类型； 1.1 Boolean True/False 1.2 Number isinstance(1, int)方法可以用来确定变量的类型； 整数和浮点数可以通过int(float1), float(int1)进行转换； 运算符：/, //, %, **; 分数：fractions.Fraction(1, 3); 三角函数：math.pi; math.sin(2); 条件判断：0(包括0.0, Fraction(0, n))-&gt; False；非0 -&gt; True； 注意：python没有运算符++/–，可以通过num += 1实现自增； 1.3 List list1 = []; list2 = [2, &#39;a&#39;]; 子list：list2[0:3]; list2[:-1]; list2[1:]; list2[:]; 增加元素：+操作符；append(&#39;a&#39;); extend([2, 3, 4]); insert(0, &#39;a&#39;); 查找元素：in操作；index(&#39;b&#39;); len(list1); 删除元素：del list1[1]; list1.remove(&#39;a&#39;); list1.pop(); remove()方法仅会删除第一个元素； 判断list是否为空：if not list1: 条件判断：空list（[]） -&gt; False；非空list -&gt; True; 将list作为queue使用，可以通过append(x); pop(0)实现;但是当数据量较大时，通过collections.deque()效率会更高一些：append(x); popleft() 1.4 Tuple tuple1(); tuple2(3,);(只有一个元素时必须加逗号)；tuple3(‘a’, ‘b’, 3); 可以看成是不可变的list；tuple比list更快； list和tuple可以通过tuple(list1)和list(tuple1)相互转换； 查找：in操作；len(set1); 条件判断：空tuple(()) -&gt; False; 非空tuple -&gt; True; 1.5 Set set1 = {1, 2}; 创建空set：set1 = set(); 而set1 = {}创建的是一个空dict； set和list可以通过set(list1), list(set1)相互转换； 修改：add()/update()/discard()/remove()/clear(); 集合操作: union()/intersection()/difference()/symmetric_difference()/issubset()/issuperset(); 查找：in操作；len(set1); 条件判断：空set（set()) -&gt; False, 非空set -&gt; True 1.6 dictionary dict1 = {}; dict2 = {&#39;key1&#39;: &#39;value1&#39;} dict支持in操作，判断key是否存在； dict的key和value都可以是任意类型； 方法：len(); 空dict({}) -&gt; False, 非空dict -&gt; True 1.7 None None是python的null，仅与None比较时为True，与其它任意值比较都为False； 在条件判断时，None -&gt; False; not None -&gt; True;","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"python","slug":"python","permalink":"http://nkcoder.github.io/tags/python/"}]},{"title":"移动社交App服务端开发总结","slug":"social-app-backend-tech-stack","date":"2015-12-24T07:15:03.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2015/12/24/social-app-backend-tech-stack/","link":"","permalink":"http://nkcoder.github.io/2015/12/24/social-app-backend-tech-stack/","excerpt":"目前在一创业公司，开发一款移动社交app，作为服务端首席码农（服务端就我一个人），也算是见证了服务端从无到有的全过程。客户端从最初上线至今5个月以来，迭代了8个版本，服务端都比较稳定（当然， 这跟我们处于发展初期，用户量和数据量不大有很大关系）。在这里分享一下，和大家交流一下，本文不谈具体的技术实现，主要谈技术栈和一些感受。由于技术能力和视野的欠缺，不足之处，欢迎指点。","text":"目前在一创业公司，开发一款移动社交app，作为服务端首席码农（服务端就我一个人），也算是见证了服务端从无到有的全过程。客户端从最初上线至今5个月以来，迭代了8个版本，服务端都比较稳定（当然， 这跟我们处于发展初期，用户量和数据量不大有很大关系）。在这里分享一下，和大家交流一下，本文不谈具体的技术实现，主要谈技术栈和一些感受。由于技术能力和视野的欠缺，不足之处，欢迎指点。 1. 云服务1.1 阿里云服务器用了2台ECS，做高可用，ECS前用nginx做负载均衡，当然，也可以使用阿里云提供的负载均衡服务，不过那是要花银子的，创业初期，能省就省了，不过ngingx服务也是蛮稳定的。（2016-02-26更新：负载均衡已更改为阿里云的LB，每天差不多0.5元，但是LB的状态监控不太准确，一会儿显示异常，一会儿显示正常，异常的原因不具体，根据异常的原因去查或报工单，完全对应不上！） 由于我们的产品面向海外用户，所以使用的是阿里云在洛杉矶的服务器，海外访问的速度应该还可以，国内稍差点，导致阿里云的监控经常超时报警。2台ECS的配置均为：2核/8G/4Mbps。 数据库用了1台RDS MySQL，可靠性和可用性由阿里云保障，自己只要针对性地做一些参数调整和优化即可，也没什么运维成本。RDS的配置为：1200M内存/50G空间/300最大连接数/600最大IOPS。 测试环境用了1台单独的ECS，然后在上面安装了MySQL。 总体来讲，阿里云提供的技术服务还是不错的，至于售后服务，至今没打过交道，无法评价。 2016-06-02更新：补充一下阿里云的工单服务，处理地比较及时，一般半个小时内会有相应的技术对接。但是，处理结果并不总是让人满意！ 1.2 环信社交产品肯定少不了消息模块，我们用的是环信。其实我是被选择了环信，因为我接手的时候，客户端已经基于环信开发了。其实对消息服务提供商，我个人印象最好的是LeanCloud，这里就多说两句（有点广告的嫌疑了，哈哈），对这个创业公司最初的印象来自于他们的创始团队，创始人来自Google，很有极客范，后来了解了一下他们的产品，感觉他们的API和Demo比环信的专业和规范太多了，大家可以自己去比较，我打算以后有机会体验一下他家的一站式后端服务。 说回环信，功能上没啥问题，该有的都有。这里说几点感受：1. 接口文档不全：有一次，我需要一个接口，很常见的需求，文档上没有（确实没有），我问了一下技术咨询，他说有啊，然后发过来一个接口示例，我想，如果写在文档上，那不是节省了双方的时间么！2. 技术客服有的很专业，有的很水，经常是问一个问题，客服转来转去的；3.环信没有部署海外节点，所以海外服务的质量一般，消息存在延时的问题。 1.3 七牛我们的图片存在七牛上，七牛提供各种图片处理方式，是很方便的。 我们的产品面向海外，晒图又是主要功能之一，所以为了优化用户体验，选择部署了海外节点的服务商是重要考量。目前来看，基本没得选，就七牛可以。但是七牛海外加速下载的费用非常高，是国内下载流量的5倍/G，创业公司伤不起哇！ 另外，10月8日的故障（官方说是电缆被挖断）影响甚广，我们也是受害者，当时app内的图片基本全部无法显示（技术客服说有CDN缓存，但是并没有什么卵用），故障持续了差不多一个半小时，后来赔付方案出炉，一共赔了我们7.6元，还是百倍赔付哦，我们都表示七牛好大方！ 我觉得七牛的文档和示例都很一般，举个例子，服务端通过七牛的接口上传下载图片经常超时报错，我找了技术客服很多次，各种日志截图，各种解释，他们最后告诉我海外上传下载有单独的域名和ip，但是文档上一字不提，跟别提配置方法了。关于七牛的客服，我有个经验，就是通过QQ交流时，他们爱搭不理，但是如果提交工单，回复会很及时。 对七牛的服务，印象并不算太好，只是现在没有更好地解决方案。 2016-02-26更新：现在还一直在使用七牛图片存储，从2016-01-28开始，七牛海外CDN服务降价了，从原来的1.5元/G，降到现在的0.39元/G，对我们面向海外的创业公司而言，是个大大的利好消息，因为每月开销是原来的1/4了。 2. 技术栈2.1 SpringMVC + Spring + MyBatis典型的Java Web结构，通过SpringMVC提供API接口与客户端交互，使用Spring处理业务逻辑，使用MyBatis处理数据逻辑。 虽然前期数据量不大，但是功能模块比较多，导致数据库的表也比较多。目前的功能主要分为两大块：社交和社区。社交模块已经有50多个表了，所以我提前进行了分库处理，将新增的社区模块放到新库，由于数据量不大，所以目前没有分表的需求。 分库可以通过MyBatis实现，在定义MapperScannerConfigurer的bean时，通过basePackage设置不同的package访问不同的数据库。 另外，项目使用logback打印日志，通过git进行版本管理，使用gradle构建，使用jetty部署项目。gradle可以实现类似于Maven的profile功能，将正式环境和测试环境的配置隔离开。 2.2 quartz定时任务，用quartz实现的，开启了quartz的集群功能和持久化功能。因为项目会在两台服务器上单独部署，quartz刚好可以构成集群；同时，将任务调度信息持久化到MySQL中，则不会因为服务器的重启或宕机导致定时任务的重复调度或错过调度。 关于quartz的使用，可以参考我之前的博文Quartz教程系列和在github上的小项目。 2.3 ElasticSearchapp具有基于用户的个人信息进行多维度的个性化推荐的功能，维度包括：位置、性别、年龄、身高、星座、国家、家乡等等，其中最主要的还是地理位置。我们使用的RDS的MySQL版本是5.6， 我们知道MySQL在5.7之前，对空间数据（Spatial Data）的支持是不太友好的，所以其它维度还好，而基于地理位置的查询，使用MySQL来实现是比较麻烦的，而且效率不高。 后来，我就决定不通过MySQL实现，采用NoSQL来做，ElasticSearch和MongoDB都支持空间索引，而且我之前也都有过一些了解，最后选择ElasticSearch的主要原因是，考虑到app以后可能会增加搜索功能，而这可是ElasticSearch的强项啊。 然后我就用两台ECS，搭建了一个ElasticSearch集群，创建一个index和一个type，然后将用户的基本信息从MySQL同步过来，将不同的维度作为查询条件，就实现了一个简单的推荐系统。 ElasticSearch集群运行非常稳定，目前偶尔会接到内存达到阀值的报警，主要是由于搜索时需要排除之前已经推荐过的用户，导致内存有时会飙到80%，等腾出时间了想办法优化一下。 2.4 Scrapy在推广前期，有些模块是没有数据的，所以需要从网络上爬取一些相关数据。爬虫使用的是python的Scrapy框架，使用起来比较简单，目前可以实现不登录爬取、登录爬取，但是具有复杂验证码验证的登录爬取要麻烦，还没有时间去研究。 关于Scrapy爬虫入门，可以参考我之前的博文： 搭建Scrapy爬虫的开发环境； Scrapy爬虫入门实例。 2.5 Django还有一个后台管理系统，使用python的Django框架写的，是让一个朋友在业余时间帮忙开发的，我基本没有参与。 Django不个不错的CMS框架，适合快速开发。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://nkcoder.github.io/tags/elasticsearch/"},{"name":"quartz","slug":"quartz","permalink":"http://nkcoder.github.io/tags/quartz/"},{"name":"redis","slug":"redis","permalink":"http://nkcoder.github.io/tags/redis/"}]},{"title":"Swift的guard语句","slug":"swift-guard-statement","date":"2015-12-18T00:15:33.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2015/12/18/swift-guard-statement/","link":"","permalink":"http://nkcoder.github.io/2015/12/18/swift-guard-statement/","excerpt":"Swift 2引入了guard关键字，特性可以概括为以下两点： 仅关注需要满足的条件，一旦条件不满足，快速返回（fast return）； 对于optional的变量，使用guard let解绑后，在包含guard的作用域内（guard语句已结束），该解绑后的值仍然可用；","text":"Swift 2引入了guard关键字，特性可以概括为以下两点： 仅关注需要满足的条件，一旦条件不满足，快速返回（fast return）； 对于optional的变量，使用guard let解绑后，在包含guard的作用域内（guard语句已结束），该解绑后的值仍然可用； 1. 失败快速返回使用if语句，如果关注点是需要满足的条件，则： if firstName != &quot;&quot; &amp;&amp; lastName != &quot;&quot; { // do stuff with firstName and lastName } else { // fast return } 如果条件满足后要执行的逻辑很长，则当条件不满足时，无法快速返回； 如果失败快速返回，则关注点是不满足的条件，如果不满足的条件很多，则这是不合理的： if firstName == &quot;&quot; || lastName == &quot;&quot; { // fast return } else { // do stuff with firstName and lastName } 使用guard语句，关注需要满足的条件，不满足快速返回： guard firstName != &quot;&quot; &amp;&amp; lastName != &quot;&quot; else { // fast return } // do stuff with firstName and lastName 2. optional解绑使用if let解绑optional变量，在if作用域外，解绑的值是不可用的： func checkAge(ageOfUser:Int?) { if let age = ageOfUser where age &gt; 17 { // do stuff with age } // cannot use age anymore // print(&quot;you age is: \\(age), continue.&quot;) } 使用guard let解绑optional变量后，在包含guard的作用域内，解绑后的值是可用的： func checkAge(ageOfUser: Int?) { guard let age = ageOfUser where age &gt; 17 else { // fast return return } // do stuff with age print(&quot;you age is: \\(age), continue.&quot;) } 鉴于guard语句带来的好处，如果可以，尽量使用guard语句。 参考 Statements The guard keyword in Swift 2: early returns made easy","categories":[{"name":"iOS","slug":"iOS","permalink":"http://nkcoder.github.io/categories/iOS/"}],"tags":[{"name":"Swift","slug":"Swift","permalink":"http://nkcoder.github.io/tags/Swift/"}]},{"title":"Scrapy爬虫入门实例","slug":"scrapy-crawl-simple-in-action","date":"2015-12-10T08:20:48.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2015/12/10/scrapy-crawl-simple-in-action/","link":"","permalink":"http://nkcoder.github.io/2015/12/10/scrapy-crawl-simple-in-action/","excerpt":"在搭建好了Scrapy的开发环境后（如果配置过程中遇到问题，请参考上一篇文章搭建Scrapy爬虫的开发环境，或者在博客里留言），我们开始演示爬取实例。 我们试图爬取论坛-东京版的主题贴。该网站需要登录后才能查看帖子附带的大图，适合演示登录过程。","text":"在搭建好了Scrapy的开发环境后（如果配置过程中遇到问题，请参考上一篇文章搭建Scrapy爬虫的开发环境，或者在博客里留言），我们开始演示爬取实例。 我们试图爬取论坛-东京版的主题贴。该网站需要登录后才能查看帖子附带的大图，适合演示登录过程。 1. 定义item我们需要保存标题、帖子详情、帖子详情的url、图片列表，所以定义item如下： class RentItem(scrapy.Item): &quot;&quot;&quot;item类&quot;&quot;&quot; title = scrapy.Field() # 标题 rent_desc = scrapy.Field() # 描述 url = scrapy.Field() # 详情的url pic_list = scrapy.Field() # 图片列表 2. 使用FormRequest模拟登录首先我们需要分析页面，找到登录的form，以及需要提交的数据（用Fiddler或Firebug分析请求即可），然后使用Scrapy提供FormRequest.from_response()模拟页面的登录过程，主要代码如下： # 需要登录，使用FormRequest.from_response模拟登录 if &quot;id=&apos;lsform&apos;&quot; in response.body: logging.info(&quot;in parse, need to login, url: {0}&quot;.format(response.url)) form_data = { &quot;handlekey&quot;: &quot;ls&quot;, &quot;quickforward&quot;: &quot;yes&quot;, &quot;username&quot;: &quot;loginname&quot;, &quot;password&quot;: &quot;passwd&quot; } request = FormRequest.from_response( response=response, headers=self.headers, formxpath=&quot;//form[contains(@id, &apos;lsform&apos;)]&quot;, formdata=form_data, callback=self.parse_list ) else: logging.info(&quot;in parse, NOT need to login, url: {0}&quot; .format(response.url)) request = Request(url=response.url, headers=self.headers, callback=self.parse_list, ) 如果请求的页面需要登录，则通过xpath定位到对应的form，将登录需要的数据作为参数，提交登录，在callback对应的回调方法里，处理登录成功后的爬取逻辑。 3. 使用XPath提取页面数据Scrapy使用XPath或CSS表达式分析页面结构，由基于lxml的Selector提取数据。XPath或者CSS都可以，另外BeautifulSoup分析HTML/XML文件非常方便，这里采用XPath分析页面，请参考zvon-XPath 1.0 Tutorial，示例丰富且易懂，看完这个入门教程，常见的爬取需求基本都能满足。我这里简单解释一下几个重要的点： /表示绝对路径，即匹配从根节点开始，./表示当前路径，//表示匹配任意开始节点； *是通配符，可以匹配任意节点； 在一个节点上使用[]，如果是数字n表示匹配第n个element，如果是@表示匹配属性，还可以使用函数，比如常用的contains()表示包含，starts-with()表示字符串起始匹配等。 在取节点的值时，text()只是取该节点下的值，而不会取该节点的子节点的值，而.则会取包括子节点在内的所有值，比如： &lt;div&gt;Welcome to &lt;strong&gt;Chengdu&lt;/strong&gt;&lt;/div&gt; sel.xpath(&quot;div/text()&quot;) // Welcome to sel.xpath(&quot;div&quot;).xpath(&quot;string(.)&quot;) // Welcome to Chengdu 4. 不同的spider使用不同的pipeline我们可能有很多的spider，不同的spider爬取的数据的结构不一样，对应的存储格式也不尽相同，因此我们会定义多个pipeline，让不同的spider使用不同的pipeline。 首先我们需要定义一个decorator，表示如果spider的pipeline属性中包含了添加该注解的pipeline，则执行该pipeline，否则跳过该pipeline： def check_spider_pipeline(process_item_method): &quot;&quot;&quot;该注解用在pipeline上 :param process_item_method: :return: &quot;&quot;&quot; @functools.wraps(process_item_method) def wrapper(self, item, spider): # message template for debugging msg = &quot;{1} {0} pipeline step&quot;.format(self.__class__.__name__) # if class is in the spider&quot;s pipeline, then use the # process_item method normally. if self.__class__ in spider.pipeline: logging.info(msg.format(&quot;executing&quot;)) return process_item_method(self, item, spider) # otherwise, just return the untouched item (skip this step in # the pipeline) else: logging.info(msg.format(&quot;skipping&quot;)) return item return wrapper 然后，我们还需要在所有pipeline类的回调方法process_item()上添加该decrator注解： @check_spider_pipeline def process_item(self, item, spider): 最后，在spider类中添加一个数组属性pipeline，里面是所有与该spider对应的pipeline，比如： # 应该交给哪个pipeline去处理 pipeline = set([ pipelines.RentMySQLPipeline, ]) 5. 将爬取的数据保存到mysql数据存储的逻辑在pipeline中实现，可以使用twisted adbapi以线程池的方式与数据库交互。首先从setttings中加载mysql配置： @classmethod def from_settings(cls, settings): &quot;&quot;&quot;加载mysql配置&quot;&quot;&quot; dbargs = dict( host=settings[&quot;MYSQL_HOST&quot;], db=settings[&quot;MYSQL_DBNAME&quot;], user=settings[&quot;MYSQL_USER&quot;], passwd=settings[&quot;MYSQL_PASSWD&quot;], charset=&quot;utf8&quot;, use_unicode=True ) dbpool = adbapi.ConnectionPool(&quot;MySQLdb&quot;, **dbargs) return cls(dbpool) 然后在回调方法process_item中使用dbpool保存数据到mysql： @check_spider_pipeline def process_item(self, item, spider): &quot;&quot;&quot;pipeline的回调. 注解用于pipeline与spider之间的对应，只有spider注册了该pipeline，pipeline才 会被执行 &quot;&quot;&quot; # run db query in the thread pool，在独立的线程中执行 deferred = self.dbpool.runInteraction(self._do_upsert, item, spider) deferred.addErrback(self._handle_error, item, spider) # 当_do_upsert方法执行完毕，执行以下回调 deferred.addCallback(self._get_id_by_guid) # at the end, return the item in case of success or failure # deferred.addBoth(lambda _: item) # return the deferred instead the item. This makes the engine to # process next item (according to CONCURRENT_ITEMS setting) after this # operation (deferred) has finished. time.sleep(10) return deferred 6. 将图片保存到七牛云查看七牛的python接口即可，这里要说明的是，上传图片的时候，不要使用BucketManager的bucket.fetch()接口，因为经常上传失败，建议使用put_data()接口，比如： def upload(self, file_data, key): &quot;&quot;&quot;通过二进制流上传文件 :param file_data: 二进制数据 :param key: key :return: &quot;&quot;&quot; try: token = self.auth.upload_token(QINIU_DEFAULT_BUCKET) ret, info = put_data(token, key, file_data) except Exception as e: logging.error(&quot;upload error, key: {0}, exception: {1}&quot; .format(key, e)) if info.status_code == 200: logging.info(&quot;upload data to qiniu ok, key: {0}&quot;.format(key)) return True else: logging.error(&quot;upload data to qiniu error, key: {0}&quot;.format(key)) return False 7. 项目部署部署可以使用scrapyd和scrapyd-client。首先安装： $ pip install scrapyd $ pip install scrapyd-client 启动scrapyd: $ sudo scrapyd &amp; 修改部署的配置文件scrapy.cfg: [settings] default = scrapy_start.settings [deploy:dev] url = http://localhost:6800/ project = scrapy_start 其中dev表示target，scrapy_start表示project，部署即可： $ scrapyd-deploy dev -p scrapy_start ok，这篇入门实例的重点就这么多，项目的源码在gitlab。 参考 Scrapy 1.0 documentation XPath 1.0 Tutorial How can I use different pipelines for different spiders in a single Scrapy project","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://nkcoder.github.io/tags/Scrapy/"}]},{"title":"记App Store审核被拒的经历","slug":"ios-app-review-reject-process","date":"2015-11-24T15:13:37.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2015/11/24/ios-app-review-reject-process/","link":"","permalink":"http://nkcoder.github.io/2015/11/24/ios-app-review-reject-process/","excerpt":"我们的app最近一次上架的过程颇费周折，被拒了两次。我试图还原整个过程，包括被拒的原因以及我与App Review Team的沟通内容，将本文作为经验教训给看到的朋友一个参考。","text":"我们的app最近一次上架的过程颇费周折，被拒了两次。我试图还原整个过程，包括被拒的原因以及我与App Review Team的沟通内容，将本文作为经验教训给看到的朋友一个参考。 1. 第一次被拒我们的app是Oct 31, 2015 at 6:34 PM提交的，第一次审核结果出来是Nov 10, 2015 at 7:56 AM，差不多9天。被拒的原因如下： 被拒的原因说得很详细，就是我们的app描述文字里出现了逼格关键字，app的启动页中有一张图片上出现了逗逼的关键字。 相关的文字是： 3. 晒图大厅，一个涵盖了世界的逼格大秀场！ 4. 社区，整合了大部分的海外租房、二手信息，以及海归招聘信息！ 涉及的图片是： 将文字和图片修改后，需要重新打包提交（因为启动页图片修改了），所以重新排队Waiting For Review。这是我们自己的错误，所以没什么可抱怨的。 反思：我们这次上线的是V2.0.0版本，是一个大版本，有很多的功能改进和优化，所以介绍性文字和图片有一些调整，但是在提交审核的时候又没有引起足够的重视，一方面可能是之前的提交太顺利了（V2.0.0之前共提交了7个版本，每次都是一次过，每次审核都没有超过7天），导致这次有点大意，另一方面确实是我自己的疏忽，提交之前没有仔细检查。所以，这样的教训也提醒大家，在提交有较多更新的大版本时，最好把更新的部分好好检查一下。 2. 第二次被拒Nov 10, 2015 at 7:56 AM第二次提交后，Nov 18, 2015 at 6:39 AM有了结果，不过还是被拒：Metadata Rejected： 被拒的原因是我们提供的演示帐号失效了。我测试了一下，果然如此！！！因为提供的演示帐号带有test关键字，肯定是哪个后台管理员（我们得好好聊聊了）误操作把它清除了，真是自作孽，不可活！ 含着悲愤的心情，准备好演示帐号后，我在想接下来该怎么办？因为按照被拒的提示，我只需要提供需要的信息，保存，重新提交，review过程会继续，但是网上也有很多朋友说，重新提交就意味着重新排队！最后我还是选择了相信官方的提示，保存，然后重新提交， App的状态变成了Waiting For Review。 反思：演示帐号失效，应该是最不应该犯的一个低级错误了。我自己吸取的教训就是：1）演示帐号不要带有test等可被忽略的关键字；2）以后提交审核时，所有被审核的内容都应该重新测试，检查其有效性，包括演示帐号、app描述、app支持URL等；3）确保线上数据可控，以免被随意修改。 3. 与App Review Team的沟通重新提交之后，我忐忑不安地等待了一天半，但是App的状态一直没变。网上有一些朋友的经验是：Metadata Rejected的情况下一般很快就会进入In Review的状态，有的是立即，有的是两三个小时。那我这个不太正常啊，不会是重新进入排队了吧？！于是我试着发信询问，选择的是get the status of my app，询问的内容为： Hi App Review Team: Yesterday morning, I got a message of &quot;Metadata Rejected&quot; for my app. According to the message, I just need to provide some information(new binary is not required), and save it and then &quot;Submit for Review&quot;, you can continue the review. I did that exactly. The status of my app is still &quot;Waiting for review&quot; after one and a half days. I just want to know that if you will continue the review or I have to wait for about one week, so that I can prepare for it. It&apos;s very kind of you to give some feedback! 第二天就收到了邮件回复，内容如下： 他说我reject了app，意思就是修改了上传的程序包并重新提交了！天啊，冤枉啊！于是我觉得有必要再发信澄清一下，这次选择了get clarification on an app rejection，询问的内容为： Dear App Review Team: Thanks for your replay, but sorry I have to appeal for my app review. You reply to me that: &quot;We noticed that you have rejected your app on at least one occasion.&quot;. but I didn&apos;t. &quot;Metadata Rejected&quot; messages ask me to do the following: - Login to iTunes Connect - Click on “My Apps” - Select your app - Click on the app version on the left side of the screen - Scroll down to “App Review Information” - Provide information in “Demo Account” and/or “Notes” as appropriate - Click “Save” - Once you’ve completed all changes, click the “Submit for Review” button at the top of the App version information page. While your iTunes Connect Application State shows as Metadata Rejected, we don&apos;t require a new binary to correct this issue. Once this information is available, we can continue your review. And I did exactly following the above steps, I promise. I didn&apos;t reject or update the binary, it doesn&apos;t make sense because it&apos;s not necessary. I don&apos;t think it&apos;s fair to add our app to the end of the review queue. I&apos;m really confused, please help to clarify, thank you! 我的意思就是我只是更新了一些信息，并没有重新提交程序包（后来想到，程序包的上传是有个时间戳的，那个完全可以证明我没有重新上传呀，只是当时没想到这一点）。还是第二天，不是邮件回复，而是Itunes Connect中的Resolution Center中有回复，内容如下： 意思是说，由于我重新提交了，为了尽快审核，他们需要关闭当前的申诉会话，继续审核；如果我执意申诉，他们就会取消审核过程！好吧，惹不起，那我保持沉默，麻烦您尽快审核吧！ 过了差不多4个小时，也是在当天，状态变成了In Review，第二天早上，状态成为了Pending Developer Release： 其中，从18号到21号那几天就是与App Review Team的沟通耗时。 至此，我终于松了一口气！这个版本，从第一次提交，到最后上线，花了整整21天，这是我们预先没有想到的，因此对整体的规划有一定的影响。 反思：app被拒时，最好按照官方的提示进行修改，因为每个人被拒的情形都不太一样，所以别人的经验不一定适合，即使后面遇到问题需要沟通，这个也是可以作为证据的嘛。另外，如果对审核的过程不理解、不明白，可以直接与App Review Team沟通，他们的回复还是挺及时有效的。我想，如果我没有发信询问，也许在第二次被拒后，还需要再等一周也说不定呢！","categories":[{"name":"iOS","slug":"iOS","permalink":"http://nkcoder.github.io/categories/iOS/"}],"tags":[{"name":"app-review","slug":"app-review","permalink":"http://nkcoder.github.io/tags/app-review/"}]},{"title":"搭建Scrapy爬虫的开发环境","slug":"Scrapy-crawl-intro-install-and-config","date":"2015-11-17T13:33:22.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2015/11/17/Scrapy-crawl-intro-install-and-config/","link":"","permalink":"http://nkcoder.github.io/2015/11/17/Scrapy-crawl-intro-install-and-config/","excerpt":"这一章主要介绍Scrapy的安装、安装过程中可能遇到的问题以及解决方式。由于我在Mac和Ubuntu环境下都尝试过，所以会将两个平台上遇到的问题都记下来以供参考。 在安装Scrapy之前，首先需要安装以下组件： python 2.7 pip lxml openssl 接下来分别介绍。","text":"这一章主要介绍Scrapy的安装、安装过程中可能遇到的问题以及解决方式。由于我在Mac和Ubuntu环境下都尝试过，所以会将两个平台上遇到的问题都记下来以供参考。 在安装Scrapy之前，首先需要安装以下组件： python 2.7 pip lxml openssl 接下来分别介绍。 1. 安装python 2.7目前Scrapy 1.x仅支持python2.x（官方说以后会支持python 3.x，但目前不支持）。一般系统都预装了python，可以通过-V命令查看版本： GuoDaniel:~ nkcoder$ python -V Python 2.7.10 当然你可以直接使用系统的python，但更好地做法是通过virtualenv虚拟化一个python环境，与系统的python隔离，避免依赖冲突等问题。 安装virtualenv： $ [sudo] pip install virtualenv 创建一个python虚拟环境： GuoDaniel:start_scrapy nkcoder$ virtualenv startenv New python executable in /Users/nkcoder/Projects/python/start_scrapy/startenv/bin/python Installing setuptools, pip, wheel...done. GuoDaniel:start_scrapy nkcoder$ source startenv/bin/activate (startenv) GuoDaniel:start_scrapy nkcoder$ ls startenv (startenv) GuoDaniel:start_scrapy nkcoder$ which pip /Users/nkcoder/Projects/python/start_scrapy/startenv/bin/pip (startenv) GuoDaniel:start_scrapy nkcoder$ python -V Python 2.7.10 virtualenv也可以指定python解释器，默认使用PATH定义的python解释器。比如创建一个python 3的虚拟环境： GuoDaniel:start_scrapy nkcoder$ virtualenv -p /Library/Frameworks/Python.framework/Versions/3.5/bin/python3 python3env Running virtualenv with interpreter /Library/Frameworks/Python.framework/Versions/3.5/bin/python3 Using base prefix &apos;/Library/Frameworks/Python.framework/Versions/3.5&apos; New python executable in /Users/nkcoder/Projects/python/start_scrapy/python3env/bin/python3 Also creating executable in /Users/nkcoder/Projects/python/start_scrapy/python3env/bin/python Installing setuptools, pip, wheel...done. GuoDaniel:start_scrapy nkcoder$ source python3env/bin/activate (python3env) GuoDaniel:start_scrapy nkcoder$ which python /Users/nkcoder/Projects/python/start_scrapy/python3env/bin/python (python3env) GuoDaniel:start_scrapy nkcoder$ python -V Python 3.5.0 参考：VirtualEnv Installation 2. 安装pip通过virtualenv安装的python，默认已经安装了对应版本的pip，查看pip版本： GuoDaniel:start_scrapy nkcoder$ source startenv/bin/activate (startenv) GuoDaniel:start_scrapy nkcoder$ which pip /Users/nkcoder/Projects/python/start_scrapy/startenv/bin/pip (startenv) GuoDaniel:start_scrapy nkcoder$ pip -V pip 7.1.2 from /Users/nkcoder/Projects/python/start_scrapy/startenv/lib/python2.7/site-packages (python 2.7) 如果需要自己安装pip，也很简单，首先下载get-pip.py脚本，然后安装： $ python get-pip.py 参考：Pip Installation 3. 安装lxml通过pip安装lxml： $ sudo pip install lxml 在Mac环境下安装lxml可能会遇到以下错误： In file included from src/lxml/lxml.etree.c:314: /private/tmp/pip_build_root/lxml/src/lxml/includes/etree_defs.h:9:10: fatal error: &apos;libxml/xmlversion.h&apos; file not found #include &quot;libxml/xmlversion.h&quot; ^ 1 error generated. error: command &apos;cc&apos; failed with exit status 1 安装或更新xcode-select即可： $ xcode-select --install 参考：Cannot install Lxml on Mac os x 10.9 在Ubuntu环境下可能会遇到以下错误： libxml/xmlversion.h: No such file or directory compilation terminated. 需要安装相应的dev包： $ sudo apt-get install libxml2-dev libxslt1-dev python-dev 参考：how-to-install-lxml-on-ubuntu 4. 安装openssl系统一般都预装有openssl： $ openssl version OpenSSL 0.9.8zg 14 July 2015 5. 安装Scrapy通过pip安装Scrapy： $ sudo pip install scrapy 在Mac环境下，如果系统版本是OS X EI Capitan，并且使用的是系统的python，而不是virtualenv的虚拟环境，则可能会遇到如下问题： OSError: [Errno 1] Operation not permitted: &apos;/tmp/pip-nIfswi-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six-1.4.1-py2.7.egg-info&apos; 原因是Scrapy的依赖库Six与系统搞得依赖库Six发生了冲突，一种解决方式是将系统的System Integrity Protection临时禁用，更好地解决方式当然是使用virtualenv创建一个隔离的python环境。 参考：“OSError: [Errno 1] Operation not permitted” 在ubuntu环境下，可能会遇到这个问题： fatal error: openssl/aes.h: No such file or directory 安装libssl-dev即可： $ sudo apt-get install libssl-dev 参考：How to fix “fatal error: openssl/aes.h: No such file or directory”","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://nkcoder.github.io/tags/Scrapy/"}]},{"title":"VirtualBox安装Ubuntu Server","slug":"virtualbox-install-ubuntu-server","date":"2014-12-16T12:47:40.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/12/16/virtualbox-install-ubuntu-server/","link":"","permalink":"http://nkcoder.github.io/2014/12/16/virtualbox-install-ubuntu-server/","excerpt":"有段时间没更新博客了，今天写一篇随手笔记吧。 公司给配置的电脑是Thinkpad， 装的是Windows，但手边没有个Linux环境，总感觉缺点啥，于是决定在虚拟机上装个Ubuntu。","text":"有段时间没更新博客了，今天写一篇随手笔记吧。 公司给配置的电脑是Thinkpad， 装的是Windows，但手边没有个Linux环境，总感觉缺点啥，于是决定在虚拟机上装个Ubuntu。 版本情况： Windows：64位Windows 7 VirtualBox：VirtualBox-4.3.20-96997-Win Ubuntu：ubuntu-14.04.1-server-amd64 在VirtualBox中新建虚拟机时，发现ubuntu的版本只有Ubuntu(32bit)一个选项，有点纳闷，不管了，先继续安装。选择完语言，开始安装后就报错了： This kernel requires an X86-64 CPU,but only detected an i686 CPU Goolge一下，很常见的问题了，需要在BIOS中启用Intel VT-x的虚拟化配置，该配置默认是禁用的。 然后，重新启动虚拟机，还是同样的错误，原来还需要在settings中将version修改为Ubuntu(64 bit)，再重新启动就OK了。 以前在VirtualBox中安装CentOS时，也遇到相同的问题，只是过了一段时间后就忘了，好记性不如烂笔头。 参考： This kernel requires an x86-64 CPU","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"virtualbox","slug":"virtualbox","permalink":"http://nkcoder.github.io/tags/virtualbox/"}]},{"title":"ElasticSearch+LogStash+Kibana+Redis日志服务的高可用方案","slug":"elkr-log-platform-deploy-ha","date":"2014-11-06T13:35:33.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/11/06/elkr-log-platform-deploy-ha/","link":"","permalink":"http://nkcoder.github.io/2014/11/06/elkr-log-platform-deploy-ha/","excerpt":"1. 高可用方案的架构在上一篇文章使用ElasticSearch+LogStash+Kibana+Redis搭建日志管理服务中介绍了日志服务的整体框架以及各组件的搭建部署，本篇文章主要讨论一下日志服务框架的高可用方案，主要从以下三个方面考虑：","text":"1. 高可用方案的架构在上一篇文章使用ElasticSearch+LogStash+Kibana+Redis搭建日志管理服务中介绍了日志服务的整体框架以及各组件的搭建部署，本篇文章主要讨论一下日志服务框架的高可用方案，主要从以下三个方面考虑： 作为broker的Redis，可以使用redis cluster或者主备结构代替单实例，提高broker组件的可用性； 作为indexer的LogStash，可以部署多个LogStash实例，协作处理日志信息，提高indexer组件的可用性； 作为search&amp;storage的ElasticSearch，采用ElasticSearch Cluster，提高search&amp;storage组件的性能和可用性； 日志服务的高可用方案的示意图为： 下面分别予以介绍。 2. Redis Cluster和主备结构2.1 Redis Cluster首先需要部署一个redis cluster，为了方便，我在本机上部署了一个三主三从的cluster，端口分别为：7000, 7001, 7002, 7003, 7004, 7005，以端口7000为例，配置文件为： include ../redis.conf daemonize yes pidfile /var/run/redis_7000.pid port 7000 logfile /opt/logs/redis/7000.log appendonly yes cluster-enabled yes cluster-config-file node-7000.conf 对于redis来说，远程Logstash和中心LogStash都是redis cluster的client，因此只需要与cluster中的任何一个节点连接即可；远程LogStash和中心LogStash的redis配置部分为： shipper.conf: output { redis { host =&gt; &quot;20.8.40.49:7000&quot; data_type =&gt; &quot;list&quot; key =&gt; &quot;key_count&quot; } } central.conf: input { redis { host =&gt; &quot;20.8.40.49&quot; port =&gt; 7000 type =&gt; &quot;redis-cluster-input&quot; data_type =&gt; &quot;list&quot; key =&gt; &quot;key_count&quot; } } 使用redis cluster的优劣分析： 可以提高broker组件的可用性：当每一个master节点都有slave节点的时候，任何一个节点挂掉，都不影响集群的正常工作；如果启用cluster-require-full-coverage no，则有效节点构成的子集群仍然可用。 当作为broker组件的redis成为瓶颈的时候，redis cluster提供良好的扩展性。但是redis cluster有一个比较头疼的问题，就是在伸缩(增删节点)时，需要手动做sharding，官方提供了redis-trib.rb工具，我自己实现了一个java版本，可以作为参考redis-toolkit。 当前的redis cluster还是RC1版，稳定版还需要等待一段时间。 2.2 redis主备结构注意，主备不是主从，备用的redis实例只是作为冗余节点，当主节点挂掉时，备用的节点会顶上，任何时刻仅有一个节点在提供服务。无论是远程LogStash还是中心LogStash，都需要明确配置所有主备redis节点的信息，LogStash会轮询节点列表，选择一个可用的节点。比如，配置两个redis实例，6379作为主，6380作为从，则远程LogStash和中心LogStash的配置分别为： shipper.conf: output { redis { host =&gt; [&quot;20.8.40.49:6379&quot;, &quot;20.8.40.49:6380&quot;] data_type =&gt; &quot;list&quot; key =&gt; &quot;key_count&quot; } } central.conf: input { redis { host =&gt; &quot;20.8.40.49&quot; port =&gt; 6379 type =&gt; &quot;redis-cluster-input&quot; data_type =&gt; &quot;list&quot; key =&gt; &quot;key_count&quot; } redis { host =&gt; &quot;20.8.40.49&quot; port =&gt; 6380 type =&gt; &quot;redis-cluster-input&quot; data_type =&gt; &quot;list&quot; key =&gt; &quot;key_count&quot; } } redis主备结构的优劣分析： 可以提高broker组件的可用性：只要主备节点中有一个节点可用，broker组件服务就是可用的； 主备结构无法解决redis成为瓶颈的情况； 3. 部署多个中心LogStash当日志信息的量很大时，作为indexer的LogStash很可能成为瓶颈，此时，可以部署多个中心LogStash，它们之间的关系是对等的，共同从broker处提取消息，中心LogStash节点之间是相互独立的。每一个中心LogStash节点的配置是完全一样的，比如当broker是redis cluster时，中心LogStash的配置为： central.conf: input { redis { host =&gt; &quot;20.8.40.49&quot; port =&gt; 7000 type =&gt; &quot;redis-cluster-input&quot; data_type =&gt; &quot;list&quot; key =&gt; &quot;key_count&quot; } } 部署多个中心LogStash的优劣分析： 可以提高indexer组件的可用性：多个中心LogStash节点之间是相互独立的，任何一个节点的失效不会影响其它节点，更不会影响整个indexer组件；当broker是redis时，各个中心LogStash都是redis的client，它们都是执行BLPOP命令从redis中提取消息，而redis的任何单个命令都是原子的，因此多中心LogStash不仅可以提高indexer组件的可用性，也可以提高indexer组件的处理能力和效率； 多中心LogStash节点的部署和维护成本，可以借助配置管理工具如Puppet、SaltStack等 4. ElasticSearch ClusterElasticSearch原生支持Cluster模式，节点之间通过单播或多播进行通信；ElasticSearch Cluster能自动检测节点的增加、失效和恢复，并重新组织索引。 比如，我们启动两个ElasticSearch实例构成集群，使用默认配置即可，如： $ bin/elasticsearch -d $ bin/elasticsearch -d 使用默认配置，两个实例的HTTP监听端口分别为9200和9201，它们的节点间通信端口分别为9300和9301，默认使用多播构成集群，集群的名称为elasticsearch； 中心LogStash只需要配置ElasticSearch的cluster name即可(如果不能发现ES集群，可以指定host: host =&gt; &quot;20.8.40.49&quot;)，如： output { elasticsearch { cluster =&gt; &quot;elasticsearch&quot; codec =&gt; &quot;json&quot; protocol =&gt; &quot;http&quot; } } 使用ElasticSearch Cluster的优劣分析： 提高组件的可用性：cluster中任何一个节点挂掉，索引及副本都会自动重新分配； 极佳的水平扩展性：向cluster中增加新的节点即可，索引会自动重新组织； 5. 后续工作关于ELKR日志服务，下一步的工作方向有： 学习grok正则表达式，匹配自定义的日志输出格式； 研究ElasticSearch的查询功能及原理； 熟悉Kibana丰富的图标展示功能； Ok，高可用方案的介绍告一段落，如果用到实际的生产环境中，应该会遇到很多意想不到的问题，后续会继续总结和分享。 参考： The LogStash Book ElasticSearch doc Redis Documentation","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://nkcoder.github.io/tags/elasticsearch/"},{"name":"kibana","slug":"kibana","permalink":"http://nkcoder.github.io/tags/kibana/"},{"name":"logstash","slug":"logstash","permalink":"http://nkcoder.github.io/tags/logstash/"}]},{"title":"使用ElasticSearch+LogStash+Kibana+Redis搭建日志管理服务","slug":"elkr-log-platform-deploy","date":"2014-10-31T01:37:56.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/10/31/elkr-log-platform-deploy/","link":"","permalink":"http://nkcoder.github.io/2014/10/31/elkr-log-platform-deploy/","excerpt":"1. 日志平台的结构示意图 说明： 多个独立的agent(Shipper)负责收集不同来源的数据，一个中心agent(Indexer)负责汇总和分析数据，在中心agent前的Broker(使用redis实现)作为缓冲区，中心agent后的ElasticSearch用于存储和搜索数据，前端的Kibana提供丰富的图表展示。 Shipper表示日志收集，使用LogStash收集各种来源的日志数据，可以是系统日志、文件、redis、mq等等； Broker作为远程agent与中心agent之间的缓冲区，使用redis实现，一是可以提高系统的性能，二是可以提高系统的可靠性，当中心agent提取数据失败时，数据保存在redis中，而不至于丢失； 中心agent也是LogStash，从Broker中提取数据，可以执行相关的分析和处理(Filter)； ElasticSearch用于存储最终的数据，并提供搜索功能； Kibana提供一个简单、丰富的web界面，数据来自于ElasticSearch，支持各种查询、统计和展示；","text":"1. 日志平台的结构示意图 说明： 多个独立的agent(Shipper)负责收集不同来源的数据，一个中心agent(Indexer)负责汇总和分析数据，在中心agent前的Broker(使用redis实现)作为缓冲区，中心agent后的ElasticSearch用于存储和搜索数据，前端的Kibana提供丰富的图表展示。 Shipper表示日志收集，使用LogStash收集各种来源的日志数据，可以是系统日志、文件、redis、mq等等； Broker作为远程agent与中心agent之间的缓冲区，使用redis实现，一是可以提高系统的性能，二是可以提高系统的可靠性，当中心agent提取数据失败时，数据保存在redis中，而不至于丢失； 中心agent也是LogStash，从Broker中提取数据，可以执行相关的分析和处理(Filter)； ElasticSearch用于存储最终的数据，并提供搜索功能； Kibana提供一个简单、丰富的web界面，数据来自于ElasticSearch，支持各种查询、统计和展示； 2. 搭建部署环境： 本机(20.8.40.49)上部署：redis, 中心agent(LogStash), ElasticSearch以及Kibana 远程测试机(20.20.79.75)上部署：独立agent(LogStash) Redis版本：3.0.0-rc1 LogStash版本；logstash-1.4.2 ElasticSearch版本：elasticsearch-1.3.4 Kibana版本：kibana-3.1.1 2.1 部署redis部署一个redis单机实例: $ wget https://github.com/antirez/redis/archive/3.0.0-rc1.tar.gz $ tar zxvf 3.0.0-rc1.tar.gz -C /usr/local redis.conf配置文件为： include ../redis.conf daemonize yes pidfile /var/run/redis_6379.pid port 6379 logfile /opt/logs/redis/6379.log appendonly yes 启动： $ redis.server redis.conf ip为10.7.40.40， 端口为6379 2.2 部署中心LogStash下载并解压： $ wget https://download.elasticsearch.org/logstash/logstash/logstash-1.4.2.tar.gz $ tar zxvf logstash-1.4.2.tar.gz -C /usr/local/ $ cd /usr/local/logstash-1.4.2 $ mkdir conf logs 配置文件conf/central.conf: input { redis { host =&gt; &quot;127.0.0.1&quot; port =&gt; 6379 type =&gt; &quot;redis-input&quot; data_type =&gt; &quot;list&quot; key =&gt; &quot;key_count&quot; } } output { stdout {} elasticsearch { cluster =&gt; &quot;elasticsearch&quot; codec =&gt; &quot;json&quot; protocol =&gt; &quot;http&quot; } } 启动： $ bin/logstash agent --verbose --config conf/central.conf --log logs/stdout.log 配置文件表示输入来自于redis，使用redis的list类型存储数据，key为”key_count”；输出到elasticsearch，cluster的名称为”elasticsearch”; 2.3 部署ElasticSearch下载并解压： $ wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.3.4.tar.gz $ tar zxvf elasticsearch-1.3.4.tar.gz -C /usr/local elasticsearch使用默认配置即可，默认的cluster name为：elasticsearch； 启动： $ bin/elasticsearch -d 2.4 部署远程LogStash与部署中心LogStash的步骤是类似的，只是配置文件不一样，使用新的配置文件启动即可； 配置文件conf/shipper.conf的内容为： input { file { type =&gt; &quot;type_count&quot; path =&gt; [&quot;/data/logs/count/stdout.log&quot;, &quot;/data/logs/count/stderr.log&quot;] exclude =&gt; [&quot;*.gz&quot;, &quot;access.log&quot;] } } output { stdout {} redis { host =&gt; &quot;20.8.40.49&quot; port =&gt; 6379 data_type =&gt; &quot;list&quot; key =&gt; &quot;key_count&quot; } } 配置文件表示输入来自于目录/data/logs/count/下的stdout.log和stderr.log两个文件，且排除该目录下所有.gz文件和access.log；(这里因为path没有使用通配符，所以exclude是没有效果的)；输出表示将监听到的event发送到redis服务器，使用redis的list保存，key为”key_count”，这里的data_type属性和key属性应该与中心agent的配置一致； 2.5 部署Kibana下载并安装： $ wget https://download.elasticsearch.org/kibana/kibana/kibana-3.1.1.tar.gz $ tar zxvf kibana-3.1.1.tar.gz 修改配置文件config.js，仅需要配置elasticsearch的地址即可： elasticsearch: &quot;http://20.8.40.49:9200&quot; 将目录kibana-3.1.1拷贝到jetty的webapp目录下，并启动jetty： $ mv kibana-3.1.1 /usr/local/jetty-distribution-9.2.1.v20140609/webapps/ $ bin/jetty start 浏览器访问： http://20.8.40.49:8080/kibana-3.1.1/ 3. 简单测试打开LogStash的远程agent和中心agent的日志： $ tail -f logs/stdout.log 远程agent的数据是以rpush操作将event推送到redis的list中，中心agent通过blpop命令从redis的list中提取数据，因此，测试时由于数据量小，通过命令llen key_count的返回结果很可能为空，因此为了观察redis中数据流的变化，可以使用monitor命令： $ redis-cli -p 6379 monitor 现在，我们向/data/logs/count目录下的stdout.log和stderr.log各发送一条数据： $ echo &quot;stdout: just a test message&quot; &gt;&gt; stdout.log $ echo &quot;stderr: just a test message&quot; &gt;&gt; stderr.log 远程agent和中心agent都会收到event消息，如远程agent的日志为： {:timestamp=&gt;&quot;2014-10-31T09:30:40.323000+0800&quot;, :message=&gt;&quot;Received line&quot;, :path=&gt;&quot;/data/logs/count/stdout.log&quot;, :text=&gt;&quot;stdout: just a test message&quot;, :level=&gt;:debug, :file=&gt;&quot;logstash/inputs/file.rb&quot;, :line=&gt;&quot;134&quot;} {:timestamp=&gt;&quot;2014-10-31T09:30:40.325000+0800&quot;, :message=&gt;&quot;writing sincedb (delta since last write = 52)&quot;, :level=&gt;:debug, :file=&gt;&quot;filewatch/tail.rb&quot;, :line=&gt;&quot;177&quot;} ...... {:timestamp=&gt;&quot;2014-10-31T09:30:49.350000+0800&quot;, :message=&gt;&quot;Received line&quot;, :path=&gt;&quot;/data/logs/count/stderr.log&quot;, :text=&gt;&quot;stderr: just a test message&quot;, :level=&gt;:debug, :file=&gt;&quot;logstash/inputs/file.rb&quot;, :line=&gt;&quot;134&quot;} {:timestamp=&gt;&quot;2014-10-31T09:30:49.352000+0800&quot;, :message=&gt;&quot;output received&quot;, :event=&gt;{&quot;message&quot;=&gt;&quot;stderr: just a test message&quot;, &quot;@version&quot;=&gt;&quot;1&quot;, &quot;@timestamp&quot;=&gt;&quot;2014-10-31T01:30:49.350Z&quot;, &quot;type&quot;=&gt;&quot;type_count&quot;, &quot;host&quot;=&gt;&quot;dn1&quot;, &quot;path&quot;=&gt;&quot;/data/logs/count/stderr.log&quot;}, :level=&gt;:debug, :file=&gt;&quot;(eval)&quot;, :line=&gt;&quot;19&quot;} 我们可以观察到redis的输出： 1414714174.936642 [0 20.20.79.75:54010] &quot;rpush&quot; &quot;key_count&quot; &quot;{\\&quot;message\\&quot;:\\&quot;stdout: just a test message\\&quot;,\\&quot;@version\\&quot;:\\&quot;1\\&quot;,\\&quot;@timestamp\\&quot;:\\&quot;2014-10-31T00:10:04.530Z\\&quot;,\\&quot;type\\&quot;:\\&quot;type_count\\&quot;,\\&quot;host\\&quot;:\\&quot;dn1\\&quot;,\\&quot;path\\&quot;:\\&quot;/data/logs/count/stdout.log\\&quot;}&quot; 1414714174.939517 [0 127.0.0.1:56094] &quot;blpop&quot; &quot;key_count&quot; &quot;0&quot; 1414714198.991452 [0 20.20.79.75:54010] &quot;rpush&quot; &quot;key_count&quot; &quot;{\\&quot;message\\&quot;:\\&quot;stderr: just a test message\\&quot;,\\&quot;@version\\&quot;:\\&quot;1\\&quot;,\\&quot;@timestamp\\&quot;:\\&quot;2014-10-31T00:10:28.586Z\\&quot;,\\&quot;type\\&quot;:\\&quot;type_count\\&quot;,\\&quot;host\\&quot;:\\&quot;dn1\\&quot;,\\&quot;path\\&quot;:\\&quot;/data/logs/count/stderr.log\\&quot;}&quot; 1414714198.993590 [0 127.0.0.1:56094] &quot;blpop&quot; &quot;key_count&quot; &quot;0&quot; 从elasticsearch中执行如下的简单查询： $ curl &apos;localhost:9200/_search?q=type:type_count&amp;pretty&apos; { &quot;took&quot; : 3, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 6, &quot;successful&quot; : 6, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : 2, &quot;max_score&quot; : 0.5945348, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;logstash-2014.10.31&quot;, &quot;_type&quot; : &quot;type_count&quot;, &quot;_id&quot; : &quot;w87bRn8MToaYm_kfnygGGw&quot;, &quot;_score&quot; : 0.5945348, &quot;_source&quot;:{&quot;message&quot;:&quot;stdout: just a test message&quot;,&quot;@version&quot;:&quot;1&quot;,&quot;@timestamp&quot;:&quot;2014-10-31T08:10:04.530+08:00&quot;,&quot;type&quot;:&quot;type_count&quot;,&quot;host&quot;:&quot;dn1&quot;,&quot;path&quot;:&quot;/data/logs/count/stdout.log&quot;} }, { &quot;_index&quot; : &quot;logstash-2014.10.31&quot;, &quot;_type&quot; : &quot;type_count&quot;, &quot;_id&quot; : &quot;wwmA2BD6SAGeNsuYz5ax-Q&quot;, &quot;_score&quot; : 0.5945348, &quot;_source&quot;:{&quot;message&quot;:&quot;stderr: just a test message&quot;,&quot;@version&quot;:&quot;1&quot;,&quot;@timestamp&quot;:&quot;2014-10-31T08:10:28.586+08:00&quot;,&quot;type&quot;:&quot;type_count&quot;,&quot;host&quot;:&quot;dn1&quot;,&quot;path&quot;:&quot;/data/logs/count/stderr.log&quot;} } ] } } 再切换到Kibana的web界面：http://20.8.40.49:8080/kibana-3.1.1 4. 后续工作 使用LogStash的Filter对日志数据进行过滤和分析； 使用Redis的Cluster模式替换单机模式； 在elasticsearch中对数据进行高级的分析和查询； 熟悉Kibana的展示组件以及查询语法； 参考： logstash-doc-1.4.2 elasticsearch redis-blpop","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://nkcoder.github.io/tags/elasticsearch/"},{"name":"kibana","slug":"kibana","permalink":"http://nkcoder.github.io/tags/kibana/"},{"name":"logstash","slug":"logstash","permalink":"http://nkcoder.github.io/tags/logstash/"}]},{"title":"为什么应该选择Gradle","slug":"why-should-choose-gradle","date":"2014-10-27T13:00:54.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/10/27/why-should-choose-gradle/","link":"","permalink":"http://nkcoder.github.io/2014/10/27/why-should-choose-gradle/","excerpt":"1. 为什么选择Gradle？","text":"1. 为什么选择Gradle？ 2. 简单入门2.1 使用Intellij Idea创建gradle项目首先在Idea中启用Gradle支持：Settings-&gt;Plugins: Gradle 然后创建一个gradle项目或模块，会发现目录结构和maven的很像，其中build.gradle是gradle的配置文件，类似于maven中pom.xml文件，以下是build.gradle的简单示例： apply plugin: &apos;java&apos; group = &apos;org.yousharp&apos; version = &apos;1.0-SNAPSHOT&apos; sourceCompatibility = 1.7 targetCompatibility = 1.7 repositories { mavenCentral() maven { url &quot;http://repo.maven.apache.org/maven2&quot; } } dependencies { compile group: &apos;com.google.guava&apos;, name: &apos;guava&apos;, version:&apos;17.0&apos; compile group: &apos;redis.clients&apos;, name: &apos;jedis&apos;, version:&apos;2.6.0&apos; testCompile group: &apos;junit&apos;, name: &apos;junit&apos;, version:&apos;4.11&apos; } 插件(plugin)：是gradle的一种扩展，gradle预定义了很多插件，常见的如java插件、war插件等；java插件中定义了一些有用的task，如编译源码、执行单元测试、生成jar包、约定默认的目录结构等；repositories定义仓库，dependencies定义项目的依赖，比maven的XML定义更简洁； 那么，如何编译、测试，以及运行gradle的项目呢？ 刚才说到java插件预定义了很多task，其中就包括编译、测试、生成jar包等task，可以在命令行通过$ gradle tasks查看项目已定义的所有task以及含义，如java插件中常用的task有： + assemble: 编译 + build：编译并执行测试 + clean：删除build目录 + jar： 生成jar包 + test：执行单元测试 2.2 将Java项目从maven迁移到gradle如何将一个java项目从maven迁移到gradle呢？gradle集成了一个很方便的插件：Build Init Plugin，使用这个插件可以很方便地创建一个新的gradle项目，或者将其它类型的项目转换为gradle项目。 要将maven项目转换为gradle项目，只需要在项目的pom文件所在的目录下执行以下命令： $ gradle init --type pom 上面的命令会根据pom文件自动生成gradle项目所需的文件和配置，然后以gradle项目重新导入即可。 关于gradle的简介就到这里，感兴趣的朋友可以参考gradle-userguide以及Gradle in Action。 参考 gradle-userguide Gradle in Action","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"gradle","slug":"gradle","permalink":"http://nkcoder.github.io/tags/gradle/"}]},{"title":"Redis-Toolkit--Java实现的redis工具(一)","slug":"redis-tookit-implement-in-java-1","date":"2014-10-24T03:21:49.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/10/24/redis-tookit-implement-in-java-1/","link":"","permalink":"http://nkcoder.github.io/2014/10/24/redis-tookit-implement-in-java-1/","excerpt":"redis作者提供了redis-trib.rb工具，用来与redis cluster进行交互，该工具使用ruby实现；对该工具提供的主要功能，我在redis的java客户端jedis的基础上进行了对应的实现，希望给同样使用java与redis交互的朋友一些参考。","text":"redis作者提供了redis-trib.rb工具，用来与redis cluster进行交互，该工具使用ruby实现；对该工具提供的主要功能，我在redis的java客户端jedis的基础上进行了对应的实现，希望给同样使用java与redis交互的朋友一些参考。 1. 创建集群使用作者在cluster-tutorial上的示例，即在localhost的7000、7001、7002、7003、7004、7005启动6个redis实例，且都启用cluster模式，使用redis-trib创建集群的命令为： ./redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 上述命令表示使用这6个实例创建集群，3主3从； 实际上，这一条命令，如果拆分为原生的redis命令来实现，则主要有以下4个过程： 1. 使用`CLUSTER MEET`命令将所有节点构建成一个集群； 2. 使用`CLUSTER REPLICATE`命令设置master/slave结构； 3. 使用`CLUSTER SETSLOTS`命令将16384个slot分配到集群中的master中； 4. 等待集群的状态变为OK； 对于以上原生命令，jedis都是支持的，所以根据以上4个步骤，使用jedis创建集群就不难了；在redis-tookit中整个创建的过程被封装到Create#create()方法中了，示例代码： /* 构建一个3主3从的集群 */ ArrayListMultimap&lt;HostAndPort, HostAndPort&gt; clusterNodes = ArrayListMultimap.create(); clusterNodes.put(HostAndPort.fromString(&quot;127.0.0.1:7000&quot;), HostAndPort.fromString(&quot;127.0.0.1:7001&quot;)); clusterNodes.put(HostAndPort.fromString(&quot;127.0.0.1:7002&quot;), HostAndPort.fromString(&quot;127.0.0.1:7003&quot;)); clusterNodes.put(HostAndPort.fromString(&quot;127.0.0.1:7004&quot;), HostAndPort.fromString(&quot;127.0.0.1:7005&quot;)); Create.create(clusterNodes); 2. 数据迁移(reshard)在beta8之前，使用redis-trib.rb做reshard时，仅支持交互时模式，beta8之后，开始支持非交互式模式。比如，从源节点38807bd0262d99f205ebd0eb3e483cc09e927731上迁移100个slot到目标节点47ef6c293bb3f9763d421f56c63f00cf06ef5b3f的非交互式命令为： redis-trib.rb reshard --from 38807bd0262d99f205ebd0eb3e483cc09e927731 --to 47ef6c293bb3f9763d421f56c63f00cf06ef5b3f --slots 100 --yes 127.0.0.1:7000 那么，使用redis的原生命令如何完成数据的reshard呢？其实，在数据reshard时，slot是逐个迁移的，redis迁移一个slot的步骤为： 1. 对目标节点发送`CLUSTER SETSLOT &lt;slot&gt; IMPORTING &lt;source_node_id&gt;`命令，表示目标节点将从源节点迁移slot； 2. 对源节点发送`CLUSTER SETSLOT &lt;slot&gt; MIGRATING &lt;target_node_id&gt;`命令，表示源节点将向目标节点迁移slot； 3. 对源节点发送`CLUSTER GETKEYSINSLOT &lt;slot&gt; &lt;count&gt;`命令，表示从slot中取出count个key/value对的key； 4. 对源节点发送`CLUSTER MIGRATE &lt;target_ip&gt; &lt;target_port&gt; &lt;key_name&gt; 0 &lt;timeout&gt;`命令，表示将key迁移到目标节点，对步骤3中的每个key都会执行该命令； 5. 重复执行步骤3和4，直到该slot中所有的key都被迁移完毕； 6. 向集群中的任一节点发送`CLUSTER SETSLOT &lt;slot&gt; NODE &lt;target_node_id&gt;`，表示告诉集群，将该slot分配给目标节点； 7. 等待集群的状态变为OK； 使用java实现时，也是通过jedis调用redis原生命令，根据以上步骤来实现的。在redis-toolkit中，封装了两个方法，Reshard#migrateSlots()表示迁移一个slot，Reshard#migrate()表示批量迁移，批量迁移时，从源节点的slot索引，从低到高依次迁移，使用示例： /** 将slot 9189从节点7002迁移到节点7006 **/ HostAndPort srcNodeInfo = HostAndPort.fromString(&quot;10.7.40.49:7002&quot;); HostAndPort destNodeInfo = HostAndPort.fromString(&quot;10.7.40.49:7006&quot;); Reshard.migrateSlots(srcNodeInfo, destNodeInfo, 9189); /** 从节点7000迁移100个slot到节点7004 **/ HostAndPort srcNodeInfo = HostAndPort.fromString(&quot;10.7.40.49:7000&quot;); HostAndPort destNodeInfo = HostAndPort.fromString(&quot;10.7.40.49:7004&quot;); Reshard.migrate(srcNodeInfo, destNodeInfo, 100); 3. 节点的增加/删除3.1 增加节点无论要添加的是master节点还是slave节点，首先都需要将该节点加入到集群中，然后，如果是master节点，则迁移一部分数据到该节点上，如果是slave节点，则使该节点成为其master节点的slave。 通过redis-trib.rb工具，增加一个master节点命令为： ./redis-trib.rb add-node 127.0.0.1:7006 127.0.0.1:7000 然后通过redis-trib.rb reshard命令迁移数据到新节点7006上； 增加一个slave节点，redis-trib.rb提供了两种方式： ./redis-trib.rb add-node --slave 127.0.0.1:7006 127.0.0.1:7000 ./redis-trib.rb add-node --slave --master-id 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 127.0.0.1:7006 第一种方式将该slave节点随机分配给集群中具有最少slave的master节点，第二种方式是将slave节点分配给指定的master节点；更通用的方式，则是先将该节点作为空的master节点加入集群，然后通过CLUSTER REPLICATE命令使其成为一个master节点的slave。 对redis集群进行扩容时，可分为垂直扩容和水平扩容，垂直扩容即通过CONFIG SET命令在线修改redis实例的maxmemory的大小，水平扩容即增加新的master节点，然后迁移一部分数据到新的节点上。 向集群中添加一个master节点，则相当于水平扩容，给集群中的一个节点增加一个slave节点，则不是扩容。 所以，增加节点的步骤为： - 使用`CLUSTER MEET`命令将节点加入到当前集群； - 如果要加入的是master节点，则迁移slot到新节点上(参考第2步的“数据迁移”)； - 如果要加入的是slave节点，则使用`CLUSTER REPLICATE`命令建立master/slave关系； redis-tookit中提供的增加节点的方法为Manage#addNewNode()，支持增加master节点和slave节点；如果增加的是master节点，则会迁移数据到新的master节点，为了迁移操作更均衡，根据每个master节点上的slot数等比例地迁移，即从slot更多的节点上迁移更多的slot，从slot少的节点上迁移更少的slot，使用示例： /** 向集群中增加一个master节点7006 **/ HostAndPort clusterNodeInfo = HostAndPort.fromString(&quot;10.7.40.49:7000&quot;); HostAndPort newMaster = HostAndPort.fromString(&quot;10.7.40.49:7006&quot;); Manage.addNewNode(clusterNodeInfo, newMaster, null); /** 给集群中增加一个slave节点8001，作为master节点7002的slave **/ HostAndPort clusterNodeInfo = HostAndPort.fromString(&quot;10.7.40.49:7000&quot;); HostAndPort master = HostAndPort.fromString(&quot;10.7.40.49:7002&quot;); HostAndPort newSlave = HostAndPort.fromString(&quot;10.7.40.49:8001&quot;); Manage.addNewNode(clusterNodeInfo, newSlave, master); 3.2 删除节点从集群中删除一个节点，如果要删除的是slave节点，直接将节点从集群中剔除即可，如果要删除的是master节点，则需要先将所有的slot迁移走，然后删除节点。 redis-trib.rb工具提供的删除一个节点的命令为： ./redis-trib del-node 127.0.0.1:7000 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 删除节点，就是调用CLUSTER FORGET命令，将要删除的节点从当前集群中踢掉，需要注意的是： - 需要对集群中的每一个节点(无论是master还是slave)都调用`CLUSTER FORGET`来“忘记”该节点，因为如果集群中有一个节点还“认识”待删除的节点，则通过消息，不久集群中的所有节点都”认识“待删除的节点了； - 一个slave节点不能”忘记“它的master节点； - 一个节点不能”忘记“它自己； 所以，删除一个节点的步骤为： - 如果节点是master节点，先将节点上的slot迁移，然后从集群中删除该master节点以及它的所有slave节点； - 如果节点是slave节点，则直接从集群中删除即可； redis-tookit中提供了删除master节点和slave节点的方法，删除master节点时，需要迁移数据，当前的实现是将待删除的master节点上的所有slot平均分配给集群中的其它master；使用示例： /* 从集群中删除一个节点，删除时，集群会判断待删除的节点是master还是slave */ HostAndPort oneNode = HostAndPort.fromString(&quot;10.7.40.49:7000&quot;); HostAndPort nodeToDelete = HostAndPort.fromString(&quot;10.7.40.49:7006&quot;); Manage.removeNode(oneNode, nodeToDelete); redis-tookit代码在https://github.com/nkcoder/redis-toolkit 参考 Redis cluster tutorial Redis设计与实现","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://nkcoder.github.io/tags/redis/"}]},{"title":"Redis 3.0.0 RC1的发布和更新","slug":"redis-3-dot-0-0-rc1-released-and-update","date":"2014-10-20T16:04:26.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/10/21/redis-3-dot-0-0-rc1-released-and-update/","link":"","permalink":"http://nkcoder.github.io/2014/10/21/redis-3-dot-0-0-rc1-released-and-update/","excerpt":"Redis 3.0.0 RC1在10月9日发布，这个版本具有重要的意义，说明Redis Cluster已经非常稳定了。 我们来简单梳理一下RC1中的重要更新。","text":"Redis 3.0.0 RC1在10月9日发布，这个版本具有重要的意义，说明Redis Cluster已经非常稳定了。 我们来简单梳理一下RC1中的重要更新。 1. Redis 3.0.0 RC1的release notes1.1 常规更新 [修复] 对一些简单问题的修复汇总，修复列表可以参考issue #1906，修复的主要内容有： 将sds的size从2GB增加到4GB(即使用unsigned int替换int，其它不变)； 如果redis在前台运行，Ctrl-C会将server优雅地SHUTDOWN，而不是仅仅将server进程kill掉； Sentinel支持INFO ALL命令； 使用redis-benchmark命令测试时，新增-a参数用于验证； 对代码的优化、重构等。 [修复] SAVE 命令不会蔓延到AOF文件和Slave实例； [修复] 当hash table处于异常状态时，限制SCAN操作的延迟(当数据库正在rehash时，可用的bucket数量很少)； [新增] redis在启动时可以加载被截断的AOF文件，而不需要先执行redis-check-aof工具，该功能可以在redis.conf中配置：aof-load-truncated yes； [新增] INCR：尽可能地在原地修改自增键，可以得到更好的性能； 1.2 Cluster更新 [修复] 即使连接断开，仍要求返回ping_sent时间； [修复] 判断是否处于少数节点构成的网络时，修复判断逻辑； [修复] 修复已加入集群的节点间的gossip通信逻辑； [新增] Redis Cluster运行稳定且通过了详尽的测试，因此从beta版晋升为stable版； [新增] 当slot没有被完全覆盖时，集群仍然可用； [新增] 当failover过程停顿的时候，slave会打印更详细的日志，包括原因； 1.3 Sentinel更新 [修复] 修复严重bug：在计算大多数(majority)节点时，之前的实现方式是错误的，而specification上说的是对的。大多数节点是针对master的所有sentinel，而不管它当前的状态。 [修复] 修复hiredis库中的一个内存泄露问题：当一个被监控的实例或者另一个sentinel不可用的时候会导致内存泄露，每一次重连都会泄露少量的内存，但是如果持续累积，将会导致大量的内存泄露； 2. redis.conf更新将redis-3.0.0-beta8中的redis.conf和redis-3.0.0-rc1中的redis.conf通过diff查看两者的区别。 # diff redis-b8.conf redis-rc1.conf &gt; diff.conf 对比后发现，大部分的更新都是针对表述上的，功能上的更新主要有三点： unixsocketperm 700: unixsocketperm的权限由755修改为700； aof-load-truncated yes: redis在启动的时候可以加载被截断的AOF文件，默认启用； cluster-require-full-coverage yes: rc1之前，如果至少有一个slot没有被节点覆盖，则整个集群不可用；现在，启用该配置，即使有部分slot没有被覆盖，被覆盖到的那部分slot构成的子集群仍然可用。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://nkcoder.github.io/tags/redis/"}]},{"title":"阻塞队列之ArrayBlockingQueue","slug":"blockqueue-arrayblockingqueue","date":"2014-09-05T15:06:18.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/09/05/blockqueue-arrayblockingqueue/","link":"","permalink":"http://nkcoder.github.io/2014/09/05/blockqueue-arrayblockingqueue/","excerpt":"ArrayBlockingQueue是一个有界阻塞队列，队列根据FIFO的顺序组织元素，从队列的头部取元素，新的元素被插入到队列的尾部。插入元素时，如果队列满，线程阻塞(也有不阻塞的方法)，取元素时，如果队列为空，线程阻塞(有不阻塞的方法)。","text":"ArrayBlockingQueue是一个有界阻塞队列，队列根据FIFO的顺序组织元素，从队列的头部取元素，新的元素被插入到队列的尾部。插入元素时，如果队列满，线程阻塞(也有不阻塞的方法)，取元素时，如果队列为空，线程阻塞(有不阻塞的方法)。 内部使用固定大小的数组实现，在构造ArrayBlockingQueue对象时，指定数组的容量compacity，之后该容量不能被修改。 支持公平策略fairness，根据FIFO的顺序选择等待的线程；该参数在构造对象的时候开启，默认禁用；启用fairness会降低吞吐量，但是可以避免线程饥饿。 同步：内部使用ReentrantLock和two-condition算法实现 /** Main lock guarding all access */ final ReentrantLock lock; /** Condition for waiting takes */ private final Condition notEmpty; /** Condition for waiting puts */ private final Condition notFull; notEmpty表示队列不为空，可以插入元素；notFull表示队列没有满，可以取出元素。 构造函数： ArrayBlockingQueue(int capacity) ArrayBlockingQueue(int capacity, boolean fair) 主要方法及区别： boolean add(E e) boolean offer(E e) boolean offer(E e, long timeout, TimeUnit unit) void put(E e) add()插入元素，如果队列不满，则立即返回true，否则抛出IllegalStateException；offer()也是插入元素，在队列满的时候，返回false，也可以指定超时，超时后，队列还是满的，则返回false；put()插入元素，队列满时等待，直到队列不满或者被中断。 E take() E poll(long timeout, TimeUnit unit) E poll() take()表示从队列取元素，如果队列为空，则等待，直到队列不空或者被中断；poll()取出元素，如果队列为空，返回null，可以指定超时，如果超时后队列还是为空，则返回null。 注意：合理地处理这些方法抛出的异常，否则线程就挂了，线程池也就崩溃了。比如add(),take(), put()等都可能会被中断，从而抛出中断异常，poll()可能返回null，如果将返回结果赋值给基本类型的变量，会抛出空指针异常。对线程中可能出现的异常，一般有两种处理方式：1)捕获所有的异常；2)给线程注册一个UncaughtExceptionHandler； 参考： ArrayBlockingQueue","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"java","slug":"java","permalink":"http://nkcoder.github.io/tags/java/"}]},{"title":"Quartz教程八--SchedulerListener","slug":"quartz-tutorial-08-scheduler-listener","date":"2014-08-24T12:45:27.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/08/24/quartz-tutorial-08-scheduler-listener/","link":"","permalink":"http://nkcoder.github.io/2014/08/24/quartz-tutorial-08-scheduler-listener/","excerpt":"SchedulerListener与TriggerListener、JobListener类似，但它仅接收来自Scheduler自身的消息，而不一定是某个具体的trigger或job的消息。 scheduler相关的消息包括：job/trigger的增加、job/trigger的删除、scheduler内部发生的严重错误以及scheduler关闭的消息等；","text":"SchedulerListener与TriggerListener、JobListener类似，但它仅接收来自Scheduler自身的消息，而不一定是某个具体的trigger或job的消息。 scheduler相关的消息包括：job/trigger的增加、job/trigger的删除、scheduler内部发生的严重错误以及scheduler关闭的消息等； org.quartz.SchedulerListener接口： public interface SchedulerListener { public void jobScheduled(Trigger trigger); public void jobUnscheduled(String triggerName, String triggerGroup); public void triggerFinalized(Trigger trigger); public void triggersPaused(String triggerName, String triggerGroup); public void triggersResumed(String triggerName, String triggerGroup); public void jobsPaused(String jobName, String jobGroup); public void jobsResumed(String jobName, String jobGroup); public void schedulerError(String msg, SchedulerException cause); public void schedulerStarted(); public void schedulerInStandbyMode(); public void schedulerShutdown(); public void schedulingDataCleared(); } SchedulerListener也是注册到scheduler的ListenerManager上的，任何实现了org.quartz.SchedulerListener接口的对象都可以是SchedulerListener(译者注：SchedulerListener与JobListener/TriggerListener一样，也可以继承SchedulerListenerSupport抽象类，重写感兴趣的方法即可)。 添加一个SchedulerListener： scheduler.getListenerManager().addSchedulerListener(mySchedListener); 删除一个SchedulerListener： scheduler.getListenerManager().removeSchedulerListener(mySchedListener); 原文链接 本系列教程由quartz-2.2.x官方文档翻译、整理而来，希望给同样对quartz感兴趣的朋友一些参考和帮助，有任何不当或错误之处，欢迎指正；有兴趣研究源码的同学，可以参考我对quartz-core源码的注释(进行中)。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"quartz","slug":"quartz","permalink":"http://nkcoder.github.io/tags/quartz/"}]},{"title":"Quartz教程七--TriggerListener和JobListener","slug":"quartz-tutorial-07-listener","date":"2014-08-17T12:46:50.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/08/17/quartz-tutorial-07-listener/","link":"","permalink":"http://nkcoder.github.io/2014/08/17/quartz-tutorial-07-listener/","excerpt":"listener是一个对象，用于监听scheduler中发生的事件，然后执行相应的操作；你可能已经猜到了，TriggerListeners接受与trigger相关的事件，JobListeners接受与jobs相关的事件。 trigger相关的事件包括：trigger的触发、trigger错过触发(mis-fire)以及trigger的完成(即trigger触发的job执行完成)。","text":"listener是一个对象，用于监听scheduler中发生的事件，然后执行相应的操作；你可能已经猜到了，TriggerListeners接受与trigger相关的事件，JobListeners接受与jobs相关的事件。 trigger相关的事件包括：trigger的触发、trigger错过触发(mis-fire)以及trigger的完成(即trigger触发的job执行完成)。 org.quartz.TriggerListener接口 public interface TriggerListener { public String getName(); public void triggerFired(Trigger trigger, JobExecutionContext context); public boolean vetoJobExecution(Trigger trigger, JobExecutionContext context); public void triggerMisfired(Trigger trigger); public void triggerComplete(Trigger trigger, JobExecutionContext context, int triggerInstructionCode); } job相关的事件包括：job即将执行的通知以及job执行完毕的通知。 org.quartz.JobListener接口： public interface JobListener { public String getName(); public void jobToBeExecuted(JobExecutionContext context); public void jobExecutionVetoed(JobExecutionContext context); public void jobWasExecuted(JobExecutionContext context, JobExecutionException jobException); } 使用自定义的listener创建一个listener，只需要实现org.quartz.TriggerListener接口或者org.quartz.JobListener 接口即可；然后在运行时将listener注册到scheduler上，并且需要给listener取个名称(因为listener需要通过其getName()方法广播它的名称)。 我们可以实现上面提到的接口，但更方便的方式是继承JobListenerSupport类或者TriggerListenerSupport类，只需重写需要的方法即可。 listener是注册到scheduler的ListenerManager上的，与listener一同注册的还有一个Matcher对象，该对象用于描述listener期望接收事件的job或trigger。 listener是在运行的时候注册到scheduler上的，而且不会与job和trigger一样保存在JobStore中。因为listener一般是应用的一个集成点(integration point)，因此，应用每次运行的时候，listener都应该重新注册到scheduler上。 给一个job添加JobListener： scheduler.getListenerManager().addJobListener(myJobListener, KeyMatcher.jobKeyEquals(new JobKey(&quot;myJobName&quot;, &quot;myJobGroup&quot;))); 可以对matcher和key下的类进行静态导入，这样使得matcher的定义更加清晰： import static org.quartz.JobKey.*; import static org.quartz.impl.matchers.KeyMatcher.*; import static org.quartz.impl.matchers.GroupMatcher.*; import static org.quartz.impl.matchers.AndMatcher.*; import static org.quartz.impl.matchers.OrMatcher.*; import static org.quartz.impl.matchers.EverythingMatcher.*; ...etc. 静态导入后，上面的实例可以写成： scheduler.getListenerManager().addJobListener(myJobListener, jobKeyEquals(jobKey(&quot;myJobName&quot;, &quot;myJobGroup&quot;))); 给一个group下的所有job添加一个JobListener： scheduler.getListenerManager().addJobListener(myJobListener, jobGroupEquals(&quot;myJobGroup&quot;)); 给两个group下的所有job添加一个JobListener： scheduler.getListenerManager().addJobListener(myJobListener, or(jobGroupEquals(&quot;myJobGroup&quot;), jobGroupEquals(&quot;yourGroup&quot;))); 给所有的job添加一个JobListener： scheduler.getListenerManager().addJobListener(myJobListener, allJobs()); TriggerListener的注册过程与JobListener类似。 Quartz的大部分用户都不使用listener，但如果应用需要对发生的事件感兴趣，当然可以让job显示通知应用，但显然listener更方便。 原文链接 本系列教程由quartz-2.2.x官方文档翻译、整理而来，希望给同样对quartz感兴趣的朋友一些参考和帮助，有任何不当或错误之处，欢迎指正；有兴趣研究源码的同学，可以参考我对quartz-core源码的注释(进行中)。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"quartz","slug":"quartz","permalink":"http://nkcoder.github.io/tags/quartz/"}]},{"title":"Quartz教程六--CronTrigger","slug":"quartz-tutorial-06-crontrigger","date":"2014-08-02T13:44:44.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/08/02/quartz-tutorial-06-crontrigger/","link":"","permalink":"http://nkcoder.github.io/2014/08/02/quartz-tutorial-06-crontrigger/","excerpt":"如果你需要的是基于日历表示法的调度，而不是基于指定间隔的简单调度，那么CronTrigger比SimpleTrigger更合适。 使用CronTrigger，你可以配置这样的调度：“每周五的中午”，或者“每个工作日的上午9:30”，或者“在一月的每个周一、周三和周五的上午9点到10点之间每隔5分钟”。","text":"如果你需要的是基于日历表示法的调度，而不是基于指定间隔的简单调度，那么CronTrigger比SimpleTrigger更合适。 使用CronTrigger，你可以配置这样的调度：“每周五的中午”，或者“每个工作日的上午9:30”，或者“在一月的每个周一、周三和周五的上午9点到10点之间每隔5分钟”。 与SimpleTrigger一样，CronTrigger需要设置startTime属性，表示调度生效的时间，以及(可选的)endTime属性，表示调度的结束时间。 Cron表达式Cron表达式用于配置CronTrigger的实例，它由7个字段组成，字段之间由空格分开，它们表示的含义如下： 1. 秒 (Seconds) 2. 分钟 (Minutes) 3. 小时 (Hours) 4. 日(一个月的一天) (Day-of-Month) 5. 月份 (Month) 6. 周(一周的一天) (Day-of-Week) 7. 年份(可选的) (Year) 一个完整的Cron表达式的例子如字符串：”0 0 12 ? * WED” - 表示“每周三的中午12:00:00”； 每一个字段可以包含范围或者列举。比如，上例中的周字段(即”WED“)可以被替换为：”MON-FRI”, “MON,WED,FRI”, 或者”MON-WED,SAT”。 通配符()表示该字段上每一个可取的值。因此，上例中月字段上的表示“每个月”。周字段上的*表示“一周的每一天”。 所有的字段都有一些可取的值的集合。有些就很明显 - 比如，秒和分钟字段可以取0到59的整数，小时字段可以取0到23的整数，日字段(Day-of-Month)可以取1到31的整数，不过你需要注意指定的月份到底有多少天。月字段可以取0到11之间的整数，或者使用字符串表示：JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV 和 DEC。周字段(Days-of-Week)可以取的值为1到7(1表示Sunday)，或者使用字符串表示：SUN, MON, TUE, WED, THU, FRI and SAT。 字符‘/’表示增量。比如，分钟字段使用”0/15“，表示”在这个小时内，从0分钟开始，每隔15分钟“；如果分钟字段使用”3/20“，则表示”在这个小时内，从第3分钟开始，每隔20分钟“，这与在分钟字段使用“3,23,43”表示的含义是一样的。注意一个细节，“/35”的含义并不是“每隔35分钟”，而是“在这个小时内，从第0分钟开始，每隔35分钟”。 字符’?’可以用于日字段和周字段，表示“没有具体的值“。它主要用于在这两个字段的某一个中指定某个值，更好的理解请参考下面的示例（以及CrontTrigger的JavaDoc）。 字符’L’可以用于日字段和周字段，它是last的缩写，但是用于这两个字段时，含义并不相同。比如，“L”用于日子段，表示“这个月的最后一天” - 即一月的第31天，非闰年二月的第28天。如果‘L’单独用于周字段，它的含义就是“7”或者“SAT”。但是，如果‘L’用于周字段时，前面还有一个值，则表示“这个月的最后一个周××” - 比如，“6L”或“FRIL”表示“这个月的最后一个周五”。我们也可以指定与这个月最后一天的偏移量，比如“L-3”表示与这个月的最后一天相差3天。需要注意的是，如果使用选项‘L’，则不要使用列举和范围，否则结果将是不可预料的。 字符’W’表示离某一天最近的工作日（weekday）。例如，日字段上的值“15W”表示“离这个月的15号最近的工作日”。 字符’#’表示“这个月的第n个周××”。例如，在周字段上的“6#3”或“FRI#3”表示“这个月的3个周五”。 以下为一些表达式的例子及对应的含义 - 在org.quartz.CronExpression的JavaDoc有更多的描述。 Cron表达式示例CronTrigger示例1 - 每隔5分钟执行一次： 0 0/5 * * * ? CronTrigger示例2 - 每隔5分钟，且在该分钟的第10秒执行（如10:00:10 am, 10:05:10 am等）： 10 0/5 * * * ? CronTrigger示例3 - 每周三和周五的10:30, 11:30, 12:30和13:30执行： 0 30 10-13 ? * WED,FRI Crontrigger示例4 - 在每个月的第5天和第20天，上午8点到上午10点之间每隔半小时执行。注意：该trigger不会在上午10点执行，只在上午8:00, 8:30, 9:00和9:30执行： 0 0/30 8-9 5,20 * ? 提示一下，有些调度需求太复杂了，无法使用一个trigger表达 - 比如：“在上午9点和上午10点之间，每隔5分钟执行一次，且在下午1点到下午10点之间每隔20分钟执行一次”。应对这样的需求，可以创建两个trigger，注册到同一个job上即可。 创建CronTriggerCronTrigger实例可以通过TriggerBuilder（配置主要属性）和CronScheduleBuilder（配置CronTrigger专有的属性）配置。为了以DSL风格使用这些builder，需要静态导入： import static org.quartz.TriggerBuilder.*; import static org.quartz.CronScheduleBuilder.*; import static org.quartz.DateBuilder.*: 构建一个trigger，在每天的上午8点到下午5点之间每隔2分钟执行一次，如： trigger = newTrigger() .withIdentity(&quot;trigger3&quot;, &quot;group1&quot;) .withSchedule(cronSchedule(&quot;0 0/2 8-17 * * ?&quot;)) .forJob(&quot;myJob&quot;, &quot;group1&quot;) .build(); 构建一个trigger，每天上午10:42执行： trigger = newTrigger() .withIdentity(&quot;trigger3&quot;, &quot;group1&quot;) .withSchedule(dailyAtHourAndMinute(10, 42)) .forJob(myJobKey) .build(); 或者： trigger = newTrigger() .withIdentity(&quot;trigger3&quot;, &quot;group1&quot;) .withSchedule(cronSchedule(&quot;0 42 10 * * ?&quot;)) .forJob(myJobKey) .build(); 构建一个trigger，在每周三的上午10:42执行，并配置TimeZone： trigger = newTrigger() .withIdentity(&quot;trigger3&quot;, &quot;group1&quot;) .withSchedule(weeklyOnDayAndHourAndMinute(DateBuilder.WEDNESDAY, 10, 42) .inTimeZone(TimeZone.getTimeZone(&quot;America/Los_Angeles&quot;))) .forJob(myJobKey) .build(); 或者： trigger = newTrigger() .withIdentity(&quot;trigger3&quot;, &quot;group1&quot;) .withSchedule(cronSchedule(&quot;0 42 10 ? * WED&quot;) .inTimeZone(TimeZone.getTimeZone(&quot;America/Los_Angeles&quot;))) .forJob(myJobKey) .build(); CronTriggter错过触发策略下面的这些策略用于告诉Quartz，如果CronTrigger错过触发时应该采取的处理方式。（关于错过触发的场景，在本教程的Trigger介绍部分以经介绍过）。这些策略值是定义在CronTrigger上的常量，包括： MISFIRE_INSTRUCTION_IGNORE_MISFIRE_POLICY MISFIRE_INSTRUCTION_DO_NOTHING MISFIRE_INSTRUCTION_FIRE_NOW 所有的trigger都可以使用Trigger.MISFIRE_INSTRUCTION_SMART_POLICY策略，该策略也是所有trigger的默认策略。在CronTrigger中，’smart policy’策略其实就是MISFIRE_INSTRUCTION_FIRE_NOW. CronTrigger.updateAfterMisfire()的API文档中对该策略有更详细的解释。 在构造CronTrigger时，将错过触发策略作为调度的一部分进行配置（通过CronScheduleBuilder): trigger = newTrigger() .withIdentity(&quot;trigger3&quot;, &quot;group1&quot;) .withSchedule(cronSchedule(&quot;0 0/2 8-17 * * ?&quot;) .withMisfireHandlingInstructionFireAndProceed()) .forJob(&quot;myJob&quot;, &quot;group1&quot;) .build(); 原文链接 本系列教程由quartz-2.2.x官方文档翻译、整理而来，希望给同样对quartz感兴趣的朋友一些参考和帮助，有任何不当或错误之处，欢迎指正；有兴趣研究源码的同学，可以参考我对quartz-core源码的注释(进行中)。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"quartz","slug":"quartz","permalink":"http://nkcoder.github.io/tags/quartz/"}]},{"title":"Java的值传递","slug":"java-pass-by-value","date":"2014-07-26T15:53:27.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/07/26/java-pass-by-value/","link":"","permalink":"http://nkcoder.github.io/2014/07/26/java-pass-by-value/","excerpt":"java pass by value or reference，我用这句话向google求助，第一条结果就是我想要的，StackOverflow上早有大神解决了我等菜鸟的困惑。排名第一的回答虽简短，但精辟，两个小例子非常准确地说明了问题的本质。我想，如果再配两幅小图，那就完美了，故作图如下。","text":"java pass by value or reference，我用这句话向google求助，第一条结果就是我想要的，StackOverflow上早有大神解决了我等菜鸟的困惑。排名第一的回答虽简短，但精辟，两个小例子非常准确地说明了问题的本质。我想，如果再配两幅小图，那就完美了，故作图如下。 简单地回答，Java中只有值传递，没有引用传递之说；对象以引用的形式传递，但引用本身是值传递的。当将一个对象传递给函数时，该对象的内存地址将按位复制给临时变量，即函数内的临时变量和函数外的变量中都是对象的地址，都指向对象在内存中的位置，对这两个引用本身的改变是相互独立的，但通过引用去修改引用的对象则是相互影响的，因为修改的是同一个对象。 StackOverflow上作者的例子太经典了，这里还是借用一下，这里仅配图： Dog aDog = new Dog(&quot;Max&quot;); foo(aDog); aDog.getName().equals(&quot;Max&quot;); // true public void foo(Dog d) { d.getName().equals(&quot;Max&quot;); // true d = new Dog(&quot;Fifi&quot;); d.getName().equals(&quot;Fifi&quot;); // true } 在刚调用函数时： 执行d = new Dog(&quot;Fifi&quot;);后： Dog aDog = new Dog(&quot;Max&quot;); foo(aDog); aDog.getName().equals(&quot;Fifi&quot;); // true public void foo(Dog d) { d.getName().equals(&quot;Max&quot;); // true d.setName(&quot;Fifi&quot;); } 在刚函数调用时： 调用d.getName(&quot;Fifi&quot;)后： 参考： Is Java “pass-by-reference” or “pass-by-value”?","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"java","slug":"java","permalink":"http://nkcoder.github.io/tags/java/"}]},{"title":"Quartz教程五--SimpleTrigger","slug":"quartz-tutorial-05-simple-trigger-intro","date":"2014-07-23T00:08:19.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/07/23/quartz-tutorial-05-simple-trigger-intro/","link":"","permalink":"http://nkcoder.github.io/2014/07/23/quartz-tutorial-05-simple-trigger-intro/","excerpt":"SimpleTrigger可以满足的调度需求是：在具体的时间点执行一次，或者在具体的时间点执行，并且以指定的间隔重复执行若干次。比如，你有一个trigger，你可以设置它在2015年1月13日的上午11:23:54准时触发，或者在这个时间点触发，并且每隔2秒触发一次，一共重复5次。 根据描述，你可能已经发现了，SimpleTrigger的属性包括：开始时间、结束时间、重复次数以及重复的间隔。这些属性的含义与你所期望的是一致的，只是关于结束时间有一些地方需要注意。","text":"SimpleTrigger可以满足的调度需求是：在具体的时间点执行一次，或者在具体的时间点执行，并且以指定的间隔重复执行若干次。比如，你有一个trigger，你可以设置它在2015年1月13日的上午11:23:54准时触发，或者在这个时间点触发，并且每隔2秒触发一次，一共重复5次。 根据描述，你可能已经发现了，SimpleTrigger的属性包括：开始时间、结束时间、重复次数以及重复的间隔。这些属性的含义与你所期望的是一致的，只是关于结束时间有一些地方需要注意。 重复次数，可以是0、正整数，以及常量SimpleTrigger.REPEAT_INDEFINITELY。重复的间隔，必须是0，或者long型的正数，表示毫秒。注意，如果重复间隔为0，trigger将会以重复次数并发执行(或者以scheduler可以处理的近似并发数)。 如果你还不熟悉DateBuilder，了解后你会发现使用它可以非常方便地构造基于开始时间(或终止时间)的调度策略。 endTime属性的值会覆盖设置重复次数的属性值；比如，你可以创建一个trigger，在终止时间之前每隔10秒执行一次，你不需要去计算在开始时间和终止时间之间的重复次数，只需要设置终止时间并将重复次数设置为REPEAT_INDEFINITELY(当然，你也可以将重复次数设置为一个很大的值，并保证该值比trigger在终止时间之前实际触发的次数要大即可)。 SimpleTrigger实例通过TriggerBuilder设置主要的属性，通过SimpleScheduleBuilder设置与SimpleTrigger相关的属性。要使用这些builder的静态方法，需要静态导入： import static org.quartz.TriggerBuilder.*; import static org.quartz.SimpleScheduleBuilder.*; import static org.quartz.DateBuilder.*: 下面的例子，是基于简单调度(simple schedule)创建的trigger。建议都看一下，因为每个例子都包含一个不同的实现点： 指定时间开始触发，不重复： SimpleTrigger trigger = (SimpleTrigger) newTrigger() .withIdentity(&quot;trigger1&quot;, &quot;group1&quot;) .startAt(myStartTime) // some Date .forJob(&quot;job1&quot;, &quot;group1&quot;) // identify job with name, group strings .build(); 指定时间触发，每隔10秒执行一次，重复10次： trigger = newTrigger() .withIdentity(&quot;trigger3&quot;, &quot;group1&quot;) .startAt(myTimeToStartFiring) // if a start time is not given (if this line were omitted), &quot;now&quot; is implied .withSchedule(simpleSchedule() .withIntervalInSeconds(10) .withRepeatCount(10)) // note that 10 repeats will give a total of 11 firings .forJob(myJob) // identify job with handle to its JobDetail itself .build(); 5分钟以后开始触发，仅执行一次： trigger = (SimpleTrigger) newTrigger() .withIdentity(&quot;trigger5&quot;, &quot;group1&quot;) .startAt(futureDate(5, IntervalUnit.MINUTE)) // use DateBuilder to create a date in the future .forJob(myJobKey) // identify job with its JobKey .build(); 立即触发，每个5分钟执行一次，直到22:00： trigger = newTrigger() .withIdentity(&quot;trigger7&quot;, &quot;group1&quot;) .withSchedule(simpleSchedule() .withIntervalInMinutes(5) .repeatForever()) .endAt(dateOf(22, 0, 0)) .build(); 在下一小时整点触发，每个2小时执行一次，一直重复： trigger = newTrigger() .withIdentity(&quot;trigger8&quot;) // because group is not specified, &quot;trigger8&quot; will be in the default group .startAt(evenHourDate(null)) // get the next even-hour (minutes and seconds zero (&quot;00:00&quot;)) .withSchedule(simpleSchedule() .withIntervalInHours(2) .repeatForever()) // note that in this example, &apos;forJob(..)&apos; is not called which is valid // if the trigger is passed to the scheduler along with the job .build(); scheduler.scheduleJob(trigger, job); 请查阅TriggerBuilder和SimpleScheduleBuilder提供的方法，以便对上述示例中未提到的选项有所了解。 TriggerBuilder(以及Quartz的其它builder)会为那些没有被显式设置的属性选择合理的默认值。比如：如果你没有调用withIdentity(..)方法，TriggerBuilder会为trigger生成一个随机的名称；如果没有调用startAt(..)方法，则默认使用当前时间，即trigger立即生效。 SimpleTrigger Misfire策略SimpleTrigger有几个misfire相关的策略，告诉quartz当misfire发生的时候应该如何处理。(Misfire策略参考教程四：Trigger介绍)。这些策略以常量的形式在SimpleTrigger中定义(JavaDoc中介绍了它们的功能)。这些策略包括： SimpleTrigger的Misfire策略常量： MISFIRE_INSTRUCTION_IGNORE_MISFIRE_POLICY MISFIRE_INSTRUCTION_FIRE_NOW MISFIRE_INSTRUCTION_RESCHEDULE_NOW_WITH_EXISTING_REPEAT_COUNT MISFIRE_INSTRUCTION_RESCHEDULE_NOW_WITH_REMAINING_REPEAT_COUNT MISFIRE_INSTRUCTION_RESCHEDULE_NEXT_WITH_REMAINING_COUNT MISFIRE_INSTRUCTION_RESCHEDULE_NEXT_WITH_EXISTING_COUNT 回顾一下，所有的trigger都有一个Trigger.MISFIRE_INSTRUCTION_SMART_POLICY策略可以使用，该策略也是所有trigger的默认策略。 如果使用smart policy，SimpleTrigger会根据实例的配置及状态，在所有MISFIRE策略中动态选择一种Misfire策略。SimpleTrigger.updateAfterMisfire()的JavaDoc中解释了该动态行为的具体细节。 在使用SimpleTrigger构造trigger时，misfire策略作为基本调度(simple schedule)的一部分进行配置(通过SimpleSchedulerBuilder设置)： trigger = newTrigger() .withIdentity(&quot;trigger7&quot;, &quot;group1&quot;) .withSchedule(simpleSchedule() .withIntervalInMinutes(5) .repeatForever() .withMisfireHandlingInstructionNextWithExistingCount()) .build(); 原文链接 本系列教程由quartz-2.2.x官方文档翻译、整理而来，希望给同样对quartz感兴趣的朋友一些参考和帮助，有任何不当或错误之处，欢迎指正；有兴趣研究源码的同学，可以参考我对quartz-core源码的注释(进行中)。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"quartz","slug":"quartz","permalink":"http://nkcoder.github.io/tags/quartz/"}]},{"title":"Quartz教程四--Trigger介绍","slug":"quartz-tutorial-04-trigger","date":"2014-07-16T12:51:37.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/07/16/quartz-tutorial-04-trigger/","link":"","permalink":"http://nkcoder.github.io/2014/07/16/quartz-tutorial-04-trigger/","excerpt":"与job一样，trigger也很容易使用，但是还有一些扩展选项需要理解，以便更好地使用quartz。trigger也有很多类型，可以根据实际需要来选择。 最常用的两种trigger会分别在教程五：SimpleTriggers和教程六：CronTrigger中讲到；","text":"与job一样，trigger也很容易使用，但是还有一些扩展选项需要理解，以便更好地使用quartz。trigger也有很多类型，可以根据实际需要来选择。 最常用的两种trigger会分别在教程五：SimpleTriggers和教程六：CronTrigger中讲到； Trigger的公共属性所有类型的trigger都有TriggerKey这个属性，表示trigger的身份；除此之外，trigger还有很多其它的公共属性。这些属性，在构建trigger的时候可以通过TriggerBuilder设置。 trigger的公共属性有： jobKey属性：当trigger触发时被执行的job的身份； startTime属性：设置trigger第一次触发的时间；该属性的值是java.util.Date类型，表示某个指定的时间点；有些类型的trigger，会在设置的startTime时立即触发，有些类型的trigger，表示其触发是在startTime之后开始生效。比如，现在是1月份，你设置了一个trigger–”在每个月的第5天执行”，然后你将startTime属性设置为4月1号，则该trigger第一次触发会是在几个月以后了(即4月5号)。 endTime属性：表示trigger失效的时间点。比如，”每月第5天执行”的trigger，如果其endTime是7月1号，则其最后一次执行时间是6月5号。 其它的属性，会在下面的小节中解释。 优先级(priority)如果你的trigger很多(或者Quartz线程池的工作线程太少)，Quartz可能没有足够的资源同时触发所有的trigger；这种情况下，你可能希望控制哪些trigger优先使用Quartz的工作线程，要达到该目的，可以在trigger上设置priority属性。比如，你有N个trigger需要同时触发，但只有Z个工作线程，优先级最高的Z个trigger会被首先触发。如果没有为trigger设置优先级，trigger使用默认优先级，值为5；priority属性的值可以是任意整数，正数、负数都可以。 注意：只有同时触发的trigger之间才会比较优先级。10:59触发的trigger总是在11:00触发的trigger之前执行。 注意：如果trigger是可恢复的，在恢复后再调度时，优先级与原trigger是一样的。 错过触发(misfire)trigger还有一个重要的属性misfire；如果scheduler关闭了，或者Quartz线程池中没有可用的线程来执行job，此时持久性的trigger就会错过(miss)其触发时间，即错过触发(misfire)。不同类型的trigger，有不同的misfire机制。它们默认都使用“智能机制(smart policy)”，即根据trigger的类型和配置动态调整行为。当scheduler启动的时候，查询所有错过触发(misfire)的持久性trigger。然后根据它们各自的misfire机制更新trigger的信息。当你在项目中使用Quartz时，你应该对各种类型的trigger的misfire机制都比较熟悉，这些misfire机制在JavaDoc中有说明。关于misfire机制的细节，会在讲到具体的trigger时作介绍。 日历(calendar)Quartz的Calendar对象(不是java.util.Calendar对象)可以在定义和存储trigger的时候与trigger进行关联。Calendar用于从trigger的调度计划中排除时间段。比如，可以创建一个trigger，每个工作日的上午9:30执行，然后增加一个Calendar，排除掉所有的商业节日。 任何实现了Calendar接口的可序列化对象都可以作为Calendar对象，Calendar接口如下： package org.quartz; public interface Calendar { public boolean isTimeIncluded(long timeStamp); public long getNextIncludedTime(long timeStamp); } 注意到这些方法的参数类型为long。你也许猜到了，他们就是毫秒单位的时间戳。即Calendar排除时间段的单位可以精确到毫秒。你也许对“排除一整天”的Calendar比较感兴趣。Quartz提供的org.quartz.impl.HolidayCalendar类可以很方便地实现。 Calendar必须先实例化，然后通过addCalendar()方法注册到scheduler。如果使用HolidayCalendar，实例化后，需要调用addExcludedDate(Date date)方法从调度计划中排除时间段。以下示例是将同一个Calendar实例用于多个trigger： HolidayCalendar cal = new HolidayCalendar(); cal.addExcludedDate( someDate ); cal.addExcludedDate( someOtherDate ); sched.addCalendar(&quot;myHolidays&quot;, cal, false); Trigger t = newTrigger() .withIdentity(&quot;myTrigger&quot;) .forJob(&quot;myJob&quot;) .withSchedule(dailyAtHourAndMinute(9, 30)) // execute job daily at 9:30 .modifiedByCalendar(&quot;myHolidays&quot;) // but not on holidays .build(); // .. schedule job with trigger Trigger t2 = newTrigger() .withIdentity(&quot;myTrigger2&quot;) .forJob(&quot;myJob2&quot;) .withSchedule(dailyAtHourAndMinute(11, 30)) // execute job daily at 11:30 .modifiedByCalendar(&quot;myHolidays&quot;) // but not on holidays .build(); // .. schedule job with trigger2 构造trigger的细节将在接下来的几节中讲到。就现在，只需要知道上面的代码创建了两个trigger，都是每天执行一次。但是在Calendar定义的时间段内，trigger的执行被跳过。 org.quartz.impl.calendar包下有各种Calendar的实现，根据需要进行选择。 原文链接 本系列教程由quartz-2.2.x官方文档翻译、整理而来，希望给同样对quartz感兴趣的朋友一些参考和帮助，有任何不当或错误之处，欢迎指正；有兴趣研究源码的同学，可以参考我对quartz-core源码的注释(进行中)。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"quartz","slug":"quartz","permalink":"http://nkcoder.github.io/tags/quartz/"}]},{"title":"Linux下命令行删除符号链接","slug":"linux-delete-symbolic-link","date":"2014-07-16T02:34:25.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/07/16/linux-delete-symbolic-link/","link":"","permalink":"http://nkcoder.github.io/2014/07/16/linux-delete-symbolic-link/","excerpt":"1. 问题Linux下，如何从命令行删除符号链接(软链接)？","text":"1. 问题Linux下，如何从命令行删除符号链接(软链接)？ 2. 命令：删除符号链接，使用rm命令或者unlink命令，后接文件名或目录名，如果是目录，注意目录后，不要加/。 语法： rm linkname/dirname unlink linkname/dirname 示例： nkcoder@ubuntu:~$ cd ~ nkcoder@ubuntu:~$ ln -s /usr/local/jetty/start.ini start.ini.bak nkcoder@ubuntu:~$ ll start.ini.bak lrwxrwxrwx 1 nkcoder nkcoder 26 Jul 16 10:28 start.ini.bak -&gt; /usr/local/jetty/start.ini nkcoder@ubuntu:~$ unlink start.ini.bak nkcoder@ubuntu:~$ ln -s /usr/local/jetty jetty.bak nkcoder@ubuntu:~$ ll jetty.bak lrwxrwxrwx 1 nkcoder nkcoder 16 Jul 16 10:28 jetty.bak -&gt; /usr/local/jetty/ nkcoder@ubuntu:~$ rm jetty.bak 参考 Linux Delete Symbolic Link ( Softlink )","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://nkcoder.github.io/tags/linux/"}]},{"title":"Mysql重连，连接丢失的问题","slug":"mysql-reconnect-packet-lost","date":"2014-07-12T12:58:06.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/07/12/mysql-reconnect-packet-lost/","link":"","permalink":"http://nkcoder.github.io/2014/07/12/mysql-reconnect-packet-lost/","excerpt":"1. 错误信息：Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: The last packet successfully received from the server was 20,820,001 milliseconds ago. The last packet sent successfully to the server was 20,820,002 milliseconds ago. is longer than the server configured value of &apos;wait_timeout&apos;. You should consider either expiring and/or testing connection validity before use in your application, increasing the server configured values for client timeouts, or using the Connector/J connection property &apos;autoReconnect=true&apos; to avoid this problem.","text":"1. 错误信息：Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: The last packet successfully received from the server was 20,820,001 milliseconds ago. The last packet sent successfully to the server was 20,820,002 milliseconds ago. is longer than the server configured value of &apos;wait_timeout&apos;. You should consider either expiring and/or testing connection validity before use in your application, increasing the server configured values for client timeouts, or using the Connector/J connection property &apos;autoReconnect=true&apos; to avoid this problem. at sun.reflect.GeneratedConstructorAccessor29.newInstance(Unknown Source) ~[na:na] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.7.0_51] at java.lang.reflect.Constructor.newInstance(Constructor.java:526) ~[na:1.7.0_51] at com.mysql.jdbc.Util.handleNewInstance(Util.java:411) ~[mysql-connector-java-5.1.29.jar:na] at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1129) ~[mysql-connector-java-5.1.29.jar:na] at com.mysql.jdbc.MysqlIO.send(MysqlIO.java:3988) ~[mysql-connector-java-5.1.29.jar:na] at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2598) ~[mysql-connector-java-5.1.29.jar:na] at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2778) ~[mysql-connector-java-5.1.29.jar:na] at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2828) ~[mysql-connector-java-5.1.29.jar:na] at com.mysql.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:5372) ~[mysql-connector-java-5.1.29.jar:na] at com.mchange.v2.c3p0.impl.NewProxyConnection.setAutoCommit(NewProxyConnection.java:881) ~[c3p0-0.9.1.1.jar:0.9.1.1] at org.quartz.impl.jdbcjobstore.AttributeRestoringConnectionInvocationHandler.setAutoCommit(AttributeRestoringConnectionInvocationHandler.java:98) ~[quartz-2.2.1.jar:na] 2. 解决方法 如果使用的是JDBC，在JDBC URL上添加?autoReconnect=true，如： jdbc:mysql://10.10.10.10:3306/mydb?autoReconnect=true 如果是在Spring中使用DBCP连接池，在定义datasource增加属性validationQuery和testOnBorrow，如： 如果是在Spring中使用c3p0连接池，则在定义datasource的时候，添加属性testConnectionOnCheckin和testConnectionOnCheckout，如： 参考 MySQL reconnect issues","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://nkcoder.github.io/tags/mysql/"}]},{"title":"【MyBatis】元素类型为\"configuration\"的内容必须匹配","slug":"mybatis-config-order-error","date":"2014-07-09T14:23:31.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/07/09/mybatis-config-order-error/","link":"","permalink":"http://nkcoder.github.io/2014/07/09/mybatis-config-order-error/","excerpt":"1. 错误信息：org.apache.ibatis.exceptions.PersistenceException: ### Error building SqlSession. ### Cause: org.apache.ibatis.builder.BuilderException: Error creating document instance. Cause: org.xml.sax.SAXParseException; lineNumber: 45; columnNumber: 17; 元素类型为 &quot;configuration&quot; 的内容必须匹配 &quot;(properties?,settings?,typeAliases?,typeHandlers?,objectFactory?,objectWrapperFactory?,plugins?,environments?,databaseIdProvider?,mappers?)&quot;。","text":"1. 错误信息：org.apache.ibatis.exceptions.PersistenceException: ### Error building SqlSession. ### Cause: org.apache.ibatis.builder.BuilderException: Error creating document instance. Cause: org.xml.sax.SAXParseException; lineNumber: 45; columnNumber: 17; 元素类型为 &quot;configuration&quot; 的内容必须匹配 &quot;(properties?,settings?,typeAliases?,typeHandlers?,objectFactory?,objectWrapperFactory?,plugins?,environments?,databaseIdProvider?,mappers?)&quot;。 2. 原因：在配置mybatis-config.xml时，其中的节点是有顺序的，配置顺序依次为：properties/settings/typeAliases/typeHandlers/objectFactory/objectWrapperFactory/plugins/environments/databaseIdProvider/mappers 我出现该错是因为将settings节点放到了environments后面。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://nkcoder.github.io/tags/mybatis/"}]},{"title":"【Guava】使用前置条件进行合法性检查","slug":"guava-basic-preconditions","date":"2014-07-08T14:55:59.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/07/08/guava-basic-preconditions/","link":"","permalink":"http://nkcoder.github.io/2014/07/08/guava-basic-preconditions/","excerpt":"本系列文章是在学习Google Guava 17.0库的过程中的整理和笔记，不是完全翻译，欢迎交流。 1. 前置条件检查 Preconditions类提供了很多static方法，用于对方法或构造函数的参数进行合法性检查，强烈推荐使用； 关于性能：Preconditions是为了提高代码的可读性，但有些情况下，可能会影响系统性能；在性能敏感的环境下，总是可以使用如下形式： if (value &lt; 0.0) { throw new IllegalArgumentException(&quot;negative value: &quot; + value); }","text":"本系列文章是在学习Google Guava 17.0库的过程中的整理和笔记，不是完全翻译，欢迎交流。 1. 前置条件检查 Preconditions类提供了很多static方法，用于对方法或构造函数的参数进行合法性检查，强烈推荐使用； 关于性能：Preconditions是为了提高代码的可读性，但有些情况下，可能会影响系统性能；在性能敏感的环境下，总是可以使用如下形式： if (value &lt; 0.0) { throw new IllegalArgumentException(&quot;negative value: &quot; + value); } 在使用com.google.common时，应尽量避免使用Objects.requireNonNull(Object)，推荐使用checkNotNull(Object)或者Verify.verifyNotNull(Object)，而且它们支持%s的格式化信息； checkNotNull(Object) 含义明确，不易混淆，而且参数合法时返回原值，可以用于‘检查并赋值‘的场合，如：this.field = checkNotNull(field)； 推荐将多个前置条件分开检查，并在异常消息中提供有用的信息，这样有利于程序的调试； 2. Preconditions API每一类方法都有三种变体，除了要检查的参数外，额外的参数组合有： 不带参数，抛出异常时不带异常信息； 带一个Object参数，抛出异常时，异常信息为：Ojbect.toString()； 带一个String参数，和不定个数的Object参数；行为类似于printf，但String中仅支持%s格式符，如： checkArgument(i &gt;= 0, “Argument was %s but expected nonnegative”, i); checkArgument(i &lt; j, “Expected i &lt; j, but %s &gt; %s”, i, j); 主要的static方法有： checkArgument(boolean) 检查参数条件是否为true； checkNotNull(T) 检查参数是否不为null，如果不为null，直接返回参数值； checkState(boolean) 检查对象的状态，不依赖于方法的参数； checkElementIndex(int index, int size) 检查index是否在[0, size)之间，其中size可以表示String、List或Array的长度； checkPositionIndex(int index, int size) 检查index是否在[0, size]之间，其中size可以表示String、List或Array的长度； checkPositionIndexes(int start, int end, int size) 检查区间[start, end)是否为[0, size]的子区间；(该方法仅此一种形式，无其它参数版)。 参考 PreconditionsExplained Preconditions API","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"guava","slug":"guava","permalink":"http://nkcoder.github.io/tags/guava/"}]},{"title":"【Guava】避免使用null","slug":"guava-basic-avoid-null","date":"2014-07-08T14:36:58.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/07/08/guava-basic-avoid-null/","link":"","permalink":"http://nkcoder.github.io/2014/07/08/guava-basic-avoid-null/","excerpt":"本系列文章是在学习Google Guava 17.0库的过程中的整理和笔记，不是完全翻译，欢迎交流。 1. 避免使用null 集合中不要接受null，对于null采取快速失败的策略； NULL的含义是模糊的，如Map.get(key)，如果返回null，可能是该map中不存在该key，也可能是存在该key，但对应的value为null； 如果打算在map中存放key对应的value为null的entry，不要这么做，宁愿另外用一个Set保存value非null的key（或value为null的key），分开操作，在你的上下文中，value为null的key应该具有明确的含义；","text":"本系列文章是在学习Google Guava 17.0库的过程中的整理和笔记，不是完全翻译，欢迎交流。 1. 避免使用null 集合中不要接受null，对于null采取快速失败的策略； NULL的含义是模糊的，如Map.get(key)，如果返回null，可能是该map中不存在该key，也可能是存在该key，但对应的value为null； 如果打算在map中存放key对应的value为null的entry，不要这么做，宁愿另外用一个Set保存value非null的key（或value为null的key），分开操作，在你的上下文中，value为null的key应该具有明确的含义； 2. Optional Optional是一个引用，如果其中有值，则为’present’，如果没有值，则为’absent’，但不会“包含null”；Optional可以看做是对null的友好的包装，在设置或取值时，会提醒你判断它的引用状态，而null很容易让人忘记检查其合法性； 3. Optional API构造Optional的静态方法： Optional.of(T) 根据非null的参数，构造一个Optional实例； Optional.absent() 构造一个不包含引用的(absent)的Optional实例； Optional.fromNullable(T) 从可为null的参数中构造一个Optional实例，如果参数不为null，则状态为present，否则状态为absent； Optional对象的方法： boolean isPresent() 如果该optional实例有非null引用，返回true，否则返回false； T get() 如果该optional实例有非null引用，返回引用对象的值，否则，抛出IllegalStateException； T or(T) 如果该optional实例有非null引用，返回引用对象的值，否则，返回参数指定的默认值； T orNull() 如果该optional实例有非null引用，返回引用对象的值，否则，返回null； Set&lt;T&gt; asSet() 如果该optional实例包含非null引用，以该实例的值构造一个Set并返回，否则返回一个空Set； 4. Strings中相关方法注意，不要将null串与空串混为一谈，Strings工具类中相关的static方法有： emptyToNull(String) 如果参数不为空，返回参数值，否则返回null； nullToEmpty(String) 如果参数不为null，返回该参数，否则返回空串(“”)； isNullOrEmpty(String) 如果参数为null或为空，返回true； 参考 Using and avoiding null Optional doc","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"guava","slug":"guava","permalink":"http://nkcoder.github.io/tags/guava/"}]},{"title":"Quartz教程三--Job与JobDetail介绍","slug":"quartz-tutorial-03-job-jobdetail","date":"2014-06-27T00:21:54.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/06/27/quartz-tutorial-03-job-jobdetail/","link":"","permalink":"http://nkcoder.github.io/2014/06/27/quartz-tutorial-03-job-jobdetail/","excerpt":"正如在教程二中讲到的，Job实现起来很容易，该接口只有一个“execute”方法。本节主要关注：Job的特点、Job接口的execute方法以及JobDetail。 你定义了一个实现Job接口的类，这个类仅仅表明该job需要完成什么类型的任务，除此之外，Quartz还需要知道该Job实例所包含的属性；这将由JobDetail类来完成。","text":"正如在教程二中讲到的，Job实现起来很容易，该接口只有一个“execute”方法。本节主要关注：Job的特点、Job接口的execute方法以及JobDetail。 你定义了一个实现Job接口的类，这个类仅仅表明该job需要完成什么类型的任务，除此之外，Quartz还需要知道该Job实例所包含的属性；这将由JobDetail类来完成。 JobDetail实例是通过JobBuilder类创建的，导入该类下的所有静态方法，会让你编码时有DSL的感觉： import static org.quartz.JobBuilder.*; 让我们先看看Job的特征（nature）以及Job实例的生命期。不妨先回头看看教程一中的代码片段： // define the job and tie it to our HelloJob class JobDetail job = newJob(HelloJob.class) .withIdentity(&quot;myJob&quot;, &quot;group1&quot;) // name &quot;myJob&quot;, group &quot;group1&quot; .build(); // Trigger the job to run now, and then every 40 seconds Trigger trigger = newTrigger() .withIdentity(&quot;myTrigger&quot;, &quot;group1&quot;) .startNow() .withSchedule(simpleSchedule() .withIntervalInSeconds(40) .repeatForever()) .build(); // Tell quartz to schedule the job using our trigger sched.scheduleJob(job, trigger); “HelloJob”类可以如下定义： public class HelloJob implements Job { public HelloJob() { } public void execute(JobExecutionContext context) throws JobExecutionException { System.err.println(&quot;Hello! HelloJob is executing.&quot;); } } 可以看到，我们传给scheduler一个JobDetail实例，因为我们在创建JobDetail时，将要执行的job的类名传给了JobDetail，所以scheduler就知道了要执行何种类型的job；每次当scheduler执行job时，在调用其execute(…)方法之前会创建该类的一个新的实例；执行完毕，对该实例的引用就被丢弃了，实例会被垃圾回收；这种执行策略带来的一个后果是，job必须有一个无参的构造函数（当使用默认的JobFactory时）；另一个后果是，在job类中，不应该定义有状态的数据属性，因为在job的多次执行中，这些属性的值不会保留。 那么如何给job实例增加属性或配置呢？如何在job的多次执行中，跟踪job的状态呢？答案就是:JobDataMap，JobDetail对象的一部分。 JobDataMapJobDataMap中可以包含不限量的（序列化的）数据对象，在job实例执行的时候，可以使用其中的数据；JobDataMap是Java Map接口的一个实现，额外增加了一些便于存取基本类型的数据的方法。 将job加入到scheduler之前，在构建JobDetail时，可以将数据放入JobDataMap，如下示例： JobDetail job = newJob(DumbJob.class) .withIdentity(&quot;myJob&quot;, &quot;group1&quot;) // name &quot;myJob&quot;, group &quot;group1&quot; .usingJobData(&quot;jobSays&quot;, &quot;Hello World!&quot;) .usingJobData(&quot;myFloatValue&quot;, 3.141f) .build(); 在job的执行过程中，可以从JobDataMap中取出数据，如下示例： public class DumbJob implements Job { public DumbJob() { } public void execute(JobExecutionContext context) throws JobExecutionException { JobKey key = context.getJobDetail().getKey(); JobDataMap dataMap = context.getJobDetail().getJobDataMap(); String jobSays = dataMap.getString(&quot;jobSays&quot;); float myFloatValue = dataMap.getFloat(&quot;myFloatValue&quot;); System.err.println(&quot;Instance &quot; + key + &quot; of DumbJob says: &quot; + jobSays + &quot;, and val is: &quot; + myFloatValue); } } 如果你使用的是持久化的存储机制（本教程的JobStore部分会讲到），在决定JobDataMap中存放什么数据的时候需要小心，因为JobDataMap中存储的对象都会被序列化，因此很可能会导致类的版本不一致的问题；Java的标准类型都很安全，如果你已经有了一个类的序列化后的实例，某个时候，别人修改了该类的定义，此时你需要确保对类的修改没有破坏兼容性；更多细节，参考现实中的序列化问题。另外，你也可以配置JDBC-JobStore和JobDataMap，使得map中仅允许存储基本类型和String类型的数据，这样可以避免后续的序列化问题。 如果你在job类中，为JobDataMap中存储的数据的key增加set方法（如在上面示例中，增加setJobSays(String val)方法），那么Quartz的默认JobFactory实现在job被实例化的时候会自动调用这些set方法，这样你就不需要在execute()方法中显式地从map中取数据了。 在Job执行时，JobExecutionContext中的JobDataMap为我们提供了很多的便利。它是JobDetail中的JobDataMap和Trigger中的JobDataMap的并集，但是如果存在相同的数据，则后者会覆盖前者的值。 下面的示例，在job执行时，从JobExecutionContext中获取合并后的JobDataMap： public class DumbJob implements Job { public DumbJob() { } public void execute(JobExecutionContext context) throws JobExecutionException { JobKey key = context.getJobDetail().getKey(); JobDataMap dataMap = context.getMergedJobDataMap(); // Note the difference from the previous example String jobSays = dataMap.getString(&quot;jobSays&quot;); float myFloatValue = dataMap.getFloat(&quot;myFloatValue&quot;); ArrayList state = (ArrayList)dataMap.get(&quot;myStateData&quot;); state.add(new Date()); System.err.println(&quot;Instance &quot; + key + &quot; of DumbJob says: &quot; + jobSays + &quot;, and val is: &quot; + myFloatValue); } } 如果你希望使用JobFactory实现数据的自动“注入”，则示例代码为： public class DumbJob implements Job { String jobSays; float myFloatValue; ArrayList state; public DumbJob() { } public void execute(JobExecutionContext context) throws JobExecutionException { JobKey key = context.getJobDetail().getKey(); JobDataMap dataMap = context.getMergedJobDataMap(); // Note the difference from the previous example state.add(new Date()); System.err.println(&quot;Instance &quot; + key + &quot; of DumbJob says: &quot; + jobSays + &quot;, and val is: &quot; + myFloatValue); } public void setJobSays(String jobSays) { this.jobSays = jobSays; } public void setMyFloatValue(float myFloatValue) { myFloatValue = myFloatValue; } public void setState(ArrayList state) { state = state; } } 你也许发现，整体上看代码更多了，但是execute()方法中的代码更简洁了。而且，虽然代码更多了，但如果你的IDE可以自动生成setter方法，你就不需要写代码调用相应的方法从JobDataMap中获取数据了，所以你实际需要编写的代码更少了。当前，如何选择，由你决定。 Job实例很多用户对于Job实例到底由什么构成感到很迷惑。我们在这里解释一下，并在接下来的小节介绍job状态和并发。 你可以只创建一个job类，然后创建多个与该job关联的JobDetail实例，每一个实例都有自己的属性集和JobDataMap，最后，将所有的实例都加到scheduler中。 比如，你创建了一个实现Job接口的类“SalesReportJob”。该job需要一个参数（通过JobdataMap传入），表示负责该销售报告的销售员的名字。因此，你可以创建该job的多个实例（JobDetail），比如“SalesReportForJoe”、“SalesReportForMike”，将“joe”和“mike”作为JobDataMap的数据传给对应的job实例。 当一个trigger被触发时，与之关联的JobDetail实例会被加载，JobDetail引用的job类通过配置在Scheduler上的JobFactory进行初始化。默认的JobFactory实现，仅仅是调用job类的newInstance()方法，然后尝试调用JobDataMap中的key的setter方法。你也可以创建自己的JobFactory实现，比如让你的IOC或DI容器可以创建/初始化job实例。 在Quartz的描述语言中，我们将保存后的JobDetail称为“job定义”或者“JobDetail实例”,将一个正在执行的job称为“job实例”或者“job定义的实例”。当我们使用“job”时，一般指代的是job定义，或者JobDetail；当我们提到实现Job接口的类时，通常使用“job类”。 Job状态与并发关于job的状态数据（即JobDataMap）和并发性，还有一些地方需要注意。在job类上可以加入一些注解，这些注解会影响job的状态和并发性。 @DisallowConcurrentExecution：将该注解加到job类上，告诉Quartz不要并发地执行同一个job定义（这里指特定的job类）的多个实例。请注意这里的用词。拿前一小节的例子来说，如果“SalesReportJob”类上有该注解，则同一时刻仅允许执行一个“SalesReportForJoe”实例，但可以并发地执行“SalesReportForMike”类的一个实例。所以该限制是针对JobDetail的，而不是job类的。但是我们认为（在设计Quartz的时候）应该将该注解放在job类上，因为job类的改变经常会导致其行为发生变化。 @PersistJobDataAfterExecution：将该注解加在job类上，告诉Quartz在成功执行了job类的execute方法后（没有发生任何异常），更新JobDetail中JobDataMap的数据，使得该job（即JobDetail）在下一次执行的时候，JobDataMap中是更新后的数据，而不是更新前的旧数据。和 @DisallowConcurrentExecution注解一样，尽管注解是加在job类上的，但其限制作用是针对job实例的，而不是job类的。由job类来承载注解，是因为job类的内容经常会影响其行为状态（比如，job类的execute方法需要显式地“理解”其”状态“）。 如果你使用了@PersistJobDataAfterExecution注解，我们强烈建议你同时使用@DisallowConcurrentExecution注解，因为当同一个job（JobDetail）的两个实例被并发执行时，由于竞争，JobDataMap中存储的数据很可能是不确定的。 Job的其它特性通过JobDetail对象，可以给job实例配置的其它属性有： Durability：如果一个job是非持久的，当没有活跃的trigger与之关联的时候，会被自动地从scheduler中删除。也就是说，非持久的job的生命期是由trigger的存在与否决定的； RequestsRecovery：如果一个job是可恢复的，并且在其执行的时候，scheduler发生硬关闭（hard shutdown)（比如运行的进程崩溃了，或者关机了），则当scheduler重新启动的时候，该job会被重新执行。此时，该job的JobExecutionContext.isRecovering() 返回true。 JobExecutionException最后，是关于Job.execute(..)方法的一些额外细节。execute方法中仅允许抛出一种类型的异常（包括RuntimeExceptions），即JobExecutionException。因此，你应该将execute方法中的所有内容都放到一个”try-catch”块中。你也应该花点时间看看JobExecutionException的文档，因为你的job可以使用该异常告诉scheduler，你希望如何来处理发生的异常。 原文链接 本系列教程由quartz-2.2.x官方文档翻译、整理而来，希望给同样对quartz感兴趣的朋友一些参考和帮助，有任何不当或错误之处，欢迎指正；有兴趣研究源码的同学，可以参考我对quartz-core源码的注释(进行中)。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"quartz","slug":"quartz","permalink":"http://nkcoder.github.io/tags/quartz/"}]},{"title":"Quartz教程二--API、Job与Trigger","slug":"quartz-tutorial-02-api-job-trigger","date":"2014-06-24T14:42:53.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/06/24/quartz-tutorial-02-api-job-trigger/","link":"","permalink":"http://nkcoder.github.io/2014/06/24/quartz-tutorial-02-api-job-trigger/","excerpt":"Quartz APIQuartz API核心接口有： Scheduler - 与scheduler交互的主要API； Job - 你通过scheduler执行任务，你的任务类需要实现的接口； JobDetail - 定义Job的实例； Trigger - 触发Job的执行； JobBuilder - 定义和创建JobDetail实例的接口; TriggerBuilder - 定义和创建Trigger实例的接口；","text":"Quartz APIQuartz API核心接口有： Scheduler - 与scheduler交互的主要API； Job - 你通过scheduler执行任务，你的任务类需要实现的接口； JobDetail - 定义Job的实例； Trigger - 触发Job的执行； JobBuilder - 定义和创建JobDetail实例的接口; TriggerBuilder - 定义和创建Trigger实例的接口； Scheduler的生命期，从SchedulerFactory创建它时开始，到Scheduler调用shutdown()方法时结束；Scheduler被创建后，可以增加、删除和列举Job和Trigger，以及执行其它与调度相关的操作（如暂停Trigger）。但是，Scheduler只有在调用start()方法后，才会真正地触发trigger（即执行job），见教程一。 Quartz提供的“builder”类，可以认为是一种领域特定语言（DSL，Domain Specific Language)。教程一中有相关示例，这里是其中的代码片段： // define the job and tie it to our HelloJob class JobDetail job = newJob(HelloJob.class) .withIdentity(&quot;myJob&quot;, &quot;group1&quot;) // name &quot;myJob&quot;, group &quot;group1&quot; .build(); // Trigger the job to run now, and then every 40 seconds Trigger trigger = newTrigger() .withIdentity(&quot;myTrigger&quot;, &quot;group1&quot;) .startNow() .withSchedule(simpleSchedule() .withIntervalInSeconds(40) .repeatForever()) .build(); // Tell quartz to schedule the job using our trigger sched.scheduleJob(job, trigger); 定义job的代码使用的是从JobBuilder静态导入的方法。同样，定义trigger的代码使用的是从TriggerBuilder静态导入的方法 - 另外，也导入了SimpleSchedulerBuilder类的静态方法； 从DSL里静态导入的语句如下： import static org.quartz.JobBuilder.*; import static org.quartz.SimpleScheduleBuilder.*; import static org.quartz.CronScheduleBuilder.*; import static org.quartz.CalendarIntervalScheduleBuilder.*; import static org.quartz.TriggerBuilder.*; import static org.quartz.DateBuilder.*; SchedulerBuilder接口的各种实现类，可以定义不同类型的调度计划（schedule）； DateBuilder类包含很多方法，可以很方便地构造表示不同时间点的java.util.Date实例（如定义下一个小时为偶数的时间点，如果当前时间为9:43:27，则定义的时间为10:00:00）。 Job和Trigger一个job就是一个实现了Job接口的类，该接口只有一个方法： Job接口： package org.quartz; public interface Job { public void execute(JobExecutionContext context) throws JobExecutionException; } 当job的一个trigger被触发后（稍后会讲到），execute()方法会被scheduler的一个工作线程调用；传递给execute()方法的JobExecutionContext对象中保存着该job运行时的一些信息 - 执行job的scheduler的引用，触发job的trigger的引用，JobDetail对象引用，以及一些其它信息。 JobDetail对象是在将job加入scheduler时，由客户端程序（你的程序）创建的。它包含job的各种属性设置，以及用于存储job实例状态信息的JobDataMap。本节是对job实例的简单介绍，更多的细节将在下一节讲到。 Trigger用于触发Job的执行。当你准备调度一个job时，你创建一个Trigger的实例，然后设置调度相关的属性。Trigger也有一个相关联的JobDataMap，用于给Job传递一些触发相关的参数。Quartz自带了各种不同类型的Trigger，最常用的主要是SimpleTrigger和CronTrigger。 SimpleTrigger主要用于“一次性”(one-shot)执行的Job（只在某个特定的时间点执行一次），或者Job在特定的时间点执行，重复执行N次，每次执行之间延迟T个时间单位。CronTrigger在基于日历的调度上非常有用，如“每个星期五的正午”，或者“每月的第十天的上午10:15”等。 为什么既有Job，又有Trigger呢？很多任务调度器并不区分Job和Trigger。有些调度器只是简单地通过一个执行时间和一些job标识符来定义一个Job；其它的一些调度器将Quartz的Job和Trigger对象合二为一。在开发Quartz的时候，我们认为将调度和要调度的任务分离是合理的。在我们看来，这可以带来很多好处。 例如，Job被创建后，可以保存在Scheduler中，与Trigger是独立的，同一个Job可以有多个Trigger；这种松耦合的另一个好处是，当与Scheduler中的Job关联的trigger都过期了时，可以配置Job稍后被重新调度，而不用重新定义Job；还有，可以修改或者替换Trigger，而不用重新定义与之关联的Job。 Key将Job和Trigger注册到Scheduler时，可以为它们设置key，配置其身份属性。Job和Trigger的key（JobKey和TriggerKey）可以用于将Job和Trigger放到不同的分组（group）里，然后基于分组进行操作。同一个分组下的Job或Trigger的名称必须唯一，即一个Job或Trigger的key由名称（name）和分组（group）组成。 对于Job和Trigger，你现在有一个大概的了解了，更详细的介绍，见教程三和教程四。 原文链接 本系列教程由quartz-2.2.x官方文档翻译、整理而来，希望给同样对quartz感兴趣的朋友一些参考和帮助，有任何不当或错误之处，欢迎指正；有兴趣研究源码的同学，可以参考我对quartz-core源码的注释(进行中)。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"quartz","slug":"quartz","permalink":"http://nkcoder.github.io/tags/quartz/"}]},{"title":"Quartz教程一--使用Quartz","slug":"quartz-tutorial-01-using-quartz","date":"2014-06-23T13:01:32.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/06/23/quartz-tutorial-01-using-quartz/","link":"","permalink":"http://nkcoder.github.io/2014/06/23/quartz-tutorial-01-using-quartz/","excerpt":"Scheduler在使用之前需要实例化。一般通过SchedulerFactory来创建一个实例。有些用户将factory的实例保存在JNDI中，但直接初始化，然后使用该实例也许更简单（见下面的示例）。 scheduler实例化后，可以启动(start)、暂停(stand-by)、停止(shutdown)。注意：scheduler被停止后，除非重新实例化，否则不能重新启动；只有当scheduler启动后，即使处于暂停状态也不行，trigger才会被触发（job才会被执行）。","text":"Scheduler在使用之前需要实例化。一般通过SchedulerFactory来创建一个实例。有些用户将factory的实例保存在JNDI中，但直接初始化，然后使用该实例也许更简单（见下面的示例）。 scheduler实例化后，可以启动(start)、暂停(stand-by)、停止(shutdown)。注意：scheduler被停止后，除非重新实例化，否则不能重新启动；只有当scheduler启动后，即使处于暂停状态也不行，trigger才会被触发（job才会被执行）。 下面的代码片段，实例化并启动一个scheduler，调度执行一个job： SchedulerFactory schedFact = new org.quartz.impl.StdSchedulerFactory(); Scheduler sched = schedFact.getScheduler(); sched.start(); // define the job and tie it to our HelloJob class JobDetail job = newJob(HelloJob.class) .withIdentity(&quot;myJob&quot;, &quot;group1&quot;) .build(); // Trigger the job to run now, and then every 40 seconds Trigger trigger = newTrigger() .withIdentity(&quot;myTrigger&quot;, &quot;group1&quot;) .startNow() .withSchedule(simpleSchedule() .withIntervalInSeconds(40) .repeatForever()) .build(); // Tell quartz to schedule the job using our trigger sched.scheduleJob(job, trigger); 你看到了，quartz的使用并不难。教程二会简要地介绍job和trigger，以及quartz的API，然后你会更好地理解上面的示例。 原文链接 本系列教程由quartz-2.2.x官方文档翻译、整理而来，希望给同样对quartz感兴趣的朋友一些参考和帮助，有任何不当或错误之处，欢迎指正；有兴趣研究源码的同学，可以参考我对quartz-core源码的注释(进行中)。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"quartz","slug":"quartz","permalink":"http://nkcoder.github.io/tags/quartz/"}]},{"title":"Quartz教程--快速入门","slug":"quartz-tutorial-quickstart","date":"2014-06-22T02:11:23.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/06/22/quartz-tutorial-quickstart/","link":"","permalink":"http://nkcoder.github.io/2014/06/22/quartz-tutorial-quickstart/","excerpt":"欢迎来到quartz快速入门教程。阅读本教程，你将会了解： quartz下载 quartz安装 根据你的需要，配置Quartz 开始一个示例应用","text":"欢迎来到quartz快速入门教程。阅读本教程，你将会了解： quartz下载 quartz安装 根据你的需要，配置Quartz 开始一个示例应用 当熟悉了quratz调度的基本功能后，可以尝试一些更高级的特性，比如Where，这个一个企业级功能，可以让job和trigger运行在指定的，而不是随机的Terracotta客户端上。 下载和安装首先，下载最新的稳定版 - 不用注册。解压并安装。 Quartz jar文件quartz安装包根目录的lib/目录下有很多的jar包。其中，quartz-xxx.jar（其中xxx是版本号）是最主要的。为了使用quartz，必须将该jar包放在应用的classpath下； 下载后，解压，然后将quartz-xxx.jar放到你的应用中。 我主要是在应用服务器的环境中使用quartz，所以一般将quartz jar包放到应用中（.ear或.war）。当然，如果你希望在很多应用中使用quartz，将quartz的jar包放在应用服务器(appserver)的classpath下即可。如果你只是希望在独立的应用中使用quartz，将quartz的jar包和你的应用依赖的其它jar包放在一起即可。 quzrtz依赖一些第三方的库（以jar包的形式），这些库位于quartz安装包的lib目录下。要使用quartz的所有功能，必须将所有的第三方jar包都放到classpath下。如果你开发的是一个独立的quartz应用，建议将所有的jar包都放到classpath下；如果是在应用服务器环境下使用quartz，其中有些包可能已经存在于classpath中了，因此你需要自己选择。 在应用服务器环境下，如果同一个jar文件，存在两个不同的版本，要注意，可能会产生一些奇怪的结果；比如，WebLogic包含了一个J2EE的实现（在weblogic.jar中），该实现与servlet.jar的实现可能不一致。此时，应该从你的应用中排除掉servlet.jar，这样你就知道使用的是哪个类了。 properties文件quartz使用名为quartz.properties的配置文件。刚开始时该配置文件不是必须的，但是为了使用最基本的配置，该文件必须位于classpath下。 基于我的个人情况举个例子，我的应用是基于WebLogic Workshop开发的。我将所有的配置文件（包括quartz.properties）放到应用根目录下的一个项目中。当我将项目打包成.ear文件时，放置配置文件的项目会以jar包的形式进入最终的.ear包，所以quartz.properties文件就自动位于classpath中了。 如果你准备构建一个使用quartz的web应用（以.war包的形式），你应该将quartz.properties文件放到WEB-INF/classes目录下。 配置这里包含很多内容。quartz是一个配置很灵活的应用。配置quartz最好的方式是，编辑quartz.properties文件，然后放到应用的classpath下。 quartz的安装包中包含了一些配置文件的示例，位于example/目录下。我建议你创建自己的quartz.properties文件，而不是简单地从示例中拷贝并删除不需要的部分。这样看起来更整洁，而且你也会了解到quartz的更多功能。 关于quartz配置文件的详细文档，请查阅Quartz配置参考 为了使用quartz，一个基本的quartz.properties配置文件如下所示： org.quartz.scheduler.instanceName = MyScheduler org.quartz.threadPool.threadCount = 3 org.quartz.jobStore.class = org.quartz.simpl.RAMJobStore 上述配置的scheduler有如下特点： org.quartz.scheduler.instanceName - scheduler的名称为“MyScheduler” org.quartz.threadPool.threadCount - 线程池中有3个线程，即最多可以同时执行3个job； org.quartz.jobStore.class - quartz的所有数据，包括job和trigger的配置，都会存储在内存中（而不是数据库里）。如果你想使用quartz的数据库存储功能，我们建议在使用数据库存储之前，先使用内存存储（RamJobStore）。 示例应用下载和安装完quartz后，是时候开发一个示例应用，并让它跑起来了。下面的示例代码，获取scheduler实例对象，启动，然后关闭。 QuartzTest.java import org.quartz.Scheduler; import org.quartz.SchedulerException; import org.quartz.impl.StdSchedulerFactory; import static org.quartz.JobBuilder.*; import static org.quartz.TriggerBuilder.*; import static org.quartz.SimpleScheduleBuilder.*; public class QuartzTest { public static void main(String[] args) { try { // Grab the Scheduler instance from the Factory Scheduler scheduler = StdSchedulerFactory.getDefaultScheduler(); // and start it off scheduler.start(); scheduler.shutdown(); } catch (SchedulerException se) { se.printStackTrace(); } } } 当你调用StdSchedulerFactory.getDefaultScheduler()获取scheduler实例对象后，在调用scheduler.shutdown()之前，scheduler不会终止，因为还有活跃的线程在执行。 注意示例代码中的静态导入(static import)，下面的代码中也会用到它们。 如果你没有配置日志输出，所有的日志会输出到控制台，比如： [INFO] 21 Jan 08:46:27.857 AM main [org.quartz.core.QuartzScheduler] Quartz Scheduler v.2.0.0-SNAPSHOT created. [INFO] 21 Jan 08:46:27.859 AM main [org.quartz.simpl.RAMJobStore] RAMJobStore initialized. [INFO] 21 Jan 08:46:27.865 AM main [org.quartz.core.QuartzScheduler] Scheduler meta-data: Quartz Scheduler (v2.0.0) &apos;Scheduler&apos; with instanceId &apos;NON_CLUSTERED&apos; Scheduler class: &apos;org.quartz.core.QuartzScheduler&apos; - running locally. NOT STARTED. Currently in standby mode. Number of jobs executed: 0 Using thread pool &apos;org.quartz.simpl.SimpleThreadPool&apos; - with 50 threads. Using job-store &apos;org.quartz.simpl.RAMJobStore&apos; - which does not support persistence. and is not clustered. [INFO] 21 Jan 08:46:27.865 AM main [org.quartz.impl.StdSchedulerFactory] Quartz scheduler &apos;Scheduler&apos; initialized from default resource file in Quartz package: &apos;quartz.properties&apos; [INFO] 21 Jan 08:46:27.866 AM main [org.quartz.impl.StdSchedulerFactory] Quartz scheduler version: 2.0.0 [INFO] 21 Jan 08:46:27.866 AM main [org.quartz.core.QuartzScheduler] Scheduler Scheduler_$_NON_CLUSTERED started. [INFO] 21 Jan 08:46:27.866 AM main [org.quartz.core.QuartzScheduler] Scheduler Scheduler_$_NON_CLUSTERED shutting down. [INFO] 21 Jan 08:46:27.866 AM main [org.quartz.core.QuartzScheduler] Scheduler Scheduler_$_NON_CLUSTERED paused. [INFO] 21 Jan 08:46:27.867 AM main [org.quartz.core.QuartzScheduler] Scheduler Scheduler_$_NON_CLUSTERED shutdown complete. 你可以在start()和shutdown()之间做一些有趣的事情： // define the job and tie it to our HelloJob class JobDetail job = newJob(HelloJob.class) .withIdentity(&quot;job1&quot;, &quot;group1&quot;) .build(); // Trigger the job to run now, and then repeat every 40 seconds Trigger trigger = newTrigger() .withIdentity(&quot;trigger1&quot;, &quot;group1&quot;) .startNow() .withSchedule(simpleSchedule() .withIntervalInSeconds(40) .repeatForever()) .build(); // Tell quartz to schedule the job using our trigger scheduler.scheduleJob(job, trigger); 在调用shutdown()之前，你需要给job的触发和执行预留一些时间，比如，你可以调用Thread.sleep(60000)让线程睡眠一段时间。 好了，自己去探索吧！ 原文链接 本系列教程由quartz-2.2.x官方文档翻译、整理而来，希望给同样对quartz感兴趣的朋友一些参考和帮助，有任何不当或错误之处，欢迎指正；有兴趣研究源码的同学，可以参考我对quartz-core源码的注释(进行中)。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"quartz","slug":"quartz","permalink":"http://nkcoder.github.io/tags/quartz/"}]},{"title":"Jetty9部署指南","slug":"jetty-deploy-help","date":"2014-06-17T16:18:15.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/06/18/jetty-deploy-help/","link":"","permalink":"http://nkcoder.github.io/2014/06/18/jetty-deploy-help/","excerpt":"1. 简单有效的方式把要部署的工程（war包、工程目录或者xml描述文件）放到${JETTY_HOME}的webapps目录下即可； 需要注意的是： jetty会对webapps目录下的几乎所有war包、目录、xml文件（有一些例外，如隐藏文件和.d结尾的目录等会被忽略）进行自动部署。如果war包、目录和xml文件同名，则部署的顺序为xml文件 &gt; war包 &gt; 目录 。比如，webapps目录下有：rank.war，rank目录以及rank.xml，其中rank目录为rank.war解压后的目录，rank.xml中引用的是rank.war包或者rank目录，则此时，仅有xml文件被部署，这里成立的前提是同名，如果不同名，但它们是同一个工程，则会导致工程被重复部署，切记！（关于重复部署，参考前一篇博文：Jetty9避免重复部署） 我建议的做法是：将war包或解压后的目录放在webapps目录下，或者将xml描述文件放在webapps目录下，将war包或目录放在单独的目录里。","text":"1. 简单有效的方式把要部署的工程（war包、工程目录或者xml描述文件）放到${JETTY_HOME}的webapps目录下即可； 需要注意的是： jetty会对webapps目录下的几乎所有war包、目录、xml文件（有一些例外，如隐藏文件和.d结尾的目录等会被忽略）进行自动部署。如果war包、目录和xml文件同名，则部署的顺序为xml文件 &gt; war包 &gt; 目录 。比如，webapps目录下有：rank.war，rank目录以及rank.xml，其中rank目录为rank.war解压后的目录，rank.xml中引用的是rank.war包或者rank目录，则此时，仅有xml文件被部署，这里成立的前提是同名，如果不同名，但它们是同一个工程，则会导致工程被重复部署，切记！（关于重复部署，参考前一篇博文：Jetty9避免重复部署） 我建议的做法是：将war包或解压后的目录放在webapps目录下，或者将xml描述文件放在webapps目录下，将war包或目录放在单独的目录里。 2. 配置context path默认，jetty将webapps目录下的工程名作为context path，如果工程名称为ROOT，则context path为/；比如，将rank.war（或rank目录）放在webapps目录下，则context path为/rank，如果将rank.war重命名为ROOT.war，则context path为/； 如果通过文件名来配置context path无法满足要求，则可以通过xml文件来配置，如将rank.xml放在webapps目录下，添加如下内容： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE Configure PUBLIC &quot;-//Jetty//Configure//EN&quot; &quot;http://www.eclipse.org/jetty/configure_9_0.dtd&quot;&gt; &lt;Configure class=&quot;org.eclipse.jetty.webapp.WebAppContext&quot;&gt; &lt;Set name=&quot;contextPath&quot;&gt;/&lt;/Set&gt; &lt;Set name=&quot;war&quot;&gt;/opt/www/ugc-base/webapps/RankByElasticSearch-1.0.war&lt;/Set&gt; &lt;/Configure&gt; contextPath配置context path，war指定工程war包或目录的路径； 3. jetty启动/停止建议通过${JETTY_HOME}下bin/jetty.sh脚本来启动/停止jetty，如： $ bin/jetty start $ bin/jetty stop 当然，也可以通过start.jar来启动，如： $ java -jar start.jar 如果希望通过start.jar停止，则在启动的时候需要指定STOP.PORT和STOP.KEY两个参数，且启动和停止时，两个参数的值必须匹配，如： $ java -jar start.jar STOP.PORT=8181 STOP.KEY=ugcKey $ java -jar start.jar STOP.PORT=8181 STOP.KEY=ugcKey --stop 所以，一般通过bin/jetty.sh控制jetty的运行，使用start.jar查看jetty的配置和状态。 4. 配置jetty环境变量和jvm参数通过bin/jetty.sh来控制jetty的运行，所以编辑bin/jetty.sh文件，可以配置的变量主要有： JAVA: 设置java命令的绝对路径，即jdk的bin目录下的java命令的路径，如果没设置，则从PATH环境变量中查找； JAVA_OPTIONS：设置jvm参数； JETTY_HOME：jetty的安装目录，如果没有设置，则从调用该脚本的上下文环境中猜测； JETTY_BASE：jetty的base目录，即当前工程使用的jetty环境的根目录，如果没有设置，则与JETTY_HOME相同； JETTY_RUN：配置保存jetty pid文件的路径，如果没有配置，根据以下顺序查找第一个可用目录：/var/run, /usr/var/run, JETTY_BASE, /tmp； JETTY_PID：pid文件路径，默认为：$JETTY_RUN/$NAME.pid（NAME变量表示启动jetty时，去掉扩展名的脚本名称）； JETTY_ARGS：jetty参数，如配置端口号等：JETTY_ARGS=8080 jetty.spdy.port=8443 JETTY_USER：配置启动用户，如以nkcoder用户启动：JETTY_USER=nkcoder 注意：以上这些变量，虽然在jetty的运行环境下都具有默认值，但是在设置时，这些参数还是空的，即不能互相引用，比如，没有显式配置JETTY_BASE，直接配置JETTY_RUN=JETTY_BASE，则此时JETTY_RUN使用的还是默认值，因为JETTY_BASE此时为空。 这里提供一个简单的配置供参考： JETTY_HOME=/usr/local/jetty9.1 JETTY_BASE=$JETTY_HOME JETTY_RUN=$JETTY_BASE JETTY_USER=www JETTY_ARGS=jetty.port=8989 JAVA=/usr/local/jdk7/bin/java JAVA_OPTIONS=&quot;-Xloggc:/opt/logs/vrsRank/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=10M -XX:+UseG1GC -XX:+UnlockExperimentalVMOptions -XX:G1MaxNewSizePercent=50 -XX:PermSize=256m -XX:MaxPermSize=256m -Xss256k -server -Xms4G -Xmx4G -Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.port=18787 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -XX:+UnlockCommercialFeatures -XX:+FlightRecorder -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/opt/logs/vrsRank/oom.log&quot; 5. 查看jetty配置通过start.jar查看帮助和配置： $ java -jar start.jar --help 主要的查看配置的参数有： --list-config: 查看启动jetty使用的配置：java环境，jetty环境，JVM参数，属性，服务器classpath，服务器的xml配置等； --list-modules: 查看系统使用的模块 --list-classpath: 查看系统使用的classpath --version：查看版本信息 --module=&lt;model-name&gt;：临时启用一个模块 6. 一台机器上同时部署多个jetty以在一台服务器上同时部署两个工程为例，需要两份jetty和一份jdk。和单独部署的唯一区别就是，只要确保pid和port是不同的即可。 第一种方式：修改jetty.sh脚本的名称，因为pid文件的名称就是脚本的名称，如： 工程1使用jetty1，将bin/jetty.sh重命名为bin/jetty1.sh，同时修改其配置如下（注意不用配置JETTY_RUN变量）： JETTY_HOME=/usr/local/jetty1 JETTY_BASE=$JETTY_HOME JETTY_USER=www JETTY_ARGS=jetty.port=8181 JAVA=/usr/local/jdk7/bin/java JAVA_OPTIONS=&quot;-Xloggc:/opt/logs/ugcRank/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps&quot; 工程2使用jetty2，将bin/jetty.sh重命名为bin/jetty2.sh，并修改配置： JETTY_HOME=/usr/local/jetty2 JETTY_BASE=$JETTY_HOME JETTY_USER=www JETTY_ARGS=jetty.port=8282 JAVA=/usr/local/jdk7/bin/java JAVA_OPTIONS=&quot;-Xloggc:/opt/logs/vrsRank/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps&quot; 此时，两个jetty的pid均位于默认的目录下，即/var/run，路径分别为/var/run/jetty1.pid，/var/run/jetty2.pid。 第二种方式：修改pid文件保存的目录，该目录由JETTY_RUN配置，默认都在/var/run下，如： 工程1使用jetty1，修改bin/jetty.sh如下： JETTY_HOME=/usr/local/jetty1 JETTY_BASE=$JETTY_HOME JETTY_RUN=$JETTY_BASE JETTY_USER=www JETTY_ARGS=jetty.port=8181 JAVA=/usr/local/jdk7/bin/java JAVA_OPTIONS=&quot;-Xloggc:/opt/logs/ugcRank/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps&quot; 工程2使用jetty2，修改bin/jetty.sh如下： JETTY_HOME=/usr/local/jetty1 JETTY_BASE=$JETTY_HOME JETTY_RUN=$JETTY_BASE JETTY_USER=www JETTY_ARGS=jetty.port=8181 JAVA=/usr/local/jdk7/bin/java JAVA_OPTIONS=&quot;-Xloggc:/opt/logs/ugcRank/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps&quot; 此时，pid文件位于各自jetty的安装目录下，虽然都为jetty.pid，但是互不影响。 7. 将jetty配置为系统服务首先，将bin/jetty.sh拷贝到/etc/init.d中： $ cp bin/jetty.sh /etc/init.d/jetty 然后，新建文件/etc/default/jetty，在其中设置环境变量JETTY_HOME： $ vim /etc/default/jetty JETTY_HOME=/usr/local/jetty9.1 启动和停止： $ service jetty start $ service jetty stop 说明：bin/jetty.sh默认将/etc/default/{pid}作为其配置文件，此时pid名称即为jetty，所以/etc/default/jetty会作为jetty的配置文件，可以在其中配置JETTY_HOME, JAVA, JAVA_OPTIONS等环境变量。 参考： Jetty : The Definitive Reference","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"jetty","slug":"jetty","permalink":"http://nkcoder.github.io/tags/jetty/"}]},{"title":"算法练习--扑克牌构成顺子","slug":"algorithm-straight-poker","date":"2014-06-04T15:20:29.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/06/04/algorithm-straight-poker/","link":"","permalink":"http://nkcoder.github.io/2014/06/04/algorithm-straight-poker/","excerpt":"1. 问题描述： 从扑克牌里抽出任意5张牌，其中J, Q, K分表表示11, 12, 13，A表示1，大小王可以表示任意点数。求该5张牌是否可以构成顺子。","text":"1. 问题描述： 从扑克牌里抽出任意5张牌，其中J, Q, K分表表示11, 12, 13，A表示1，大小王可以表示任意点数。求该5张牌是否可以构成顺子。 2. 思路： 思路：思路一：首先求出抽出的5张牌中，大小王的数量，然后将其它的牌按点数排序，计算它们之间的空隙，如果空隙数小于王牌的数量，则表示可以构成顺子；否则，不能构成顺子。复杂度O(n)。思路二：特殊牌做特殊处理，将大小王的点数当做0，求出最大点数和最小点数（非0），如果最大点数和最小点数之间的差值小于5（抽出的牌的总数量），则表示可以构成顺子，否则不能；注意：为了方便排重，因为牌的数量小，可以先排序，然后排重，最后比较。复杂度O(n)。 3. Java参考代码/** * 检查抽出的n张牌是否可以构成顺子 * @param cards 表示抽出的牌的数组 * @param length 抽出的牌的数量 * @return 是否可以构成顺子 */ public boolean checkStraightPoker(int[] cards, int length) { // 桶排序 int[] allCards = new int[ALL_CARDS_SIZE]; for (int i = 0; i &lt; length; i++) { allCards[cards[i]]++; } // 牌的排重 for (int i = 1; i &lt; ALL_CARDS_SIZE; i++) { if (allCards[i] &gt; 1) { logger.error(&quot;identical cards exists.&quot;); return false; } } // 查找最小索引（非0） int firstIndex = 1; while (0 == allCards[firstIndex]) { firstIndex ++; } // 查找最大索引（非0） int lastIndex = ALL_CARDS_SIZE - 1; while (0 == allCards[lastIndex]) { lastIndex --; } // 构成顺子的条件 if ((lastIndex - firstIndex + 1) &lt;= length) { return true; } return false; }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"算法练习--合并两个有序单链表","slug":"algorithm-merge-two-sorted-list","date":"2014-05-25T02:42:57.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/25/algorithm-merge-two-sorted-list/","link":"","permalink":"http://nkcoder.github.io/2014/05/25/algorithm-merge-two-sorted-list/","excerpt":"1. 问题描述： 给定两个单链表，都是递增有序的，将它们合并，使合并后的链表仍然有序。","text":"1. 问题描述： 给定两个单链表，都是递增有序的，将它们合并，使合并后的链表仍然有序。 2. 思路： 思路：先比较两个单链表的第一个节点，值小的的节点作为合并后新链表的第一个节点；值小的节点的下一个节点作为该单链表的新的头节点。相当于第一个节点已排好序，剩下的问题仍然是对两个有序链表进行合并，则可以用递归的思路解决。 3. Java参考代码/** * 合并两个有序单链表 * @param firstHead 第一个单链表的头节点 * @param secondHead 第二个单链表的头节点 * @return 合并后的链表的头节点 */ public static ListNode merge(ListNode firstHead, ListNode secondHead) { // 退出条件 if (firstHead == null) { return secondHead; } if (secondHead == null) { return firstHead; } // 比较两个单链表的第一个节点，值小的节点作为合并后的链表的节点 ListNode currentHead = null; if (firstHead.value &lt; secondHead.value) { currentHead = firstHead; currentHead.next = merge(firstHead.next, secondHead); } else { currentHead = secondHead; currentHead.next = merge(firstHead, secondHead.next); } return currentHead; }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"Jetty9避免重复部署","slug":"jetty-avoid-repeat-deploy","date":"2014-05-22T14:34:28.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/22/jetty-avoid-repeat-deploy/","link":"","permalink":"http://nkcoder.github.io/2014/05/22/jetty-avoid-repeat-deploy/","excerpt":"jetty是一个轻量级的web容器，部署简单，同时高度可定制化。由于刚接触，对其约定和配置不太熟练，在自动部署时，跳过一次重复部署的坑，这里简要说明一下，希望给同样打算在项目中使用jetty9的朋友一个参考。 默认情况下，jetty会对根目录(也可以配置jetty.base)下webapps/目录下的内容实现自动部署，部署的规则如下：","text":"jetty是一个轻量级的web容器，部署简单，同时高度可定制化。由于刚接触，对其约定和配置不太熟练，在自动部署时，跳过一次重复部署的坑，这里简要说明一下，希望给同样打算在项目中使用jetty9的朋友一个参考。 默认情况下，jetty会对根目录(也可以配置jetty.base)下webapps/目录下的内容实现自动部署，部署的规则如下： 隐藏文件(.开头)和.d结尾的目录被忽略； CVS目录如”CVS”和”CVSROOT”被忽略； 任何war包都会被自动部署； 任何xml描述文件被认为是可部署的； 任何目录都被认为是可部署的； 同名的war包和目录同时存在，目录不被部署，仅war包部署，且认为war包引用该目录； 同名的war包和xml文件同时存在，war包不被部署，仅xml文件描述符被部署，且认为该xml文件引用该war包； 同名的目录和xml文件同时存在，目录不被部署，xml文件被部署，且认为xml文件引用该目录； 关于更详细的说明，请参考官方文档的这里。我主要提醒的是：在webapps目录中，如果存在同名的目录、war包和xml文件，它们会被当做同一个工程，部署的优先级是xml文件&gt;war包&gt;目录。一定要注意同名，如果不同名，在webapps下存在一个war包，同时存在一个引用该war包的xml文件，则会导致重复部署，这就是我跳的坑。 部署时，推荐的做法是，将xml描述文件放到自动部署的webapps目录下，里面定义war包的路径、上下文路径、是否解压、临时目录、日志文件等，然后将war包放在自定义的固定目录下，项目更新，只需要备份和替换war包，重启jetty即可。 参考： Deployment Architecture","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"jetty","slug":"jetty","permalink":"http://nkcoder.github.io/tags/jetty/"}]},{"title":"算法练习--环中最后的数字","slug":"algorithm-last-num-in-circle","date":"2014-05-21T16:00:43.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/22/algorithm-last-num-in-circle/","link":"","permalink":"http://nkcoder.github.io/2014/05/22/algorithm-last-num-in-circle/","excerpt":"1. 问题描述： 有n个数，从0到n-1，形成环状，即n-1后的数字为0；从0开始，每次从环中删除第m个数，然后将删除元素的下一个元素作为第一个元素。如此循环，求最后剩下的数。","text":"1. 问题描述： 有n个数，从0到n-1，形成环状，即n-1后的数字为0；从0开始，每次从环中删除第m个数，然后将删除元素的下一个元素作为第一个元素。如此循环，求最后剩下的数。 2. 思路： 思路： 思路一：构造环形链表，每个节点有两个属性，一个是值，另一个是指向下一个元素的指针。每次从环中删除第m个元素，直到链表中剩下一个元素。复杂度O(m*n)； 思路二：也是用链表模拟环，使用标准库的LinkedList，每次遍历到最后一个元素后，继续遍历链表第一个元素； 思路三：寻找规律：f(n,m)表示从n个数(0,1,2,…,n-1)的环中删除第m个数后剩下的数；第一次删掉了的元素为k（显然，k=m-1），剩下的元素为(k+1,k+2,…,n-1, 0, 1, …,k-1)，此时的序列有n-1个元素，但排列不同，从该序列每次删除第m个元素后最后剩下的元素记为f’(n-1,m)，则有f(n,m)=f&#39;(n-1,m)，但是我们将f&#39;(n-1,m)与f(n-1,m)（序列为0,1,2,…,n-2)做一下映射，找规律： f&apos;(n-1,m) f(n-1,m) k+1 0 k+2 1 ... ... k-2 n-3 k-1 n-2 发现的映射规律为：[f(n-1,m) + (k+1)] % n = f&#39;(n-1,m)将该公式以及(k=m-1)与f(n,m)=f’(n-1,m)组合，有：f(n,m) = [f(n-1)+m]%n，即如果要求f(n,m)，可以先求f(n-1)，得到的递归公式为： f(n,m) = [f(n-1,m)+m]%n, (n=1时 f(n,m)=0) 利用该公式，使用递归或者循环很容易解决。复杂度O(n) 3. Java参考代码/** * 自定义链表节点，形成环状，每次从环中删除第m个节点，直到环中只剩下一个节点 * @param head 头节点 * @param m 要删除的第m个节点 * @return 最后剩下的节点 */ public ListNode ListNodeMethod(ListNode head, int m) { if (null == head || m &lt;= 0) { return null; } // 寻找链表最后一个节点 ListNode loopNode = head; while (null != loopNode.next) { loopNode = loopNode.next; } // 如果每次都是删除第一个节点，则返回的应该是最后一个节点 if (m == 1) { return head; } // 否则形成环状 loopNode.next = head; // 循环删除，直到最后只剩下一个节点 while (head.next != head) { // 先找到第m-1个节点 ListNode preNode = head; for (int i = 0; i &lt; m - 2; i++) { preNode = preNode.next; } // 删除第m个节点 preNode.next = preNode.next.next; // 修改起点 head = preNode.next; } return head; } /** * 使用标准库的LinkedList模拟环形链表，遍历到最后一个元素后，返回到第一个元素，继续 * @param numberList 包含n个数的链表 * @param m 要删除的第m个数 * @return 最后剩下的元素 */ public int LinkedListMethod(LinkedList&lt;Integer&gt; numberList, int m) { if (null == numberList || 0 == numberList.size()) { return -1; } int n = numberList.size(); int start = 0; while (n &gt; 1) { // 要删除的元素 int delIndex = (start + m - 1) % n; numberList.remove(delIndex); n = numberList.size(); // 删除元素后，该索引对应的即为下一个元素 start = delIndex; if (start &gt;= n) { start = 0; } } return numberList.getLast(); } /** * 使用循环实现由规律得出的公式： * f(n,m) = f&apos;(n-1,m) = (f(n-1,m)+m) % n; * 因为数组的下标索引从0到n-1，所以返回的值即为最后元素在数组中的索引 * @param m 要删除的第m个元素 * @param n 数组的大小 * @return 数组中最后一个元素的索引 */ public int loopMethod(int m, int n) { // 只有一个元素：f(1,m) = 0; int last = 0; // 有n个元素：f(n,m) = (f(n-1,m) + m) % n; for (int i = 2; i &lt;= n; i++) { last = (last + m) % i; } return last; } /** * 使用递归实现由规律得到的公式： * f(n,m) = (f(n-1,m) + m) % n * 因为数组的下标索引从0到n-1，所以返回的值即为最后元素在数组中的索引 * @param m 要删除的第m个元素 * @param n 数组中元素的个数 * @return 最后一个元素在数组里的索引 */ public int recursiveMethod(int n, int m) { if (1 == n) { return 0; } return (recursiveMethod(n-1, m) + m) % n; }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"array","slug":"array","permalink":"http://nkcoder.github.io/tags/array/"}]},{"title":"算法练习--返回链表中倒数第K个节点","slug":"algorithm-find-kth-node","date":"2014-05-18T15:51:04.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/18/algorithm-find-kth-node/","link":"","permalink":"http://nkcoder.github.io/2014/05/18/algorithm-find-kth-node/","excerpt":"1. 问题描述： 返回链表中倒数第k个节点。","text":"1. 问题描述： 返回链表中倒数第k个节点。 2. 思路： 思路：使用两个指针，第一个指针先走(k-1)步，第二个指针不动，此时两个指针相差k个节点；然后两个指针同时向前走，直到第一个指针到达链表尾节点，此时第二个指针即为链表的倒数第k个节点。注意：需要考虑到链表不够k个节点的情况。 3. Java参考代码/** * 查找链表中倒数第k个节点 * @param head 链表的头节点 * @param k 倒数第k个节点 * @return */ public static ListNode find(ListNode head, int k) { if (head == null || k &lt;= 0) { return null; } // 第一个节点先向前走k-1步 ListNode firstNode = head; int i = 0; while ((i &lt; k -1) &amp;&amp; (firstNode.next != null)) { firstNode = firstNode.next; i++; } // 链表的长度小于k if (i != k - 1) { return null; } // 两个节点同时走，直到第一个节点到达尾节点 ListNode secondNode = head; while (firstNode.next != null) { firstNode = firstNode.next; secondNode = secondNode.next; } return secondNode; }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"算法练习--根据对角线打印矩阵","slug":"algorithm-print-matrix-diagonally","date":"2014-05-16T15:41:54.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/16/algorithm-print-matrix-diagonally/","link":"","permalink":"http://nkcoder.github.io/2014/05/16/algorithm-print-matrix-diagonally/","excerpt":"1. 问题描述： 输入一个矩阵，从右上角开始按照斜对角线打印矩阵的值，如矩阵为： 1, 2, 3, 4 5, 6, 7, 8 9, 10, 11, 12 13,14, 15, 16 输出： 4, 3, 8, 2, 7, 12, 1, 6, 11, 16, 5, 10, 15, 9, 14, 13","text":"1. 问题描述： 输入一个矩阵，从右上角开始按照斜对角线打印矩阵的值，如矩阵为： 1, 2, 3, 4 5, 6, 7, 8 9, 10, 11, 12 13,14, 15, 16 输出： 4, 3, 8, 2, 7, 12, 1, 6, 11, 16, 5, 10, 15, 9, 14, 13 2. 思路： 思路：将整个输出以最长的斜对角线分为两部分：右上部分和左下部分。右上部分：对角线的起点在第一行，列数递减，对角线上相邻元素之间横坐标和纵坐标均相差1；左下部分：对角线的起点在第一列上，行数递减，对角线上相邻元素之间横坐标和纵坐标均相差1；复杂度：O(n^2) 3. Java参考代码 /** * 以对角线的方式打印n*n矩阵 * @param data 矩阵数组 * @param n 矩阵的维度 */ public void print(int[][] data, int n) { // 打印右上部分 for (int i = n - 1; i &gt;= 0; i--) { int row = 0; int col = i; while ((row &gt;= 0 &amp;&amp; row &lt; n) &amp;&amp; (col &gt;= 0 &amp;&amp; col &lt; n)) { System.out.println(data[row][col]); row++; col++; } } // 打印左下部分 for (int i = 1; i &lt; n; i++) { int row = i; int col = 0; while ((row &gt;= 0 &amp;&amp; row &lt; n) &amp;&amp; (col &gt;= 0 &amp;&amp; col &lt; n)) { System.out.println(data[row][col]); row++; col++; } } }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"算法练习--从链表中删除一个节点","slug":"algorithm-del-one-node-from-linklist","date":"2014-05-16T14:31:03.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/16/algorithm-del-one-node-from-linklist/","link":"","permalink":"http://nkcoder.github.io/2014/05/16/algorithm-del-one-node-from-linklist/","excerpt":"1. 问题描述： 给定一个链表和其中一个节点，删除该节点；","text":"1. 问题描述： 给定一个链表和其中一个节点，删除该节点； 2. 思路： 思路：节点的差异体现在节点对象的内容不同。要删除当前节点，可以将当前节点与下一节点的值互换，然后删除下一个节点即可。需要注意的是，如果要删除的节点是最后一个节点，没有下一个节点，此时需要从头遍历了。如果要删除的节点不是最后一个节点，复杂度为O(1)，否则复杂度为O(n)，平均复杂度为O(1)。 3. Java参考代码/** * 从链表中删除某一个节点 * @param head 链表的头节点 * @param toDelete 要删除的节点 * @return 头节点 */ public static ListNode delete(ListNode head, ListNode toDelete) { // param error if (head == null || toDelete == null) { return head; } // 最后一个节点 if (toDelete.next == null) { // 也是头节点 if (head == toDelete) { return null; } // 遍历查找前一个节点 ListNode node = head; while (node.next != toDelete) { node = node.next; } node.next = node.next.next; return head; } // 不是最后一个节点，将下一个节点的值覆盖当前节点的值，删除下一个节点 toDelete.value = toDelete.next.value; toDelete.next = toDelete.next.next; return head; }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"算法练习--二维数组中查找元素","slug":"algorithm-search-sort-two-dim-array","date":"2014-05-14T13:57:12.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/14/algorithm-search-sort-two-dim-array/","link":"","permalink":"http://nkcoder.github.io/2014/05/14/algorithm-search-sort-two-dim-array/","excerpt":"1. 问题描述： 给定一个二维数组：每一行的元素从左到右递增，每一列的元素从上到下递增；输入一个整数，求该整数是否在该二位数组中。比如，输入的二位数组为： 2 5 7 9 10 3 8 9 11 13 5 9 12 15 20 9 11 32 40 50 如果输入的整数为12，则输出true，输入23，则输出false；","text":"1. 问题描述： 给定一个二维数组：每一行的元素从左到右递增，每一列的元素从上到下递增；输入一个整数，求该整数是否在该二位数组中。比如，输入的二位数组为： 2 5 7 9 10 3 8 9 11 13 5 9 12 15 20 9 11 32 40 50 如果输入的整数为12，则输出true，输入23，则输出false； 2. 思路： 因为数组的元素从左向右递增，从上到下递增，则以左下角的元素为例（右上角的元素亦可），如果被查找的整数比该元素大，由于该元素所在列的所有元素都比它小，因此该列可以去掉；如果被查找的整数比该元素小，则该元素所在的行可以去掉，这样每次比较都可以去掉一行或者一列。复杂度为O(m+n)（m、n分别为数组的行数和列数），该方法比直接遍历数组（复杂度O(mn)）要好。 3. Java参考代码/** * 在二维数组中查找元素 * @param array 二维数组 * @param row 数组的行 * @param col 数组的列 * @param key 待查找的元素 * @return 存在则返回true，否则返回false */ private static boolean searchInTwoDimensionArray(int[][] array, int row, int col, int key) { int i = row - 1, j = 0; while (i &gt;= 0 &amp;&amp; j &lt;= col - 1) { if (array[i][j] == key) { return true; } else if (array[i][j] &lt; key) { j++; } else { i--; } } return false; }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"算法练习--打印从1到最大的n位数","slug":"algorithm-print-num-to-max","date":"2014-05-13T16:19:34.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/14/algorithm-print-num-to-max/","link":"","permalink":"http://nkcoder.github.io/2014/05/14/algorithm-print-num-to-max/","excerpt":"1. 问题描述： 输入整数n，打印从1到最大的n位数，比如n为4，输出1，2，3一直到最大的4位数，即9999；","text":"1. 问题描述： 输入整数n，打印从1到最大的n位数，比如n为4，输出1，2，3一直到最大的4位数，即9999； 2. 思路： n值可能较大，所以此题为大数问题。大数问题一般可通过字符串或者数组来模拟解决。 思路一：通过使用字符数组来模拟整数的加1操作。即每次在当前字符串表示的整数上加1，从最低位到最高位，记录进位。复杂度为O(10^n)； 思路二：使用整数数组通过递归实现，即打印i位表示的整数，假设i-1表示的整数可列举，则只需将第i位从0到9遍历，同时与i-1位表示的整数序列组合即可。复杂度为O(10^n)；注意，递归可能会导致栈溢出。 3. Java参考代码 方法一：字符串模拟加1 /** * 【方法一】：用字符串（字符数组）模拟整数的加1操作：在字符数组表示的整数上加一，同时记录进位，进位从低位向高位传递， * 该操作在两种情况下退出：1. 在某一位上没有发生进位；2. 在最高位（字符数组第一位）发生了进位； * @param data 字符数组，表示当前整数 * @param length 字符串数组的长度 * @return 是否发生了溢出 */ public static boolean addByOne(char[] data, int length) { boolean isOverflow = false; // 溢出标志 int carry = 0; // 进位标志 // 从低位向高位遍历 for (int i = length - 1; i &gt;= 0; i--) { int sum = data[i] - &apos;0&apos; + carry; // 如果是最后一位，则加1 if (i == length - 1) { sum++; } // 是否发生进位 if (sum &gt;= 10) { if (i == 0) { // 如果最高位发生了进位，表示溢出 isOverflow = true; break; } // 发生了进位，需要向更高一位加1 data[i] = &apos;0&apos;; carry = 1; } else { // 没有发生进位，加1操作结束 data[i] = (char)(sum + &apos;0&apos;); break; } } return isOverflow; } /** * 【方法一】：将字符数组表示的整数打印出来，去掉前缀0 * @param data 字符数组表示的整数值 * @param length 字符数组的长度 */ public static void printNumber(char[] data, int length) { boolean zeroPrefix = true; for (int i = 0; i &lt; length; i++) { if (zeroPrefix) { if (data[i] != &apos;0&apos;) { zeroPrefix = false; System.out.print(data[i]); } } else { System.out.print(data[i]); } } System.out.println(); } /** * 【方法一】：循环模拟加1操作，直到发生溢出，则遍历了所有的整数 * @param data 字符数组，各位初始化为&apos;0&apos; * @param length 数组的长度 */ public static void stringMethod(char[] data, int length) { for (int i = 0; i &lt; length; i++) { data[i] = &apos;0&apos;; } while (!addByOne(data, length)) { printNumber(data, length); } } 方法二：通过整数数组递归实现 /** * 【方法二】：使用整数数组递归实现，数组索引较小表示整数的高位，比如要求第i位开始的所有整数，假设i-1位开始的所有 * 整数都已经排列好，则只需将第i位从0到9遍历一遍即可。 * @param data 整数数组 * @param length 数组的长度 * @param index 当前索引 */ public static void recursionMethod(int[] data, int length, int index) { // 如果遍历过了最后一位，则遍历结束，打印当前数组表示的整数 if (index == length) { printNumber(data, length); return; } // 每一位都从0到9递归遍历 for (int i = 0; i &lt;= 9; i++) { data[index] = i; recursionMethod(data, length, index + 1); } } /** * 【方法二】：打印数组表示的整数值，去掉前缀0 * @param data 数组表示的整数值 * @param length 数组的长度 */ public static void printNumber(int[] data, int length) { boolean zeroPrefix = true; for (int i = 0; i &lt; length; i++) { if (zeroPrefix == true) { if (data[i] != 0) { zeroPrefix = false; System.out.print(data[i]); } } else { System.out.print(data[i]); } } System.out.println(); }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"算法练习--顺时针打印矩阵","slug":"algorithm-print-matrix-clockwise","date":"2014-05-12T14:41:10.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/12/algorithm-print-matrix-clockwise/","link":"","permalink":"http://nkcoder.github.io/2014/05/12/algorithm-print-matrix-clockwise/","excerpt":"1. 问题描述： 输入一个矩阵（二维数组），按照顺时针的方向打印矩阵中的元素；比如，输入： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 输出： 1 2 3 4 5 10 15 20 19 18 17 16 11 6 7 8 9 14 13 12","text":"1. 问题描述： 输入一个矩阵（二维数组），按照顺时针的方向打印矩阵中的元素；比如，输入： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 输出： 1 2 3 4 5 10 15 20 19 18 17 16 11 6 7 8 9 14 13 12 2. 思路： 可以将顺时针打印分成四个步骤，从左到右的一行，从上到下的一列，从右到左的一行，从下到上的一列。打印时有两个条件限制：一是不能出界，二是不能重复打印。第一个条件，可以通过矩阵的行列值进行限制，第二个条件，可以设置一个与矩阵同样结构的标志数组，用于标记矩阵的每一个元素是否被遍历过。 这里有一个常用的技巧，即试探法，先试探下一个元素是否可遍历，如果是，则遍历，否则终止本轮遍历。如果不是使用试探法，直接通过边界值的限制，循环遍历，当越界时，需要回退。 时间复杂度O(mn)，空间复杂度O(mn)，其中m、n分别表示矩阵的行数和列数。 3. Java参考代码/** * 顺时针打印矩阵，分为四步：左-&gt;右，上-&gt;下，右-&gt;左，下-&gt;上，每一步 * 使用试探法，通过矩阵的行数、列数作为限制，遍历矩阵。 * @param matrix 矩阵 * @param rows 矩阵的行数 * @param cols 矩阵的列数 */ public static void print(int[][] matrix, int rows, int cols) { // 标志数组初始化 boolean[][] visited = new boolean[rows][cols]; for (int i = 0; i &lt; rows; i ++) { for (int j = 0; j &lt; cols; j++) { visited[i][j] = false; } } int i = 0; int j = -1; int k = 0; // 遍历矩阵 while (k++ &lt; rows * cols) { // 左到右的一行 while (j + 1 &lt; cols &amp;&amp; !visited[i][j+1]) { logger.info(&quot;{} &quot;, matrix[i][++j]); visited[i][j] = true; } // 上到下的一列 while (i + 1 &lt; rows &amp;&amp; !visited[i+1][j]) { logger.info(&quot;{} &quot;, matrix[++i][j]); visited[i][j] = true; } // 右到左的一行 while (j - 1 &gt;= 0 &amp;&amp; !visited[i][j-1]) { logger.info(&quot;{} &quot;, matrix[i][--j]); visited[i][j] = true; } // 下到上的一列 while (i - 1 &gt;= 0 &amp;&amp; !visited[i-1][j]) { logger.info(&quot;{} &quot;, matrix[--i][j]); visited[i][j] = true; } } }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"算法练习--整数二进制中1的个数","slug":"algorithm-num-of-one-bit","date":"2014-05-11T15:42:05.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/11/algorithm-num-of-one-bit/","link":"","permalink":"http://nkcoder.github.io/2014/05/11/algorithm-num-of-one-bit/","excerpt":"1. 问题描述： 给定一个整数num，求num的二进制表示中1的个数；输入：整数num；输出：num二进制表示中1的个数","text":"1. 问题描述： 给定一个整数num，求num的二进制表示中1的个数；输入：整数num；输出：num二进制表示中1的个数 2. 思路： 可以判断整数的二进制表示中每一位是否为1，即每次将整数与1进行与操作，然后整数右移一位，如果整数是正数，没问题，移位直到整数值为0，移位的次数为整数二进制的有效位数；如果整数是负数，直接循环移位，会陷入无限循环（负数最高位为1，右移时高位补符号位），此时需要完整移动32位（整数由4个字节构成）。 也是判断整数的每一位是否为1，但不是对整数移位，而是每次对1左移位，判断整数对应的位上是否为1；移位的次数为整数二进制的有效位数。 n&amp;(n-1)：每次消掉n最低位上的1，直到结果为0，比较的次数为整数二进制中1的个数。此方位最优。 3. Java参考代码/** * 通过n&amp;(n-1)，每次可以消掉n的二进制表示中最低位上的1，比较 * 的次数为n二进制中1的个数。 * @param num * @return */ private int getOneBitNum(int num) { int count = 0; while (0 != num) { num = (num &amp; (num - 1)); count++; } return count; }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"算法练习--重排数组，使奇数位于偶数之前","slug":"algorithm-odd-before-even","date":"2014-05-09T15:40:24.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/09/algorithm-odd-before-even/","link":"","permalink":"http://nkcoder.github.io/2014/05/09/algorithm-odd-before-even/","excerpt":"1. 问题描述 给定数组，调整数组中元素的顺序，使得所有的奇数都位于数组的前部分，所有的偶数都位于数组的后部分。输入：数组输出：重排后的数组","text":"1. 问题描述 给定数组，调整数组中元素的顺序，使得所有的奇数都位于数组的前部分，所有的偶数都位于数组的后部分。输入：数组输出：重排后的数组 2. 思路 使用两个指针，第一个指针指向数组首元素，第二个指针指向数组尾元素，在两个指针相遇前，第一个指针向后移动，直到遇到偶数，第二个指针向前移动，直到遇到奇数，然后两个指针指向的元素交换；重复上述步骤，直到两个指针相遇为止；复杂度O(n)。 这里，奇偶重排只是同类型问题中的一种，因此可以奇偶判断提取成一个方法，降低耦合度，可以提高代码的重用性。 3. java参考代码/** * 调整数组中元素的顺序，使所有的奇数位于数组的前部分，偶数位于数组的后部分； * 定义首尾指针，首指针前进，尾指针后退，当首指针遇到偶数且尾指针遇到奇数时，元素互换。 * @param data 输出数组 * @param length 数组的长度 */ public static int[] adjust(int[] data, int length) { int first = 0; int last = length - 1; while (first &lt; last) { // 首指针寻找第一个偶数 while ((first &lt; last) &amp;&amp; (isOdd(data[first]))) { first++; } // 尾指针寻找第一个奇数 while ((first &lt; last) &amp;&amp; (!isOdd(data[last]))) { last--; } // 首尾元素互换 if (first &lt; last) { int tmp = data[first]; data[first++] = data[last]; data[last++] = tmp; } } return data; } /** * 判断一个数是否为奇数 * @param num * @return */ public static boolean isOdd(int num) { if ((num &amp; 0x01) == 1) { return true; } return false; }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"算法练习--求和为S的连续正整数序列","slug":"algorithm-find-continue-sum","date":"2014-05-08T14:38:26.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/08/algorithm-find-continue-sum/","link":"","permalink":"http://nkcoder.github.io/2014/05/08/algorithm-find-continue-sum/","excerpt":"1. 问题描述 给定一个正整数s，求和为s的所有连续正整数序列（至少包含两个数），如s为15时，和为15的连续正整数序列有：1+2+3+4+5，4+5+6, 7+8。输入：正整数s；输出：和为s的所有连续正整数序列","text":"1. 问题描述 给定一个正整数s，求和为s的所有连续正整数序列（至少包含两个数），如s为15时，和为15的连续正整数序列有：1+2+3+4+5，4+5+6, 7+8。输入：正整数s；输出：和为s的所有连续正整数序列 2. 思路 要求连续序列之和，可以从最短最小的子序列入手(即1和2)，求其和，如果和小于s，序列向元素大的方向扩展，如果大于s，从序列中剔除最小的元素，不断扩展，直到序列终止；那么，序列何时终止呢？因为序列至少两个数，两个数的和不能大于s，所以序列的最大边界值即为s/2。 同类问题： 给定递增数组data和数值s，在数组中寻找两个数，使其和为s，如果有多对符合条件的数对，都打印出来。 思路：设置两个元素指针，一个指向数组的首元素，一个指向数组尾元素，求其和并与s比较，如果大于s，尾元素指针前移，否则，首元素指针后移，直到两个指针元素相遇； 3. java参考代码/** * 给定值sum，求所有和为sum的连续整数序列 * @param sum */ public void findContinuousSum(int sum) { int start = 1; int end = 2; int boundary = (sum + 1) &gt;&gt; 1; // 序列右边界 while ((start &lt; end) &amp;&amp; (end &lt;= boundary)) { int currentSum = 0; for (int i = start; i &lt;= end; i++) { currentSum += i; } if (currentSum == sum) { logger.info(&quot;find a sequence: [{}...{}]&quot;, start, end); end++; } else if (currentSum &lt; sum) { end++; } else { start++; } } }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"算法练习--求旋转数组的最小值","slug":"algorithm-rotated-array-min","date":"2014-05-08T13:13:56.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/08/algorithm-rotated-array-min/","link":"","permalink":"http://nkcoder.github.io/2014/05/08/algorithm-rotated-array-min/","excerpt":"1. 问题描述 有一个递增排序的数组，将数组旋转（将前若干个元素移动数组的末尾）后，求旋转后数组的最小值；比如：有序数组为：2, 5, 8, 12, 21, 34, 50，旋转后为: 12,, 21, 34, 50, 2, 5, 8，求数组的最小值。输入：旋转后的的数组；输出：数组的最小值；","text":"1. 问题描述 有一个递增排序的数组，将数组旋转（将前若干个元素移动数组的末尾）后，求旋转后数组的最小值；比如：有序数组为：2, 5, 8, 12, 21, 34, 50，旋转后为: 12,, 21, 34, 50, 2, 5, 8，求数组的最小值。输入：旋转后的的数组；输出：数组的最小值； 2. 思路 2.1 遍历整个数组寻找最小值，复杂度为O(n)； 2.2 旋转后，数组部分有序，可以利用二分查找的思路，降低复杂度： 首先求中间元素，如果比首元素大，说明最小元素在后半区间；如果比尾元素小，说明最小元素在前半区间；调整二分查找的区间。 二分的过程中，有一个特殊情况：即中间元素既大于等于首元素，又小于等于尾元素，亦即中间元素等于首元素，且等于尾元素，此时无法判断最小值位于哪个半区间，此时只能顺序查找。比如有两个旋转数组：[5, 5, 5, 5, 2, 2, 5]和[5, 2, 2, 5, 5, 5, 5]都可以看成数组[2, 2, 5, 5, 5, 5, 5]旋转得到的。 还有一种特殊情况，即数组完全有序，亦即0个元素被旋转，此时不能直接二分，需要对这种情况做出处理； 3. java参考代码/** * 在部分有序的数组里查找最小值 * @param data 数组 * @param start 数组首元素下标 * @param end 数组尾元素下标 * @return 最小值 */ public int binarySearchMin(int[] data, int start, int end) { int mid = start; while (data[start] &gt;= data[end]) { // 仅有两个元素时,第二个元素为最小值 if (start + 1 == end) { return data[end]; } mid = (start + end) &gt;&gt; 1; // 顺序查找 if (data[mid] == data[start] &amp;&amp; data[mid] == data[end]) { return seqSearchMin(data, start, end); } // 最小元素位于右半区间 if (data[mid] &gt;= data[start]) { start = mid; } else if (data[mid] &lt;= data[start]) { // 最小元素位于左半区间 end = mid; } } return data[mid]; } /** * 遍历数组，寻找最小值 * @param data 数组 * @param start 数组首元素索引 * @param end 数组尾元素索引 * @return 最小值 */ public int seqSearchMin(int data[], int start, int end) { int min = data[start]; for (int i = start;i &lt;= end; i++) { if (data[i] &lt; min) { min = data[i]; } } return min; }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"算法练习--求Fibonacci序列和","slug":"algorithm-fibonacci","date":"2014-05-07T14:20:55.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/07/algorithm-fibonacci/","link":"","permalink":"http://nkcoder.github.io/2014/05/07/algorithm-fibonacci/","excerpt":"1. 问题描述 求Fibonacci序列之和：f(n) = f(n-1) + f(n-2); 【n&gt;0, f(1)=f(2)=1】","text":"1. 问题描述 求Fibonacci序列之和：f(n) = f(n-1) + f(n-2); 【n&gt;0, f(1)=f(2)=1】 2. 思路 2.1 根据公式递归求解，会有大量重复计算，效率低；复杂度（n的指数级） 2.2 保存中间结果，迭代累加： f(3) = f(1) + f(2); f(1)和f(2)是已知的； f(4) = f(2) + f(3); f(2)和f(3)是已知的，可以将f(2)看作原来的f(1)，将f(3)看作原来的f(2)； … 逐步累加即可，时间复杂度O(n); 3. 同类题目 3.1 青蛙跳台阶：一只青蛙一次可以跳一级台阶，也可以跳2级台阶，请问跳n级台阶，一共有多少中跳法？ 思路： n级是f(n)，第一次跳一级，后面n-1级，f(n-1)；第一次跳二级，后面n-2级，f(n-2)，即满足：f(n) = f(n-1) + f(n-2) 【f(1) = 1, f(2) = 1, n &gt; 0】 3.2 矩形覆盖：我们可以使用2x1的矩形去横着或者竖着去覆盖更大的矩形。请问，用8个2x1的矩形无重叠地覆盖一个2x8的大矩形，总共有多少种方法？ 思路： 覆盖2x8的矩形的方法记为f(8)，如果横着覆盖，剩下的为f(6)，如果竖着覆盖，剩下的为f(7)，所以一共为：f(8) = f(7) + f(6) 4. java参考代码/** * 迭代求和的思路 * @param n * @return */ private long iterativeMethod(int n) { if (n &lt; 0) { return 0; } long f1 = 1; long f2 = 1; long f = 0; for (int i = 3; i &lt;= n; i++) { f = f1 + f2; f1 = f2; f2 = f; } return f; }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"算法练习--实现库函数Pow(double Base, Int Exp)","slug":"algorithm-calculate-pow","date":"2014-05-06T15:31:35.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/06/algorithm-calculate-pow/","link":"","permalink":"http://nkcoder.github.io/2014/05/06/algorithm-calculate-pow/","excerpt":"1. 问题描述： 实现函数pow(double base, int exp)，即base的exp次方，不考虑大数问题，不能使用任何库函数。","text":"1. 问题描述： 实现函数pow(double base, int exp)，即base的exp次方，不考虑大数问题，不能使用任何库函数。 2. 思路： 2.1 首先考虑正常情况，计算b^e，最简单的方法是循环累乘，优点是简单直观，缺点是循环次数较多, 复杂度为(n)； 2.2 为了减少循环的次数，可以采取“折半平方”的思路， 如x^6=(x^3)^2，x^7=(x^3)^2*x，可以使用递归实现，优点是减少幂乘的次数，缺点是如果递归层次太深，可能发生栈溢出；复杂度为O(logn)。（递归的本质也是栈，我们可以自己用栈来实现） 异常处理：当指数e为负数的时候，需要判断底数b的值是否为0，而b是double类型，因此涉及到浮点数与0的比较问题； 3. java代码：private final double THRESHOLD = 1E-6; /** * 计算浮点数base的exponent次幂，不考虑大数（溢出）问题 * @param base * @param exponent * @return -1表示异常，否则，返回值为结果值 */ public double getPow(double base, int exponent) { if (isEqual(base, 0.0)) { if (exponent &lt; 0) { logger.info(&quot;error input&quot;); return -1; } return 1.0; } // 计算exponent为正值时的结果 int absExponent = exponent; if (exponent &lt; 0) { absExponent = -exponent; } double result = positivePow(base, absExponent); // 如果exponent为负值，取其倒数 if (exponent &lt; 0) { result = 1.0 / result; } return result; } /** * 通过平方的思路计算幂值（指数exponent为正值）： * 如果指数为偶数, b^e = b^(e/2) * b^(e/2) * 如果指数为奇数, b^e = b^(e/2) * b^(e/2) * b * 这比通过循环求幂值的效率要好一些，可以通过递归来实现。 * @param base * @param exponent * @return */ private double positivePow(double base, int exponent) { if (exponent == 1) { return base; } double result = positivePow(base, exponent &gt;&gt; 1); result *= result; // 先计算平方 if ((exponent &amp; 0x1) == 1) { // 如果指数exponent为奇数，平方外应该再乘以base result *= base; } return result; } /** * 比较两个浮点数是否相等 * @param d1 * @param d2 * @return 如果相等，返回true，否则返回false； */ private boolean isEqual(double d1, double d2) { if ((d1 - d2) &lt; THRESHOLD &amp;&amp; (d1 - d2) &gt; -THRESHOLD) { return true; } return false; }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"Xshell鼠标选中，终端立即中断(CTRL-C)的问题","slug":"xshell-select-interrupt-dict","date":"2014-05-05T15:24:38.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/05/xshell-select-interrupt-dict/","link":"","permalink":"http://nkcoder.github.io/2014/05/05/xshell-select-interrupt-dict/","excerpt":"Xshell选中文字复制时中断 在Xshell中设置了“自动将选中的文字复制到粘贴板”，之前一直没有问题，最近发现，只要选中屏幕上的文字，复制上了，但shell终端立即被中断了，即向终端发送了CTRL-C；","text":"Xshell选中文字复制时中断 在Xshell中设置了“自动将选中的文字复制到粘贴板”，之前一直没有问题，最近发现，只要选中屏幕上的文字，复制上了，但shell终端立即被中断了，即向终端发送了CTRL-C； 原因： 很可能是“选中文字复制”与词典软件的“划词翻译”相冲突。我的原因是，最近安装了必应词典，且开启了“划词翻译”功能。我在网上求助时，发现也有一些朋友遇到和我一样的问题，都是由于与词典的冲突导致的，如有道词典，关掉划译功能即可。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"xshell","slug":"xshell","permalink":"http://nkcoder.github.io/tags/xshell/"}]},{"title":"算法练习--不使用加减乘除对两个整数求和","slug":"algorithm-practice-addbybit","date":"2014-05-04T10:15:57.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/05/04/algorithm-practice-addbybit/","link":"","permalink":"http://nkcoder.github.io/2014/05/04/algorithm-practice-addbybit/","excerpt":"1. 问题描述 不使用加法（+）、减法（-）、乘法（*）和除法（/）操作，求两个整数的和。","text":"1. 问题描述 不使用加法（+）、减法（-）、乘法（*）和除法（/）操作，求两个整数的和。 2. 思路 可以从位运算上考虑，将两个整数以二进制表示，两个数的加法，即对应位上的加法，如果两个数的某一位不同（一个为0，一个为1），则相加后该位为1；如果相同，分两种情况，同为0，则相加后还是0，如果同为1，相加后结果仍为0，但需要向更高位进位； 于是我们发现，对位进行相加时，如果两个位相同，结果为0，不同，结果为1，这即位的异或运算；进位仅发生在两个位同为1的情况下，即位的与运算，与运算的结果，位为1的表示该位上发生了进位，进位实质上就是在更高位上加1，即左移一位，于是，最终的结果为异或的结果与进位的结果之和，问题又回到了求两个数之和，重复上述操作，直到进位为0为止；复杂度为O(n)。例如，求01100101和00101100的和：[xor: 01001001, and: 00100100, &lt;&lt;: 01001000]-&gt;[xor: 00000001, and: 01001000, &lt;&lt;: 10010000]-&gt;[xor: 10010001, and: 00000000, &lt;&lt;: 00000000]-&gt;[result: 10010001] 3. Java代码/** * 输入两个整数，求它们的和（不能使用加减乘除四种运算） * @param firstNum * @param secondNum * @return */ public int add(int firstNum, int secondNum) { int bitXor = 0; int bitAnd = 0; while (0 != secondNum) { bitXor = firstNum ^ secondNum; // 异或，提取出不同位 bitAnd = firstNum &amp; secondNum; // 与，提取进位 firstNum = bitXor; secondNum = bitAnd &lt;&lt; 1; } return firstNum; }","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nkcoder.github.io/categories/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://nkcoder.github.io/tags/algorithm/"}]},{"title":"将Jetty配置为Linux服务","slug":"jetty-as-linux-service","date":"2014-04-11T00:04:08.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/04/11/jetty-as-linux-service/","link":"","permalink":"http://nkcoder.github.io/2014/04/11/jetty-as-linux-service/","excerpt":"最近将项目都升级到了Jetty 9 和JDK 7，记录一下配置的过程。以下配置基于的版本为： Jetty：9.1.1.v20140108 JDK：1.7.0_51 官方文档版本：2014-03-17T10:14:41-05:00","text":"最近将项目都升级到了Jetty 9 和JDK 7，记录一下配置的过程。以下配置基于的版本为： Jetty：9.1.1.v20140108 JDK：1.7.0_51 官方文档版本：2014-03-17T10:14:41-05:00 1. 作为root用户配置如果你希望以root用户启动jetty。并将jetty配置为服务，以下为快速通道： 1.1 将脚本jetty.sh拷贝init.d目录下# cp bin/jetty.sh /etc/init.d/jetty jetty.sh是jetty的启动脚本。 1.2 配置/etc/default/jetty# vim /etc/default/jetty --------------------------------------------------- PATH=$PATH:/usr/local/jdk1.7.0_51/bin JETTY_HOME=/usr/local/jetty9.1 TMPDIR=/usr/local/jetty9.1/tmp --------------------------------------------------- jetty.sh脚本会将/etc/default/下同名的文件作为其配置文件。需要配置几个环境变量：主要是java和jetty的根目录，以及指定一个目录作为jetty解压war包时的临时目录，默认是/tmp（不够安全，因为系统服务可能会不定时删除/tmp目录）。 1.3 启动与关闭# service jetty start # service jetty stop init.d目录下的jetty就是服务名.参考：Startup a Unix Service using jetty.sh 2. 作为普通用户配置如果你希望以普通用户运行jetty，并且可以灵活扩展到多工程的部署，同时jetty升级方便，可以参考一下步骤。 2.1 配置工程目录新建工程目录vrs-base [root@localhost www]# mkdir vrs-base [root@localhost www]# cd vrs-base/ [root@localhost www]# mkdir tmp [root@localhost vrs-base]# pwd /opt/www/vrs-base 配置jetty环境 [root@localhost vrs-base]# cp $JETTY_HOME/start.ini /opt/www/vrs-base [root@localhost vrs-base]# java -jar $JETTY_HOME/start.jar --add-to-start=http,deploy,logging 修改端口 [root@localhost vrs-base]# grep port start.ini jetty.port=8080 2.2 配置服务脚本和属性[root@localhost www]# cp $JETTY_HOME/bin/jetty.sh /etc/init.d/jetty-vrs [root@localhost www]# vim /etc/default/jetty-vrs --------------------------------------------------- PATH=$PATH:/usr/local/jdk1.7.0_51/bin JETTY_HOME=/usr/local/jetty9.1 JETTY_BASE=/opt/www/vrs-base TMPDIR=/opt/www/vrs-base/tmp --------------------------------------------------- JETTY_HOME环境变量表示jetty的安装根目录，JETTY_BASE环境变量表示当前应用的根目录，TEPDIR表示应用被jetty解压后的临时目录，不建议使用默认的/tmp目录。 2.3 应用部署[root@localhost vrs-base]# cp /opt/www/vrs.xml /opt/www/vrs-base/webapps/ [root@localhost vrs-base]# cp /opt/www/vrs.war /opt/www/vrs-base/webapps/ 将应用的xml配置文件和war包放到$JETTY_BASE/webapps目录中，jetty对该目录下的内容进行自动部署和扫描。 2.4 新建用户/修改权限# useradd -U www # chown -R www $JETTY_HOME # chown -R www /opt/www # chown -R www /opt/logs 注意：所有jetty会访问的目录都需要修改权限，如：排行榜系统在/opt/rank目录下生成文件，则该目录也需要修改为www用户权限。 2.5 启动与停止[root@localhost vrs-base]# su - www service jetty-vrs start Starting Jetty: 2014-04-04 10:17:58.452:INFO::main: Redirecting stderr/stdout to /opt/www/vrs-base/logs/2014_04_04.stderrout.log . . . . OK Fri Apr 4 10:18:17 CST 2014 [root@localhost vrs-base]# ps -ef | grep jetty www 2821 1 65 10:19 pts/1 00:00:15 /usr/local/jdk1.7.0_51/bin/java -Djetty.state=/opt/www/vrs-base/jetty-vrs.state -Djetty.logs=/opt/www/vrs-base/logs -Djetty.home=/usr/local/jetty9.1 -Djetty.base=/opt/www/vrs-base -Djava.io.tmpdir=/opt/www/vrs-base/tmp -jar /usr/local/jetty9.1/start.jar jetty-logging.xml jetty-started.xml root 2887 1929 0 10:19 pts/1 00:00:00 grep jetty [root@localhost vrs-base]# su - www service jetty-vrs stop Stopping Jetty: OK 2.6 多应用部署可以使用同一个jetty，启动多个实例，每个实例部署一个应用，并作为服务运行。重复2.1~2.5的步骤，只需要再定义一个JETTY_BASE，在/etc/init.d目录下添加一个对应的服务名即可。即：将2.1中的vrs-base修改为ugc-base，将2.2中的jetty-vrs修改为jetty-ugc，启动和停止： $ su - www service jetty-ugc start $ su - www service jetty-ugc stop 附录xml配置文件示例(feedback.xml): &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE Configure PUBLIC &quot;-//Jetty//Configure//EN&quot; &quot;http://www.eclipse.org/jetty/configure_9_0.dtd&quot;&gt; &lt;Configure class=&quot;org.eclipse.jetty.webapp.WebAppContext&quot;&gt; &lt;Set name=&quot;contextPath&quot;&gt;/&lt;/Set&gt; &lt;Set name=&quot;war&quot;&gt;/opt/www/feedback-base/webapps/feedback.war&lt;/Set&gt; &lt;Set name=&quot;tempDirectory&quot;&gt;/opt/www/feedback-base/tmp&lt;/Set&gt; &lt;Set name=&quot;persistTempDirectory&quot;&gt;true&lt;/Set&gt; &lt;Set name=&quot;handler&quot;&gt; &lt;New id=&quot;RequestLog&quot; class=&quot;org.eclipse.jetty.server.handler.RequestLogHandler&quot;&gt; &lt;Set name=&quot;requestLog&quot;&gt; &lt;New id=&quot;RequestLogImpl&quot; class=&quot;org.eclipse.jetty.server.NCSARequestLog&quot;&gt; &lt;Set name=&quot;filename&quot;&gt;&lt;Property name=&quot;jetty.logs&quot; default=&quot;./logs&quot;/&gt;/feedback-yyyy_mm_dd.request.log&lt;/Set&gt; &lt;Set name=&quot;filenameDateFormat&quot;&gt;yyyy_MM_dd&lt;/Set&gt; &lt;Set name=&quot;LogTimeZone&quot;&gt;Asia/Shanghai&lt;/Set&gt; &lt;Set name=&quot;retainDays&quot;&gt;60&lt;/Set&gt; &lt;Set name=&quot;append&quot;&gt;true&lt;/Set&gt; &lt;/New&gt; &lt;/Set&gt; &lt;/New&gt; &lt;/Set&gt; &lt;/Configure&gt; /etc/default目录下的配置文件示例(jetty-feedback)： JAVA=/usr/local/jdk7/bin/java JETTY_HOME=/usr/local/jetty9 JETTY_BASE=/opt/www/feedback-base TMPDIR=/opt/www/feedback-base/tmp 启动/停止脚本示例(jettyctl.sh) # usage usage() { echo &quot;Usage: ${0##*/} {start|stop|status|restart}&quot; exit 1 } # need one param if [ $# -lt 1 ]; then usage fi action=$1 # control jetty case &quot;$action&quot; in start) su - www service jetty-vrs start ;; stop) su - www service jetty-vrs stop ;; status) su - www service jetty-vrs status ;; restart) su - www service jetty-vrs restart ;; *) usage ;; esac 参考 Startup a Unix Service using jetty.sh","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"jetty","slug":"jetty","permalink":"http://nkcoder.github.io/tags/jetty/"}]},{"title":"Spring持久化之Hibernate","slug":"spring-hibernate-example","date":"2014-03-30T10:37:27.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/03/30/spring-hibernate-example/","link":"","permalink":"http://nkcoder.github.io/2014/03/30/spring-hibernate-example/","excerpt":"本文简要介绍Spring通过Hibernate实现持久化需要的基本配置，分别通过注解和xml配置来实现。 1. 通过注解实现1.1 添加pom依赖在pom的xml里添加Hibernate相关的依赖，如：","text":"本文简要介绍Spring通过Hibernate实现持久化需要的基本配置，分别通过注解和xml配置来实现。 1. 通过注解实现1.1 添加pom依赖在pom的xml里添加Hibernate相关的依赖，如： &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-core&lt;/artifactId&gt; &lt;version&gt;3.6.10.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.persistence&lt;/groupId&gt; &lt;artifactId&gt;persistence-api&lt;/artifactId&gt; &lt;version&gt;${javax.persist}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javassist&lt;/groupId&gt; &lt;artifactId&gt;javassist&lt;/artifactId&gt; &lt;version&gt;3.12.1.GA&lt;/version&gt; &lt;/dependency&gt; 1.2 实体类的注解定义先定义一个实体类StudentEntity.java，映射到数据库中的表students： @Entity @Table(name = &quot;students&quot;) public class StudentEntity { @Id @Column(name = &quot;id&quot;, nullable = false) @GeneratedValue private int id; @Column(name = &quot;name&quot;, nullable = false) private String name; @Column(name = &quot;sex&quot;, nullable = true) private String sex; public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getSex() { return sex; } public void setSex(String sex) { this.sex = sex; } public StudentEntity() {} public StudentEntity(String name, String sex) { this.name = name; this.sex = sex; } } @Entity表示这是一个实体类，@Table(name = “students”)表示映射到数据源指定的数据库中的一个名为students的表；@Id表示该字段是主键，@Column(name = “id”, nullable = false)表示该属性映射到表中名为id的属性，且不能为空；@GeneratedValue表示该字段是自增的；这样定义之后，该实体类便与表students完成了一一对应。 1.3 定义dao实现类我们的dao实现类为HibernateDaoImpl.java，依赖一个属性，即SessionFactory的实例，我们通过注解实现注入： @Repository (value = &quot;hibernateDaoImpl&quot;) public class HibernateDaoImpl implements StudentDao { @Resource(name = &quot;sessionFactory&quot;) SessionFactory sessionFactory; @Override @Transactional public StudentEntity getUserNameById(int id) { StudentEntity studentEntity = (StudentEntity) sessionFactory.getCurrentSession().get(StudentEntity.class, id); return studentEntity; } @Override @Transactional public void saveStudents(StudentEntity studentEntity) { sessionFactory.getCurrentSession().persist(studentEntity); } } Repository注解表示该类会被Spring的注解扫描器自动定义为bean，bean的名字为hibernateDaoImpl，同时实现异常转换，将Hibernate的检查型异常，转换为Spring的非检查型异常，这是通过在Spring配置文件里配置PersistenceExceptionTranslationPostProcessor的ban实现的。通过sessionFactory的getCurrentSession方法获取当前的session，实现对数据库的持久化操作。Hibernate还要求这些操作是与事务绑定的，因此需要在dao层或者service层添加事务的注解，即@Transactional。 1.4 添加Spring配置在Spring配置文件applicationContext.xml主要是添加SessionFactory的配置，以及配置事务，与Hibernate相关的配置如下： &lt;bean name=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;feedbackDataSource&quot;/&gt; &lt;property name=&quot;packagesToScan&quot; value=&quot;com.sohu.tv.bean&quot;/&gt; &lt;property name=&quot;hibernateProperties&quot;&gt; &lt;props&gt; &lt;prop key=&quot;hibernate.dialect&quot;&gt;org.hibernate.dialect.MySQL5Dialect&lt;/prop&gt; &lt;prop key=&quot;hibernate.show_sql&quot;&gt;true&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean class=&quot;org.springframework.dao.annotation.PersistenceExceptionTranslationPostProcessor&quot;/&gt; &lt;bean id=&quot;tx&quot; class=&quot;org.springframework.orm.hibernate3.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot;/&gt; &lt;/bean&gt; &lt;tx:annotation-driven transaction-manager=&quot;tx&quot;/&gt; 在定义sessionFactory的bean时，因为是基于注解的，所以引用的class为：AnnotationSessionFactoryBean；属性dataSource即配置的数据源；属性packagesToScan即要扫描的包名，在这个包下的所有类，如果使用了@Entity、@Column等注解，都会被扫描器处理；@hibernateProperties属性定义了Hibernate相关的属性，如使用的dialect，是否显示输出sql语句等。PersistenceExceptionTranslationPostProcessor定义的bean是进行异常转换的；HibernateTransactionManager用于定义事务；类似于，扫描事务类型的注解。 2. 通过xml映射文件实现通过xml映射文件与通过注解实现的区别就在于实体类与数据库之间的映射的定义，所以其它相同的部分就不重复了。 2.1 定义映射文件这个映射文件定义实体类与数据库的表之间的映射，即属性与字段的映射，如hibernate.xml文件内容可以如下定义： &lt;?xml version=&quot;1.0&quot;?&gt; &lt;!DOCTYPE hibernate-mapping PUBLIC &quot;-//Hibernate/Hibernate Mapping DTD 3.0//EN&quot; &quot;http://hibernate.sourceforge.net/hibernate-mapping-3.0.dtd&quot;&gt; &lt;hibernate-mapping&gt; &lt;class name=&quot;com.sohu.tv.bean.Students&quot; table=&quot;students&quot;&gt; &lt;id name=&quot;id&quot; type=&quot;int&quot;&gt; &lt;column name=&quot;id&quot; /&gt; &lt;generator class=&quot;identity&quot; /&gt; &lt;/id&gt; &lt;property name=&quot;name&quot; type=&quot;string&quot;&gt; &lt;column name=&quot;name&quot; length=&quot;255&quot; not-null=&quot;true&quot; /&gt; &lt;/property&gt; &lt;property name=&quot;sex&quot; type=&quot;string&quot;&gt; &lt;column name=&quot;sex&quot; not-null=&quot;false&quot; /&gt; &lt;/property&gt; &lt;/class&gt; &lt;/hibernate-mapping&gt; 2.2 修改Spring配置文件需要修改的是sessionFactory的bean的定义，如： &lt;bean name=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3.LocalSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;feedbackDataSource&quot;/&gt; &lt;property name=&quot;mappingLocations&quot;&gt; &lt;list&gt; &lt;value&gt;classpath*:hibernate.xml&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;hibernateProperties&quot;&gt; &lt;props&gt; &lt;prop key=&quot;hibernate.dialect&quot;&gt;org.hibernate.dialect.MySQL5Dialect&lt;/prop&gt; &lt;prop key=&quot;hibernate.show_sql&quot;&gt;true&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; 将AnnotationSessionFactoryBean修改为LocalSessionFactoryBean，同时，需要将属性packagesToScan修改为mappingLocations，即xml映射文件的位置。 参考： Spring 实战 Struts + Spring + Hibernate Integration Example","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://nkcoder.github.io/tags/spring/"},{"name":"hibernate","slug":"hibernate","permalink":"http://nkcoder.github.io/tags/hibernate/"}]},{"title":"Elasticsearch查询api--Match Query","slug":"elasticsearch-match-query","date":"2014-03-23T14:55:03.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/03/23/elasticsearch-match-query/","link":"","permalink":"http://nkcoder.github.io/2014/03/23/elasticsearch-match-query/","excerpt":"1. match query 与term query不同，match query的查询词是被分词处理的（analyzed），即首先分词，然后构造相应的查询，所以应该确保查询的分词器和索引的分词器是一致的； 与terms query相似，提供的查询词之间默认是or的关系，可以通过operator属性指定； match query有两种形式，一种是简单形式，一种是bool形式；","text":"1. match query 与term query不同，match query的查询词是被分词处理的（analyzed），即首先分词，然后构造相应的查询，所以应该确保查询的分词器和索引的分词器是一致的； 与terms query相似，提供的查询词之间默认是or的关系，可以通过operator属性指定； match query有两种形式，一种是简单形式，一种是bool形式； 2. REST API# curl -XGET &apos;localhost:9200/video/video_info/_search?pretty&apos; -d @match_query.json 简单形式的match query： { &quot;query&quot;: { &quot;match&quot;: { &quot;tvName&quot;: &quot;决战&quot; } } } bool形式的match query： { &quot;query&quot;: { &quot;match&quot;: { &quot;tvName&quot;: { &quot;query&quot;: &quot;决战华岩寺&quot;, &quot;operator&quot;: &quot;or&quot;, &quot;minimum_should_match&quot;: &quot;2&quot; } } } } 在REST API中，tvName表示要查询的目标字段，也可以是_all。bool形式的match query支持的属性有： operator: 指定构造查询时的布尔操作，可以是and、or，默认是or； analyzer：指定查询分词器，默认为默认的分词器； minimum_should_match: 最小匹配个数 3. Java API以下为match query的Java API示例： public static void matchQuery() { String queryWord = &quot;天津电视台&quot;; QueryBuilder matchQuery = QueryBuilders.matchQuery(ConstantUtil.FIELD_TV_NAME, queryWord) .analyzer(&quot;ik&quot;).operator(MatchQueryBuilder.Operator.OR).minimumShouldMatch(&quot;1&quot;); SearchResponse response = searchRequestBuilder.setQuery(matchQuery) .setFrom(0).setSize(5).execute().actionGet(); printResult(response); } 4. 看看Java源码public static MatchQueryBuilder matchQuery(String name, Object text) { return new MatchQueryBuilder(name, text).type(MatchQueryBuilder.Type.BOOLEAN); } match query对应的是matchQuery()函数，内部先调用MatchQueryBuilder的构造函数，然后将类型设置为Boolean，即先分词，然后构造布尔查询； 5. 参考： match query","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://nkcoder.github.io/tags/elasticsearch/"}]},{"title":"Spring持久化之MyBatis","slug":"spring-mybatis","date":"2014-03-22T12:12:52.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/03/22/spring-mybatis/","link":"","permalink":"http://nkcoder.github.io/2014/03/22/spring-mybatis/","excerpt":"MyBatis是一个优秀的轻量级持久化框架，本文主要介绍MyBatis与Spring集成的配置与用法。 1. Spring MyBatis配置1.1 添加Maven依赖在pom.xml文件里添加mybatis-spring和mybatis的依赖：","text":"MyBatis是一个优秀的轻量级持久化框架，本文主要介绍MyBatis与Spring集成的配置与用法。 1. Spring MyBatis配置1.1 添加Maven依赖在pom.xml文件里添加mybatis-spring和mybatis的依赖： &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;${mybatis.spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;${mybatis.version}&lt;/version&gt; &lt;/dependency&gt; mybatis-spring当前最新版本为1.2.2，mybatis当前版本是3.2.5. 1.2 添加dao接口这里的dao必须是接口，而不是具体的实现，如MyBatisDao.java内容为： public interface MyBatisTest { public String getUserNameById(int id); public List&lt;Students&gt; getStudentByNumAndCity(Map&lt;String, Object&gt; queryMap); } 接口中定义的每一个方法对应于mapper映射文件中定义的jdbc执行模块，如&lt;select/&gt;、&lt;update/&gt;、&lt;insert/&gt;等。 1.3 添加mybatis配置文件该配置文件里主要配置类型别名&lt;typeAliases/&gt;、设置&lt;settings/&gt;，mapper映射文件路径&lt;mappers/&gt;也可以放在这里，但更建议将所有的mapper文件都放在一个目录下，在定义sqlSessionFactory时通过属性mapperLocations指定。如mybatis.xml配置文件可以如下定义： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt; &lt;configuration&gt; &lt;typeAliases&gt; &lt;typeAlias type=&quot;com.sohu.tv.bean.Students&quot; alias=&quot;Students&quot;/&gt; &lt;/typeAliases&gt; &lt;!--&lt;mappers&gt;--&gt; &lt;!--&lt;mapper resource=&quot;com/sohu/tv/mapper/MybatisTest.xml&quot;/&gt;--&gt; &lt;!--&lt;/mappers&gt;--&gt; &lt;/configuration&gt; 类型别名是用别名来代表全限定类名，如在需要用到com.sohu.tv.bean.Students的地方，都可以使用Students来代替。 1.4 添加mapper映射文件：mapper映射文件可以定义数据库列与POJO类属性的映射，以及与dao接口类中的方法对应的JDBC执行模块，如MyBatisMapper.xml的内容为： &lt;mapper namespace=&quot;com.sohu.tv.dao.MyBatisTest&quot;&gt; &lt;resultMap id=&quot;studentMap&quot; type=&quot;Students&quot;&gt; &lt;result column=&quot;name&quot; property=&quot;name&quot;/&gt; &lt;result column=&quot;sex&quot; property=&quot;sex&quot;/&gt; &lt;result column=&quot;number&quot; property=&quot;number&quot;/&gt; &lt;result column=&quot;enable&quot; property=&quot;enable&quot;/&gt; &lt;result column=&quot;city&quot; property=&quot;city&quot;/&gt; &lt;/resultMap&gt; &lt;select id=&quot;getUserNameById&quot; parameterType=&quot;int&quot; resultType=&quot;String&quot;&gt; select name from students where id = #{id} &lt;/select&gt; &lt;select id=&quot;getStudentByNumAndCity&quot; parameterType=&quot;map&quot; resultMap=&quot;studentMap&quot;&gt; select * from students where number = #{num} and city = #{city} &lt;/select&gt; &lt;/mapper&gt; &lt;resultMap/&gt;即定义列与属性的字段映射；&lt;select/&gt;中的参数和返回值的类型，既可以为基本类型，如string，int，long，也可以是map，返回类型还可以是&lt;resultMap/&gt;定义的映射map；如果参数类型是map，则sql中的参数名（如#{num})必须是map的key；如果返回类型为map，则sql语句中返回的列名为key；如果是基本类型，使用type，如parameterType，resultType，如果是自定义map，使用parameterMap，resultMap. 1.5 Spring配置文件的配置首先需要配置sqlSessionFactory： &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;feedbackDataSource&quot;/&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatis.xml&quot;/&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:com/sohu/tv/mapper/*.xml&quot;/&gt; &lt;/bean&gt; 属性dataSource引用JDBC数据源；属性configLocation指定mybatis配置文件的位置，配置文件中定义别名&lt;typeAliases/&gt;，设置&lt;settings/&gt;等。mapperLocations指定mapper映射文件的路径。有一点需要注意的是，要确保mapper映射文件被打包进classpath中，默认情况下，maven会忽略源文件中的资源文件，可以通过在pom文件中配置，使得资源文件被一起打包进classpath中；如在pom配置文件中添加： &lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;excludes&gt;&lt;exclude&gt;**/*.java&lt;/exclude&gt;&lt;/excludes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; 其次，需要定义与dao接口相关联的mapperFactoryBean： &lt;bean id=&quot;mybatisDaoImpl&quot; class=&quot;org.mybatis.spring.mapper.MapperFactoryBean&quot;&gt; &lt;property name=&quot;mapperInterface&quot; value=&quot;com.sohu.tv.dao.MyBatisTest&quot;/&gt; &lt;property name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactory&quot;/&gt; &lt;/bean&gt; mapperInterface属性的值为相关的dao接口，sqlSessionFactory属性引用了上述定义的sqlSessionFacotry； 1.6 service类中调用dao类实现业务逻辑在MyBatisServiceImpl.java中使用dao接口中提供的方法： @Resource(name = &quot;mybatisDaoImpl&quot;) MyBatisDao myBatisDaoImpl; String userName = mybatisDaoImpl.getUserNameById(2); System.out.println(userName); Map&lt;String, Object&gt; queryMap = new HashMap&lt;String, Object&gt;(); queryMap.put(&quot;num&quot;, 333); queryMap.put(&quot;city&quot;, &quot;beijing&quot;); List&lt;Students&gt; studentsList = mybatisDaoImpl.getStudentByNumAndCity(queryMap); for (Students students: studentsList) { System.out.println(students.getName()); } 2. 启动自动扫描注解我们可以在applicationContext.xml配置文件里为每个dao接口定义bean，但mybatis还提供了一种更简便的自动扫描注解的机制，即&lt;mybatis:scan/&gt;和&lt;MapperScannerConfigurer/&gt;。配置&lt;mybatis:scan/&gt;，需要在applicationContext.xml配置文件里添加： &lt;mybatis:scan base-package=&quot;com.sohu.tv.dao&quot;/&gt; &lt;mybatis:scan/&gt;与Spring的&lt;context:component-scan/&gt;非常相似，base-package指定要扫描的包，并将包下的所有接口注册为对应的bean。命名规则：和Spring一样，如果该接口没有被注解，则bean的名称为首字母小写的非限定类名，如接口为com.sohu.tv.dao.MyBatisDao，则bean的名字为myBatisDao；如果dao接口使用了Spring的注解，如@Component或@Named等注解，并提供了bean的名称，则mybatis使用该注解的名称作为bean的名称。如将MyBatisDao接口重定义如下： @Repository(value = &quot;mybatisDao&quot;) public interface MyBatisDao { public String getUserNameById(int id); public List&lt;Students&gt; getStudentByNumAndCity(Map&lt;String, Object&gt; queryMap); } 测试MyBatisDao被自动注解后的bean的名称为mybatisDao。建议通过注解指定bean的名称，防止类类名的变化导致了bean名称的变化； 配置&lt;MapperScannerConfigurer/&gt;，需要在applicationContext.xml中添加： &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;property name=&quot;basePackage&quot; value=&quot;com.sohu.tv.dao&quot;/&gt; &lt;property name=&quot;sqlSessionFactoryBeanName&quot; value = &quot;sqlSessionFactory&quot;/&gt; &lt;/bean&gt; 这里的basePackage与&lt;mybatis:scan/&gt;的base-package的含义一致，bean的命名规则也是一样的，所以这两种方式等价。 如果启动了自动扫描注解，则在spring配置文件中不再需要dao接口的bean定义了。 3. 总结-最佳实践 mapper映射文件放在单独的目录中，统一管理，在配置sqlSessionFactory时，通过属性mapperLocations指定； mybatis配置文件中只定义typeAliases、settings等配置信息； Spring配置文件中，通过&lt;mybatis:scan/&gt;或者&lt;MapperScannerConfigurer/&gt;启动自动注解，并通过Spring的注解对bean命名。 参考资料： MyBatis-Spring","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://nkcoder.github.io/tags/spring/"},{"name":"mybatis","slug":"mybatis","permalink":"http://nkcoder.github.io/tags/mybatis/"}]},{"title":"Elasticsearch查询api--Terms Query","slug":"elasticsearch-terms-query","date":"2014-03-20T00:24:42.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/03/20/elasticsearch-terms-query/","link":"","permalink":"http://nkcoder.github.io/2014/03/20/elasticsearch-terms-query/","excerpt":"1. terms query查询 提供多个查询词，以数组形式出现，查询指定的字段是否包含其中一个或者多个查询词； 功能与：bool query通过对多个term进行should操作的功能是一致的，是更简单的一种语法形式； 提供的查询词也是不分词的，只有完全包含才算匹配；可以通过minimum_should_match属性指定最少匹配的个数；","text":"1. terms query查询 提供多个查询词，以数组形式出现，查询指定的字段是否包含其中一个或者多个查询词； 功能与：bool query通过对多个term进行should操作的功能是一致的，是更简单的一种语法形式； 提供的查询词也是不分词的，只有完全包含才算匹配；可以通过minimum_should_match属性指定最少匹配的个数； 2. REST APIterms query的REST API示例如下： { &quot;query&quot;: { &quot;terms&quot;: { &quot;tvName&quot;: [&quot;日本&quot;, &quot;停电&quot;], &quot;minimum_should_match&quot;: 1 } } } 在REST API中，terms query使用terms关键字，其中tvName表示要查询的字段，minimum_should_match表示至少要匹配多少个查询词。 3. Java APIterms query的Java API示例如下： public static void termsQuery() { String[] queryWords = new String[] {&quot;日本&quot;, &quot;停电&quot;}; QueryBuilder queryBuilder = QueryBuilders.termsQuery(ConstantUtil.FIELD_TV_NAME, queryWords) .minimumShouldMatch(&quot;2&quot;); SearchResponse response = searchRequestBuilder.setQuery(queryBuilder) .setFrom(0).setSize(5).execute().actionGet(); printResult(response); } 在Java API中，terms query使用termsQuery()函数，除了minimumShouldMatch()属性，还有boost()和minimumMatch()属性。 4. 看看Java源码public static TermsQueryBuilder termsQuery(String name, String... values) { return new TermsQueryBuilder(name, values); } public static TermsQueryBuilder termsQuery(String name, Object... values) { return new TermsQueryBuilder(name, values); } public static TermsQueryBuilder termsQuery(String name, Collection&lt;?&gt; values) { return new TermsQueryBuilder(name, values); } public TermsQueryBuilder(String name, Collection values) { this(name, values.toArray()); } public TermsQueryBuilder(String name, Object... values) { this.name = name; this.values = values; } termsQuery()函数的第二个参数是一个查询词数组，可以是基本类型的数组，可以是Object数组，也可以是一个集合类型；其实，内部都是调用TermsQueryBuilder的一个构造函数，即上述的最后一个函数，第二个参数是Object数组； 参考 terms-query","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://nkcoder.github.io/tags/elasticsearch/"}]},{"title":"Elasticsearch查询api--Term Query","slug":"elasticsearch-term-query","date":"2014-03-18T12:46:56.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/03/18/elasticsearch-term-query/","link":"","permalink":"http://nkcoder.github.io/2014/03/18/elasticsearch-term-query/","excerpt":"1. term query查询 在给定的字段里查询词或者词组； 提供的查询词是不分词的(not analyzed)，即只有完全包含才算匹配； 支持boost属性，boost可以提高field和document的相关性；","text":"1. term query查询 在给定的字段里查询词或者词组； 提供的查询词是不分词的(not analyzed)，即只有完全包含才算匹配； 支持boost属性，boost可以提高field和document的相关性； 2. REST API$ curl -XGET &apos;localhost:9200/video/video_info/_search?pretty&apos; -d @term_query.json term_query.json简单的字段查询： { &quot;query&quot;: { &quot;term&quot;: { &quot;tvName&quot;: &quot;童年&quot; } } } 添加boost属性： { &quot;query&quot;: { &quot;term&quot;: { &quot;tvName&quot;: { &quot;value&quot;: &quot;童年&quot;, &quot;boost&quot;: 10 } } } } 3. Java APIpublic static void termQuery() { String queryWord = &quot;巨头&quot;; QueryBuilder queryBuilder = QueryBuilders.termQuery(ConstantUtil.FIELD_TV_NAME, queryWord) .boost(10); SearchResponse response = searchRequestBuilder.setQuery(queryBuilder) .setFrom(0).setSize(5).execute().actionGet(); printResult(response); } 4. 看看Java源码以下是term_query相关的java源码： public static TermQueryBuilder termQuery(String name, String value) { return new TermQueryBuilder(name, value); } public static TermQueryBuilder termQuery(String name, int value) { return new TermQueryBuilder(name, value); } public TermQueryBuilder(String name, Object value) { this.name = name; this.value = value; } public TermQueryBuilder boost(float boost) { this.boost = boost; return this; } 从源码可以看到：termQuery方法内部都是调用TermQueryBuilder的构造函数，而TermQueryBuilder有很多重载的构造函数，最主要的就是上面的第三个函数，即第二个参数是Object类型，其它的构造函数都是调用该函数实现的。 参考资料： elasticsearch-guide","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://nkcoder.github.io/tags/elasticsearch/"}]},{"title":"Spring持久化之JDBC","slug":"spring-jdbc","date":"2014-03-16T10:01:26.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/03/16/spring-jdbc/","link":"","permalink":"http://nkcoder.github.io/2014/03/16/spring-jdbc/","excerpt":"这个小系列介绍Spring的持久化策略，主要关注当前应用最广泛的三种方式：Spring JDBC， MyBatis以及Hibernate。使用JDBC的优劣如下： 优势：相对于持久化框架，使用JDBC，不需要掌握其它框架的查询语言，允许我们在更低的层次上处理数据，能够访问数据库中单独的列，而且能够更好地对数据访问的性能进行调优。 劣势：随着项目的规模和复杂度的提升，使用JDBC会非常繁琐，同时不易于处理复杂的问题。 Spring JDBC：提供数据访问模板，简化JDBC编程，同时提供了平台无关的持久化异常体系。","text":"这个小系列介绍Spring的持久化策略，主要关注当前应用最广泛的三种方式：Spring JDBC， MyBatis以及Hibernate。使用JDBC的优劣如下： 优势：相对于持久化框架，使用JDBC，不需要掌握其它框架的查询语言，允许我们在更低的层次上处理数据，能够访问数据库中单独的列，而且能够更好地对数据访问的性能进行调优。 劣势：随着项目的规模和复杂度的提升，使用JDBC会非常繁琐，同时不易于处理复杂的问题。 Spring JDBC：提供数据访问模板，简化JDBC编程，同时提供了平台无关的持久化异常体系。 1. 配置数据源在生产环境，考虑到性能，应该使用数据库连接池。以commons dbcp配置为例，配置如下： &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt; &lt;bean id=&quot;vrsDataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot; destroy-method=&quot;close&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;${jdbc.drvier}&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;${jdbc.videodb.url}&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;${jdbc.videodb.username}&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;${jdbc.videodb.password}&quot;/&gt; &lt;property name=&quot;maxActive&quot; value=&quot;${jdbc.videodb.maxActive}&quot;/&gt; &lt;property name=&quot;initialSize&quot; value=&quot;${jdbc.videodb.initialSize}&quot;/&gt; &lt;/bean&gt; 主要属性参数说明： driverClassName: JDBC驱动的全限定类名，如mysql就是：com.mysql.jdbc.Driver； url：JDBC的url，如使用mysql的url：jdbc:mysql://10.11.132.193:3306/vrs； username, password: 连接该数据源的用户名和密码； initialSize：表示初始大小，即连接池启动时创建的连接数量； maxActive：表示同一时间可从池中分配的最大连接数，0表示无限制； maxIdle：池里不会被释放的最大连接数，0表示无限制； minIdle：在不创建新连接的情况下，池中保持空闲的最小连接数； maxWait：没有可用连接时，在抛出异常之前，池等待连接回收的最大时间；-1表示无线等待； validationQuery：验证连接的sql查询，至少返回一行；更多属性参考wiki页 其它推荐的连接池有： c3p0: 适合多线程环境； druid：完善的监控功能； 2. 使用JdbcTemplate使用JdbcTemplate类，在sql语句中，以?作为参数的占位符，传入的参数的顺序与sql语句中?的顺序必须是一一对应的。 首先在xml配置文件里添加jdbcTemplate的bean，其参数dataSource引用之前定义的数据源： &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;vrsDataSource&quot;/&gt; &lt;/bean&gt; 然后，在dao类中引用JdbcTemplate类，并注入： @Resource(name = &quot;jdbcTemplate&quot;) JdbcTemplate jdbcTemplate; 查询返回基本类型： String getUserById = &quot;select name from students where id = ?&quot;; String userName = jdbcTemplate.queryForObject(getUserById, new Object[]{3}, String.class); 返回Map&lt;String, Object&gt;，key为列名，value为对应列的值，此时返回值只能有一行，否则报错： String getStudentByNumber = &quot;select * from students where number = ?&quot;; Map&lt;String, Object&gt; studentMap = jdbcTemplate.queryForMap(getStudentByNumber, 111); 返回List&lt;Map&lt;String, Object&gt;&gt;, 可以返回多列： String getStudentsByCity = &quot;select * from students where city = ?&quot;; List&lt;Map&lt;String, Object&gt;&gt; studentList = jdbcTemplate.queryForList(getStudentsByCity, &quot;tianjin&quot;); 返回自定义class的对象，需要实现RowMapper接口，定义列名和属性的映射：首先实现RowMapper接口： public class StudentRowMapper implements RowMapper&lt;Students&gt; { @Override public Students mapRow(ResultSet rs, int rowNum) throws SQLException { Students students = new Students(); students.setId(rs.getInt(&quot;id&quot;)); students.setName(rs.getString(&quot;name&quot;)); students.setSex(rs.getString(&quot;sex&quot;)); students.setNumber(rs.getInt(&quot;number&quot;)); students.setEnable(rs.getInt(&quot;enable&quot;)); students.setCity(rs.getString(&quot;city&quot;)); return students; } } 然后，使用query或者queryForObject查询多行或一行： String getStudentsByCity = &quot;select * from students where city = ?&quot;; List&lt;Students&gt; studentsList = jdbcTemplate.query(getStudentsByCity, new StudentRowMapper(), &quot;tianjin&quot;); 2. 使用NamedParameterJdbcTemplate使用NamedParameterJdbcTemplate类，sql语句中的参数是以命名的变量来表示，传入参数时，只要参数名一致即可，索引位置不必一一对应。首先在xml配置文件里定义NamedParameterJdbcTemplate的bean： &lt;bean id=&quot;namedParameterJdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate&quot;&gt; &lt;constructor-arg name=&quot;dataSource&quot; ref=&quot;feedbackDataSource&quot;/&gt; &lt;/bean&gt; 然后将namedParameterJdbcTemplate bean注入到dao中， @Resource(name = &quot;namedParameterJdbcTemplate&quot;) NamedParameterJdbcTemplate namedParameterJdbcTemplate; 定义sql，参数以(:变量名)形式给出即可，如： String getStudentByNumber = &quot;select * from students where number = (:number) and city = (:city)&quot;; NamedParameterJdbcTemplate namedParameterJdbcTemplate = (NamedParameterJdbcTemplate) context.getBean(&quot;namedParameterJdbcTemplate&quot;); Map&lt;String, Object&gt; queryMap = new HashMap&lt;String, Object&gt;(); queryMap.put(&quot;number&quot;, 333); queryMap.put(&quot;city&quot;, &quot;tianjin&quot;); List&lt;Map&lt;String, Object&gt;&gt; mapList = namedParameterJdbcTemplate.queryForList(getStudentByNumber, queryMap); 除了使用命名参数外，NamedParameterJdbcTemplate与JdbcTempate的主要用法都是一致的。 3. 使用JdbcDaoSupportJdbcDaoSupport是一个父类，如果有多个dao类，通过继承JdbcDaoSupport，可以更方便地获取jdbcTemplate。首先让dao类继承JdbcDaoSupport类： public class InfoJdbcImpl extends JdbcDaoSupport implements VideoInfoDao { 然后在定义dao类的bean时，注入一个jdbcTemplate属性，或者直接注入一个dataSource属性（这两个属性来自于JdbcDaoSupport）： &lt;bean id=&quot;infoJdbcImpl&quot; class=&quot;com.sohu.tv.dao.impl.InfoJdbcImpl&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;feedbackDataSource&quot;/&gt; &lt;/bean&gt; 然后通过getJdbcTemplate()获取jdbcTemplate，实现dao层的逻辑： String getStudentByNumber = &quot;select * from students where number = ?&quot;; Map&lt;String, Object&gt; studentMap = getJdbcTemplate().queryForMap(getStudentByNumber, 111); NamedParameterJdbcDaoSupport与JdbcDaoSupport的用法类似。 参考资料 Spring实战(第3版) spring-javadoc-api","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://nkcoder.github.io/tags/spring/"}]},{"title":"Spring AOP之入门实例","slug":"spring-aop-example","date":"2014-03-13T14:58:06.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/03/13/spring-aop-example/","link":"","permalink":"http://nkcoder.github.io/2014/03/13/spring-aop-example/","excerpt":"","text":"依赖注入（DI）有助于应用对象之间的解耦，而面向切面编程（AOP）有助于横切关注点与所影响的对象之间的解耦。所谓横切关注点，即影响应用多处的功能，这些功能各个应用模块都需要，但又不是其主要关注点，常见的横切关注点有日志、事务和安全等。 将横切关注点抽离形成独立的类，即形成了切面。切面主要由切点和通知构成，通知定义了切面是什么，以及何时执行何种操作；切点定义了在何处执行通知定义的操作。 下面以简单的日志作为切面，分别介绍通过xml配置和注解实现AOP。 0. 添加依赖因为Spring AOP用到AspectJ的相关对象和注解，需要添加AspectJ的依赖： &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjrt&lt;/artifactId&gt; &lt;version&gt;1.7.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.7.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; 1. 通过XML配置实现1.1 将日志类作为切面，定义为bean即可： @Service(&quot;logService&quot;) public class LogService { private Logger logger = LoggerFactory.getLogger(LogService.class); public void printLog(ProceedingJoinPoint joinPoint) throws Throwable{ logger.info(&quot;before service is called...&quot;); String methodModifier = joinPoint.getSignature().getDeclaringTypeName(); String methodName = joinPoint.getSignature().getName(); Object[] argsArr = joinPoint.getArgs(); logger.info(&quot;class: {}; method: {}; args: {}&quot;, methodModifier, methodName, Arrays.toString(argsArr)); joinPoint.proceed(); logger.info(&quot;after service is called....&quot;); } } 首先，通过@Service注解将类定义为bean；方法printLog的参数ProceedingJoinPoint对象属于AspectJ，通过这个对象可以获取被通知的对象和方法的信息，并通过proceed方法执行被通知的对象。 1.2 在xml配置文件里定义切面和切点 &lt;aop:config&gt; &lt;aop:aspect ref=&quot;logService&quot;&gt; &lt;aop:pointcut id=&quot;serviceLog&quot; expression=&quot;execution(* org.yousharp.service.VideoService..*(..))&quot;/&gt; &lt;aop:around method=&quot;printLog&quot; pointcut-ref=&quot;serviceLog&quot;/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt; aop:aspect通过ref引用logService bean，将LogService定义为切面。aop:pointcut定义一个切点，被通知的对象通过execution表达式指定，即当VideoService下的任意方法被执行时，触发切面。aop:aroud定义一个环绕通知，引用切点*serviceLog，当切面被触发时，调用method`指定的方法。 2. 通过注解实现通过注解，可以直接在类上定义切面、通知和切点，无需任何xml配置。 @Service(&quot;logService&quot;) @Aspect public class LogService { private Logger logger = LoggerFactory.getLogger(LogService.class); @Pointcut(value = &quot;execution(* org.yousharp.service.VideoService.saveVideoInfo(..))&quot;) public void saveVideo() { } @Around(value = &quot;saveVideo()&quot;) public void printLog(ProceedingJoinPoint joinPoint) throws Throwable{ logger.info(&quot;before service is called...&quot;); String methodModifier = joinPoint.getSignature().getDeclaringTypeName(); String methodName = joinPoint.getSignature().getName(); Object[] argsArr = joinPoint.getArgs(); logger.info(&quot;class: {}; method: {}; args: {}&quot;, methodModifier, methodName, Arrays.toString(argsArr)); joinPoint.proceed(); logger.info(&quot;after service is called....&quot;); } } @Aspect注解表示将该类定义为切面，@Pointcut定义切点，execution表达式与xml配置一样；切点的id即为空方法的方法名（这里的方法内容不重要，主要是供切点依附），这里即为saveVideo；@Around表示定义环绕通知，需要引用切点。 参考资料 Spring实战(第3版)","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://nkcoder.github.io/tags/spring/"}]},{"title":"Spring依赖注入之Java注解","slug":"spring-di-java","date":"2014-03-10T15:29:44.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/03/10/spring-di-java/","link":"","permalink":"http://nkcoder.github.io/2014/03/10/spring-di-java/","excerpt":"基于Java的注解，是在类级别上定义beans，在方法级别上定义bean。和自动注解一样，可以减少xml的配置。 1. xml配置在xml配置文件中添加自动扫描的配置： &lt;context:component-scan base-package=&quot;org.yousharp&quot;/&gt; 该配置不仅扫描构造型注解(@Controller, @Service, @Repository, @Component)，也会扫描@Configuration注解。","text":"基于Java的注解，是在类级别上定义beans，在方法级别上定义bean。和自动注解一样，可以减少xml的配置。 1. xml配置在xml配置文件中添加自动扫描的配置： &lt;context:component-scan base-package=&quot;org.yousharp&quot;/&gt; 该配置不仅扫描构造型注解(@Controller, @Service, @Repository, @Component)，也会扫描@Configuration注解。 2. 定义dao类和service类dao类是service类的一个属性，是需要注入的对象。 public class VideoService { SaveVideoInfoDao saveVideoInfoDao; public VideoService(){} public VideoService(SaveVideoInfoDao saveVideoInfoDao) { this.saveVideoInfoDao = saveVideoInfoDao; } public void saveVideoInfo(String info) { saveVideoInfoDao.printMesg(info); } } public class SaveVideoInfoDao { public void printMesg(String message) { System.out.println(&quot;saving video info....&quot;); } } dao类和service类就是两个很普通的java类，没有任何注解依附。 3. 定义一个配置类单独定义一个配置类（在自动扫描包下），在类上使用@Configuration注解，在方法上使用@Bean注解： @Configuration public class AppConfig { @Bean public SaveVideoInfoDao saveVideoInfoDao() { return new SaveVideoInfoDao(); } @Bean public VideoService videoService() { VideoService videoService = new VideoService(saveVideoInfoDao()); return videoService; } } @Configuration注解等价于xml配置中的beans，该注解告诉Spring该类中包含一个或多个bean的定义；@Bean注解等价于xml配置中的bean，将方法返回的对象定义为bean，方法名为bean的id；上面第一个bean的定义等价于xml配置： &lt;bean id=&quot;saveInfoDao&quot; class=&quot;org.yousharp.AppConfig&quot;/&gt; 第二个bean的定义通过构造函数实现依赖注入，其含义等价于xml配置： &lt;bean id=&quot;videoService&quot; class=&quot;org.yousharp.AppConfig&quot;&gt; &lt;constructor-arg name=&quot;videoService&quot; ref=&quot;saveVideoInfoDao&quot;&gt; &lt;/bean&gt; 这里是通过构造函数实现注入，当然也可以通过setter方法实现注入。在VideoInfoService中去掉带参数的构造函数，同时对属性saveVideoInfoDao添加setter方法，修改AppConfig类中的第二个bean为： @Bean public VideoService videoService() { VideoService videoService = new VideoService(); videoService.setSaveVideoInfoDao(saveVideoInfoDao()); return videoService; } 4. Controller中引用Controller里和使用xml配置bean和自动注解一样，通过bean的id获取对bean的引用。这里VideoService的bean的id为videoService。 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); public void execute() { VideoService videoService = (VideoService) context.getBean(&quot;videoService&quot;); String videoInfo = &quot;vrs video info&quot;; videoService.saveVideoInfo(videoInfo); } 参考资料 Spring实战(第3版)","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://nkcoder.github.io/tags/spring/"}]},{"title":"Spring依赖注入之注解注入","slug":"spring-di-annotation","date":"2014-03-09T10:19:11.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/03/09/spring-di-annotation/","link":"","permalink":"http://nkcoder.github.io/2014/03/09/spring-di-annotation/","excerpt":"1. bean的自动装配和自动检测的区别启用注解装配，在xml配置文件里添加： &lt;context:annotation-config/&gt; 启用自动扫描注解，在xml配置文件里添加： &lt;context:component-scan base-package=&quot;org.yousharp.base&quot;/&gt;","text":"1. bean的自动装配和自动检测的区别启用注解装配，在xml配置文件里添加： &lt;context:annotation-config/&gt; 启用自动扫描注解，在xml配置文件里添加： &lt;context:component-scan base-package=&quot;org.yousharp.base&quot;/&gt; 二者的区别：自动装配表示通过@Autowired, @Inject, @Resource等实现对属性或构造函数的自动注入；仍然需要在配置文件里定义bean，只是通过自动装配省去了property和constructor-arg的配置。自动检测是扫描特定的注解（包括：@Component, @Controller, @Service, @Repository)，将注解过的类自动定义为bean，自动检测是自动装配的超集，通过自动检测，可以省去在xml配置文件里定义bean了。 2. 通过自动装配和注解实现注入在xml配置文件里启动注解装配： &lt;context:annotation-config/&gt; 定义bean： &lt;bean id=&quot;saveVideoInfoDao&quot; class=&quot;org.dao.SaveVideoInfoDao&quot;/&gt; 然后在需要注入的属性上或者其setter方法上，或者构造函数上，添加@Autowired, @Inject, @Resource等注解实现自动注入： //@Autowired //@Inject @Resource public void setSaveVideoInfoDao(SaveVideoInfoDao videoInfoDao) { this.saveVideoInfoDao = videoInfoDao; } @Autowired, @Inject, @Resource的区别：@Autowired是Spring 3.0的注解，是通过byType形式实现注解，可以通过@Qualifier根据bean的id进行限定；使用@Autowired注解即引入了对Spring的依赖。@Inject是JSR 330的注解，使用该注解需要导入包javax.inject，@Named(value=””)可以根据bean的id进行限定；@Resource是JSR 250的注解，可以通过value限定bean的id，如@Resource(value=””); 3. 使用自动检测注解实现注入在xml配置文件里增加自动检测的配置： &lt;context:component-scan base-package=&quot;org.yousharp.base&quot;/&gt; 将需要被自动检测而注册为bean的类使用对应的构造型注解： @Repository public class SaveVideoInfoDao { public void printMesg(String message) { System.out.println(&quot;saving video info....&quot;); } } 使用注解对依赖的属性进行输入： @Service public class VideoService { @Resource SaveVideoInfoDao saveVideoInfoDao; public void saveVideoInfo(String info) { saveVideoInfoDao.printMesg(info); } } @Component是通用的注解，@Controller表示将该类定义为Spring MVC的控制器，@Service定义服务层，@Repository定义数据仓库；这些构造型注解默认以无限定类名作为bean的id，也可以显式指定id名称，如@Service(“videoInfoService”)或者@Service(name=”videoInfoService”); 4. 自动检测注解的过滤可以为扫描行为定义过滤器，如context:include-filter, context:exclude-filter &lt;context:component-scan base-package=&quot;org.yousharp&quot;&gt; &lt;context:include-filter type=&quot;assignable&quot; expression=&quot;org.yousharp.dao.SaveVideoInfoDao&quot;/&gt; &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Component&quot;/&gt; &lt;/context:component-scan&gt; context:include-filter表示需要扫描并注解的类，context:exclude-filter表示扫描时需要排除的包；type一种有5中，assignable表示继承自expression所指定的包，annotation表示所有expression所指定的注解。 参考资料 Spring实战(第3版)","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://nkcoder.github.io/tags/spring/"}]},{"title":"Spring依赖注入之XML配置","slug":"spring-di-xml","date":"2014-03-08T13:36:47.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/03/08/spring-di-xml/","link":"","permalink":"http://nkcoder.github.io/2014/03/08/spring-di-xml/","excerpt":"因为项目中一直要用到Spring相关的知识，所以最近在看Spring实战这本书，希望对Spring有一个整理的了解和把握。不得不说，这是一本好书，中文译本也不错。这个系列，记录自己关于Spring的笔记和理解，一是加深理解，同时也希望对别人有所帮助。","text":"因为项目中一直要用到Spring相关的知识，所以最近在看Spring实战这本书，希望对Spring有一个整理的了解和把握。不得不说，这是一本好书，中文译本也不错。这个系列，记录自己关于Spring的笔记和理解，一是加深理解，同时也希望对别人有所帮助。 1. 通过构造方法和xml配置注入 定义带参数的构造函数，使依赖对象作为构造函数的参数： public class VideoInfoServiceImpl implements VideoInfoService { VideoInfoDao videoInfoDao; public VideoInfoServiceImpl() {} public VideoInfoServiceImpl(VideoInfoDao videoInfoDao) { this.videoInfoDao = videoInfoDao; } @Override public List&lt;Map&lt;String, Object&gt;&gt; getVideos(Map&lt;String, Object&gt; queryMap) { List&lt;Map&lt;String, Object&gt;&gt; videoList = videoInfoDao.getVideos(queryMap); return videoList; } } 在定义bean时，使用constructor-arg标签，name表示构造函数的参数，ref表示引用另一个bean；参数也可以为基本类型，此时使用value标签，而不是ref。(如果不使用constructor-arg，则bean实例化时调用的是默认的不带参数的构造函数) &lt;bean id=&quot;videoInfoServiceImpl&quot; class=&quot;com.sohu.tv.service.impl.VideoInfoServiceImpl&quot;&gt; &lt;constructor-arg name=&quot;videoInfoDao&quot; ref=&quot;videoInfoDaoImpl&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;videoInfoServiceImpl&quot; class=&quot;com.sohu.tv.service.impl.VideoInfoServiceImpl&quot;&gt; &lt;constructor-arg name=&quot;id&quot; value=&quot;25&quot;/&gt; &lt;/bean&gt; 2. 通过setter方法和xml配置注入 定义依赖对象的setter方法，在bean定义时，使用property标签表示调用指定属性的setter方法，实现注入： public class VideoInfoServiceImpl implements VideoInfoService { VideoInfoDao videoInfoDao; @Override public List&lt;Map&lt;String, Object&gt;&gt; getVideos(Map&lt;String, Object&gt; queryMap) { List&lt;Map&lt;String, Object&gt;&gt; videoList = videoInfoDao.getVideos(queryMap); return videoList; } public void setVideoInfoDao(VideoInfoDao videoInfoDao) { this.videoInfoDao = videoInfoDao; } } property指定属性为videoInfoDao，则bean实例化后调用setVideoInfoDao方法实现注入；ref表示引用另一个bean，value表示基本类型值，int, float, true等。 &lt;bean id=&quot;videoInfoServiceImpl&quot; class=&quot;com.sohu.tv.service.impl.VideoInfoServiceImpl&quot;&gt; &lt;property name=&quot;videoInfoDao&quot; ref=&quot;videoInfoDaoImpl&quot;/&gt; &lt;/bean&gt; 此时也可以通过bean的p属性实现注入，比较简洁，需要加入p命令空间，和property方式是等价的。 xmlns:p=&quot;http://www.springframework.org/schema/p&quot; &lt;bean id=&quot;videoInfoServiceImpl&quot; class=&quot;com.sohu.tv.service.impl.VideoInfoServiceImpl&quot; p:videoInfoDao-ref=&quot;videoInfoDaoImpl&quot; /&gt; 3. 通过SpEL表达式注入 SpEL可以实现在运行时装配。SpEL通过#{}获取变量的值，{}中既可以是值类型，也可以是引用类型，通过bean的id引用其它的bean，此时，在SpEL中通过value引用其它的bean和通过ref直接引用bean的效果是相同的。 &lt;bean id=&quot;videoInfoServiceImpl&quot; class=&quot;com.sohu.tv.service.impl.VideoInfoServiceImpl&quot;&gt; &lt;!--&lt;property name=&quot;videoInfoDao&quot; ref=&quot;videoInfoDaoImpl&quot;/&gt;--&gt; &lt;property name=&quot;videoInfoDao&quot; value=&quot;#{videoInfoDaoImpl}&quot; /&gt; &lt;/bean&gt; 4. 通过setter方法和autowire=”byName”自动装配注入 根据bean的名字进行注入，使用autowire=”byName”，则寻找与属性名字相同的bean，通过属性的setter方法注入。 &lt;bean id=&quot;saveVideoInfoDao&quot; class=&quot;org.dao.SaveVideoInfoDao&quot;/&gt; &lt;bean id=&quot;videoService&quot; class=&quot;org.service.VideoService&quot; autowire=&quot;byName&quot;/&gt; 5. 通过setter方法和autowire=”byType”自动装配注入 根据bean的类型进行注入，使用autowire=”byType”，则寻找与属性名字相同的bean，通过属性的setter方法注入： &lt;bean id=&quot;saveVideoInfoDao&quot; class=&quot;org.dao.SaveVideoInfoDao&quot;/&gt; &lt;bean id=&quot;videoService&quot; class=&quot;org.service.VideoService&quot; autowire=&quot;byType&quot;/&gt; 6. 通过factory-method给单例类定义bean 如果类的构造函数不可用，比如单例类，构造函数私有，通过public的接口获取实例；定义单例类的bean时，使用factory-method，表示调用类的静态方法构造实例： &lt;bean id=&quot;singleton&quot; class=&quot;yousharp.Singleton&quot; factory-method=&quot;getInstance&quot;/&gt; &lt;bean id=&quot;factoryService&quot; class=&quot;yousharp.FactoryService&quot;&gt; &lt;constructor-arg name=&quot;singleton&quot; ref=&quot;singleton&quot;/&gt; &lt;/bean&gt; 7. 定义bean的作用域 默认是singleton，可以设置scope属性为prototype，表示每次调用都创建一个新实例： &lt;bean id=&quot;videoInfoServiceImpl&quot; class=&quot;com.sohu.tv.service.impl.VideoInfoServiceImpl&quot; scope=&quot;prototype&quot;/&gt; 8. 装配list 数组或Collection的任意实现如List, ArrayList等都可以使用list和set来装配，区别在于set中不允许有重复元素。定义list成员的setter方法： private List&lt;VideoInfoDao&gt; videoInfoDaoList; public void setVideoInfoDaoList(List&lt;VideoInfoDao&gt; videoInfoDaoList) { this.videoInfoDaoList = videoInfoDaoList; } 在xml配置文件里，在property里定义list，给list成员注入实例： &lt;bean id=&quot;videoInfoServiceImpl&quot; class=&quot;com.sohu.tv.service.impl.VideoInfoServiceImpl&quot;&gt; &lt;property name=&quot;videoInfoDaoList&quot;&gt; &lt;list&gt; &lt;ref bean=&quot;jdbcDaoImple&quot;/&gt; &lt;ref bean=&quot;mybatisDaoImpl&quot;/&gt; &lt;ref bean=&quot;hibernateDaoImpl&quot;/&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; 9. 装配map 如果需要对一个map成员变量进行注入，在property里，通过map来实现注入，其中key和value都可以是值类型或引用类型； private Map&lt;String, VideoInfoDaoImpl&gt; videoInfoDaoMap ; public void setVideoInfoDaoList(Map&lt;String, VideoInfoDaoImpl&gt; videoInfoDaoMap) { this.videoInfoDaoMap = videoInfoDaoMap; } &lt;bean id=&quot;videoInfoServiceImpl&quot; class=&quot;com.sohu.tv.service.impl.VideoInfoServiceImpl&quot;&gt; &lt;property name=&quot;videoInfoDaoList&quot;&gt; &lt;map&gt; &lt;entry key=&quot;jdbc&quot; value-ref=&quot;videoInfoDaoImpl&quot;/&gt; &lt;entry key=&quot;mybatis&quot; value-ref=&quot;videoInfoDaoImpl&quot;/&gt; &lt;entry key=&quot;hibernate&quot; value-ref=&quot;videoInfoDaoImpl&quot;/&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt; 10. 装配property property也是key-value对，与map的区别是，其key和value必须都是String类型。 Properties props; public void setProps(Properties props) { this.props = props; } &lt;bean id=&quot;videoInfoServiceImpl&quot; class=&quot;com.sohu.tv.service.impl.VideoInfoServiceImpl&quot; &gt; &lt;property name=&quot;props&quot;&gt; &lt;props&gt; &lt;prop key=&quot;daily&quot;&gt;30 00 * * *&lt;/prop&gt; &lt;prop key=&quot;weekly&quot;&gt;30 00 * * 1&lt;/prop&gt; &lt;prop key=&quot;monthly&quot;&gt;30 00 * 1 *&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; 参考资料 Spring实战(第3版)","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://nkcoder.github.io/tags/spring/"}]},{"title":"分布式搜索引擎ElasticSearch安装与配置","slug":"elasticsearch-install-config","date":"2014-02-17T14:49:20.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/02/17/elasticsearch-install-config/","link":"","permalink":"http://nkcoder.github.io/2014/02/17/elasticsearch-install-config/","excerpt":"1. 安装与启动1.1 安装ElasticSearch 两种安装方式，一种是根据发行版，通过ElasticSearch提供的repositories安装，参考官方指南; 另一种是利用压缩包安装；由于第二种方式，在配置上更加灵活，这里主要介绍第二种方式的安装和配置。","text":"1. 安装与启动1.1 安装ElasticSearch 两种安装方式，一种是根据发行版，通过ElasticSearch提供的repositories安装，参考官方指南; 另一种是利用压缩包安装；由于第二种方式，在配置上更加灵活，这里主要介绍第二种方式的安装和配置。 从ElasticSearch网站下载压缩包：elasticsearch-1.0.0.tar.gz，解压到指定目录： $ wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.0.0.tar.gz -P /opt/app $ cd /opt/app $ tar zxvf elasticsearch-1.0.0.tar.gz -C ./ 启动ElasticSearch很简单，执行安装目录下bin/elasticsearch即可；ElasticSearch 1.0.0的启动机制有一些变化，默认在前台运行，-d参数表示在后台运行；另外还提供-p参数，后接文件名，保存当前ElasticSearch进程的pid，方便关闭进程； $ bin/elasticsearch -d -p /tmp/es.pid $ kill `cat /tmp/es.pid` 另外还有一种启动方式，通过serviceWrapper，可以更加方便地控制ElasticSearch的启动、关闭以及重启，在下一节安装插件中介绍。 1.2 安装插件ElasticSearch官方和社区提供了很多插件用于将集群和节点的状态可视化。推荐安装的插件有：elasticsearch-head， marvel，bigdesk，servicewrapper // elasticsearch-head的安装与访问 # bin/plugin --install mobz/elasticsearch-head http://localhost:9200/_plugin/head/ // marvel的安装与访问 # bin/plugin -i elasticsearch/marvel/latest http://localhost:9200/_plugin/marvel // bigdesk的安装与访问（当前不支持ElasticSearch 1.0.0） # bin/plugin --install lukas-vlcek/bigdesk http://localhost:9200/_plugin/bigdesk/ 下面介绍servicewrapper的使用：从该链接下载，解压后将service文件夹放到ElasticSearch安装目录下的bin目录下。 $ wget https://github.com/elasticsearch/elasticsearch-servicewrapper/archive/master.zip -P /opt/app $ cd /opt/app $ unzip elasticsearch-servicewrapper-master -d ./ $ mv elasticsearch-servicewrapper-master/service elasticsearch-1.0.0/bin/ 配置: 编辑elasticsearch.conf文件，主要是配置前两行：ES_HOME及ES_HEAP_SIZE $ vim bin/service/elasticsearch.conf // set.default.ES_HOME=/data/app/elasticsearch-1.0.0 // set.default.ES_HEAP_SIZE=2048 用法：bin/service/elasticsearch start | stop | restart | install | remove | console | condrestart | status $ bin/service/elasticsearch start 2. 配置 配置文件为{ES_HOME}/config目录下的elasticsearch.yml，主要对集群、节点、索引（分片和副本）以及持久化和集群发现机制等进行参数设置。这里对一些常见的配置进行简单的说明和示例，配置文件的详细解释和说明会单独写一篇文章。 // 集群的名称，用于集群内节点的自动发现 cluster.name: es-test // 节点的名称，标识一个节点 node.name: node_79_75 // 可以被选为主节点，同时可以存储数据 node.master: true node.data: true //节点的分片和副本的数量 index.number_of_shards: 3 index.number_of_replicas: 1 // 配置文件、索引数据以及日志和插件等的目录，如果不存在，新建即可 path.conf: /data/app/elasticsearch-1.0.0/config path.data: /data/app/elasticsearch-1.0.0/data path.logs: /data/app/elasticsearch-1.0.0/logs path.plugins: /data/app/elasticsearch-1.0.0/plugins // 锁定内存，防止内存交换影响ES性能 bootstrap.mlockall: true // 持久化策略，本次存储 gateway.type: local // 启用单播，显式指定节点的发现，当节点不在同一网段，无法自动发现时，启动单播，指定要发现的节点列表 // 比如当前节点的ip为10.10.79.75，与另两个节点不在同一网段，使用单播机制 discovery.zen.ping.multicast.enabled: false discovery.zen.ping.unicast.hosts: [&quot;10.11.52.131:9300&quot;, &quot;10.11.52.134:9300&quot;] 参考 ElasticSearch reference ElasticSearch配置文件详解","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://nkcoder.github.io/tags/elasticsearch/"}]},{"title":"xmemcached主要用法及与Spring集成","slug":"xmemcached-usage-with-spring","date":"2014-02-16T13:35:36.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/02/16/xmemcached-usage-with-spring/","link":"","permalink":"http://nkcoder.github.io/2014/02/16/xmemcached-usage-with-spring/","excerpt":"xMemcached是memcached的一个java客户端，基于java nio，支持memcached的所有协议。本文简要介绍xMemcached的基本用法，以及与Spring的集成配置。 xMemcached的主要方法示例","text":"xMemcached是memcached的一个java客户端，基于java nio，支持memcached的所有协议。本文简要介绍xMemcached的基本用法，以及与Spring的集成配置。 xMemcached的主要方法示例 /** * XMemcachedClientBuilder是MemcachedClientBuilder的一个实现类，XMemcachedClient是MemcachedClient的一个实现类； * 在实际使用中，应该使用接口，利用多态特性，这里直接使用实现类，是为了方便查看实现类的源码。 */ /** * MemcachedClientBuilder builder = new XMemcachedClientBuilder(AddrUtil.getAddresses(&quot;192.168.56.200:11211&quot;)); * MemcachedClient memcachedClient = null; */ XMemcachedClientBuilder builder = new XMemcachedClientBuilder(AddrUtil.getAddresses(&quot;192.168.56.200:11211 192.168.56.200:11212&quot;)); XMemcachedClient xMemcachedClient = null; // 注意处理相关异常 // 默认是采用余数哈希，可以修改为一致性哈希 builder.setSessionLocator(new KetamaMemcachedSessionLocator()); // 启用二进制协议，getAndTouch等方法仅在二进制协议中支持 builder.setCommandFactory(new BinaryCommandFactory()); // build the memcached client xMemcachedClient = (XMemcachedClient) builder.build(); // set: 第一个参数是key，第二个参数是超时时间，第三个参数是value xMemcachedClient.set(&quot;first&quot;, 0, &quot;tianjin&quot;); xMemcachedClient.add(&quot;second&quot;, 20, &quot;chengdu&quot;); xMemcachedClient.replace(&quot;first&quot;, 0, &quot;Beijing&quot;); // get：根据key获取value，第一个参数为key，第二个为超时； String firstValue = xMemcachedClient.get(&quot;first&quot;, new StringTranscoder()); logger.info(&quot;get first value: {}&quot;, xMemcachedClient.get(&quot;first&quot;)); // delete: 删除item xMemcachedClient.delete(&quot;second&quot;); xMemcachedClient.set(&quot;third&quot;, 20, &quot;xian&quot;); xMemcachedClient.deleteWithNoReply(&quot;third&quot;); // touch：修改item过期时间 xMemcachedClient.touch(&quot;first&quot;, 0); firstValue = xMemcachedClient.getAndTouch(&quot;first&quot;, 10); logger.info(&quot;getAndTouch op, first value: {}&quot;, firstValue); // append, prepend: 追加数据 xMemcachedClient.append(&quot;first&quot;, &quot;, come on&quot;); xMemcachedClient.prepend(&quot;first&quot;, &quot;hello &quot;); logger.info(&quot;append and prepend op, first value: {}&quot;, xMemcachedClient.get(&quot;first&quot;)); /** * cas: 通过gets操作返回GetResponse，其中包括value值和cas值 */ GetsResponse&lt;String&gt; response = xMemcachedClient.gets(&quot;first&quot;); long cas = response.getCas(); if (!xMemcachedClient.cas(&quot;first&quot;, 0, &quot;guangzhou&quot;, cas)) { logger.info(&quot;cas error&quot;); } // cas: 重试更方便；第一个方法表示重试次数，第二个方法表示更新的值 xMemcachedClient.cas(&quot;first&quot;, 0, new CASOperation&lt;String&gt;() { @Override public int getMaxTries() { return 1; } @Override public String getNewValue(long currentCAS, String currentValue) { return &quot;xian&quot;; } }); logger.info(&quot;cas op, first value: {}&quot;, xMemcachedClient.get(&quot;first&quot;)); // incr/decr: 值的增加减少 xMemcachedClient.incr(&quot;id&quot;, 3, 0); xMemcachedClient.incr(&quot;id&quot;, 5); xMemcachedClient.decr(&quot;id&quot;, 2); logger.info(&quot;incr/decr op, value: {}&quot;, xMemcachedClient.get(&quot;id&quot;)); // namespace: set时指定命名空间，get时也需要指定，可以使命名空间中的所有items失效 String nsSohu = &quot;sohu&quot;; xMemcachedClient.withNamespace(nsSohu, new MemcachedClientCallable&lt;Object&gt;() { @Override public Object call(MemcachedClient client) throws MemcachedException, InterruptedException, TimeoutException { client.set(&quot;typeTv&quot;, 0, &quot;tv&quot;); client.set(&quot;typeMv&quot;, 0, &quot;mv&quot;); client.set(&quot;typeDrama&quot;, 0, &quot;drama&quot;); return null; } }); logger.info(&quot;namespace op, typeTv: {}&quot;, xMemcachedClient.withNamespace(nsSohu, new MemcachedClientCallable&lt;String&gt;() { @Override public String call(MemcachedClient client) throws MemcachedException, InterruptedException, TimeoutException { return client.get(&quot;typeTv&quot;); } })); xMemcachedClient.invalidateNamespace(nsSohu); // stats: 获取统计，也可以根据items获取统计 Map&lt;InetSocketAddress, Map&lt;String, String&gt;&gt; stats = xMemcachedClient.getStats(); for (InetSocketAddress addr: stats.keySet()) { logger.info(&quot;stats map: {}: {}&quot;, addr, stats.get(addr).toString()); } // flush_all: 是所有item都过期 xMemcachedClient.flushAll(&quot;192.168.56.200:11211&quot;); // 删除一个server xMemcachedClient.removeServer(&quot;192.168.56.200:11212&quot;); xMemcached与Spring集成 applicationContext.xml的配置： &lt;!--定义一个server--&gt; &lt;bean name=&quot;server1&quot; class=&quot;java.net.InetSocketAddress&quot;&gt; &lt;constructor-arg&gt;&lt;value&gt;192.168.56.200&lt;/value&gt;&lt;/constructor-arg&gt; &lt;constructor-arg&gt;&lt;value&gt;11211&lt;/value&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!--定义XMemcachedClientBuilder实例，第一个参数设置server，第二个参数设置权重--&gt; &lt;bean id=&quot;memcachedClientBuilder&quot; class=&quot;net.rubyeye.xmemcached.XMemcachedClientBuilder&quot;&gt; &lt;constructor-arg&gt; &lt;list&gt; &lt;bean class=&quot;java.net.InetSocketAddress&quot;&gt; &lt;constructor-arg&gt;&lt;value&gt;192.168.56.200&lt;/value&gt;&lt;/constructor-arg&gt; &lt;constructor-arg&gt;&lt;value&gt;11211&lt;/value&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean class=&quot;java.net.InetSocketAddress&quot;&gt; &lt;constructor-arg&gt;&lt;value&gt;192.168.56.200&lt;/value&gt;&lt;/constructor-arg&gt; &lt;constructor-arg&gt;&lt;value&gt;11212&lt;/value&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;/list&gt; &lt;/constructor-arg&gt; &lt;constructor-arg&gt; &lt;list&gt; &lt;value&gt;1&lt;/value&gt; &lt;value&gt;2&lt;/value&gt; &lt;/list&gt; &lt;/constructor-arg&gt; &lt;!--&amp;lt;!&amp;ndash;设置验证信息&amp;ndash;&amp;gt;--&gt; &lt;!--&lt;property name=&quot;authInfoMap&quot;&gt;--&gt; &lt;!--&lt;map&gt;--&gt; &lt;!--&lt;entry key-ref=&quot;server1&quot;&gt;--&gt; &lt;!--&lt;bean class=&quot;net.rubyeye.xmemcached.auth.AuthInfo&quot; factory-method=&quot;typical&quot;&gt;--&gt; &lt;!--&lt;constructor-arg index=&quot;0&quot;&gt;&lt;value&gt;index&lt;/value&gt;&lt;/constructor-arg&gt;--&gt; &lt;!--&lt;constructor-arg index=&quot;1&quot;&gt;&lt;value&gt;index-pd&lt;/value&gt;&lt;/constructor-arg&gt;--&gt; &lt;!--&lt;/bean&gt;--&gt; &lt;!--&lt;/entry&gt;--&gt; &lt;!--&lt;/map&gt;--&gt; &lt;!--&lt;/property&gt;--&gt; &lt;!--设置线程池--&gt; &lt;property name=&quot;connectionPoolSize&quot; value=&quot;2&quot;&gt;&lt;/property&gt; &lt;!--使用二进制协议--&gt; &lt;property name=&quot;commandFactory&quot;&gt; &lt;bean class=&quot;net.rubyeye.xmemcached.command.BinaryCommandFactory&quot;&gt;&lt;/bean&gt; &lt;/property&gt; &lt;!--设置序列化方式--&gt; &lt;property name=&quot;transcoder&quot;&gt; &lt;bean class=&quot;net.rubyeye.xmemcached.transcoders.SerializingTranscoder&quot;&gt;&lt;/bean&gt; &lt;/property&gt; &lt;!--设置一致性哈希--&gt; &lt;property name=&quot;sessionLocator&quot;&gt; &lt;bean class=&quot;net.rubyeye.xmemcached.impl.KetamaMemcachedSessionLocator&quot;&gt;&lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!--定义memcachedClient，通过memcachedClientBuilder的build方法--&gt; &lt;bean name=&quot;memcachedClient&quot; factory-bean=&quot;memcachedClientBuilder&quot; factory-method=&quot;build&quot; destroy-method=&quot;shutdown&quot;&gt; &lt;/bean&gt; java测试类： MemcachedClient memcachedClient = null; ApplicationContext applicationContext = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); try { memcachedClient = (MemcachedClient) applicationContext.getBean(&quot;memcachedClient&quot;); memcachedClient.set(&quot;spring&quot;, 0, &quot;3.0.0.RELEASE&quot;); logger.info(&quot;spring: {}&quot;, memcachedClient.get(&quot;spring&quot;)); memcachedClient.replace(&quot;spring&quot;, 0, &quot;4.0.0.RELEASE is coming&quot;); logger.info(&quot;spring, {}&quot;, memcachedClient.get(&quot;spring&quot;)); } catch (Exception e) { logger.info(&quot;spring test&quot;, e); } 参考 xmemcached user_guide_zh","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"http://nkcoder.github.io/tags/memcached/"}]},{"title":"Memcached用法--参数和命令详解","slug":"memcached-usage-parameters-commands","date":"2014-02-15T15:02:36.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/02/15/memcached-usage-parameters-commands/","link":"","permalink":"http://nkcoder.github.io/2014/02/15/memcached-usage-parameters-commands/","excerpt":"1. memcached 参数说明：# memcached -h 1.1 memcached 的参数常用参数","text":"1. memcached 参数说明：# memcached -h 1.1 memcached 的参数常用参数 -p &lt;num&gt; 监听的TCP端口号，默认是11211；（port） -l &lt;addr&gt; 监听的主机地址，默认是INADDR_ANY，即所有地址，&lt;addr&gt;可以是host:port的形式，如果没有指定port，则使用-p或者-U的值；可以指定多个地址，以逗号分隔或者多次使用-l参数；尽量不要使用默认值，有安全隐患。(listen) -d 以守护进程运行 (daemon) -u &lt;username&gt; 指定进程的所有者（只有以root用户执行时才可以使用该参数）(username) -m &lt;num&gt; 用于存储数据的最大内存，单位是MB，默认是64MB；(memory) -c &lt;num&gt; 最大并发连接数，默认是1024； -vv 显示更详细的信息（还显示客户端的命令和响应） -vvv 显示最详细的信息（还显示内部的状态转变） -h 显示帮助信息 -P &lt;file&gt; 将PID保存到&lt;file&gt;中，仅和-d参数一起使用； -f &lt;factor&gt; chunk的增幅因子，默认是1.25，不同的slab class，slab page大小相同，但是chunk大小不等，chunk的大小根据这个增幅因子增长；(factor) -n &lt;bytes&gt; 为key+value+flags分配的最小内存，单位bytes，默认是48；chunk数据结构本身要占据48字节，所以实际大小是n+48； -t &lt;num&gt; 使用多少个线程，默认是4；（thread） -I 设置slab page的大小，即设置可以保存的item的最大值，默认1MB，最小是1K，最大值128M； 其它参数 -U &lt;num&gt; 监听的UDP端口号，默认是11211，0表示关闭UDP监听；（UDP） -s &lt;file&gt; 要监听的UNIX socket路径（禁用网络支持）（socket) -a &lt;mask&gt; UNIX socket的访问掩码（access mask），八进制表示，默认是0700. (mask) -r 文件数量的最大值 (rlimit) -M 内存耗尽时返回错误，而不是通过LRU淘汰内容； -k 锁定所有页内存；允许被锁定的内存是有限制的，超过限制可能会失败。 -v 显示启动信息（错误和警告信息）(verbose) -i 显示memcached和libevent的licence信息 -L 一次申请大的内存页（如果可以）；增大内存页的大小，可以提高性能； -D &lt;char&gt; 指定key前缀与ID的分隔符，用于stats信息显示，默认是冒号：，如果使用了该参数，则stats收集自动启用了，否则，需要发送命令“stats detail on”命令来启动stats的收集。 -R 每一个事件（event）的最大请求数，限制最大请求数可以防止线程饥饿，默认是20； -C 禁用CAS； -b 设置backlog队列限制，默认1024； -B 指定绑定协议，ascii，binary或者auto，其中auto是默认值； 1.2 repcached的参数：-x &lt;ip_addr&gt; peer主机的主机名或者ip地址； -X peer主机的TCP端口，即主从同步端口，共同的监听端口 1.3 常用的参数组合# memcached -d -m -p 11212 -u nobody -l 127.0.0.1 -x 127.0.0.1 -X 11222 -P /tmp/localhost_slave.pid -vv 2. 基本命令与操作2.1 存储的命令主要有：set，add，replace，append，prepend，cas；格式为： command key flag expiration_time bytes value key表示键，flag表示key/value的额外信息，expiration_time表示过期时间，单位为秒，0表示永不过期，bytes表示值所占的字节数，必须完全匹配，value表示key对应的值，总是出现在第二行。 set命令表示存储一个key/value对，如果该key已存在，则更新对应的value值；如果成功，返回STORED。 set file_path 0 0 5 /opt/ &lt;29 rep file_path 0 0 5 12 REP&gt;29 STORED STORED add命令也表示增加key/value，如果key/value已存在，add操作失败；保存成功返回STORED，失败返回NOT_STORED。 add file_path 0 60 5 /opt/ NOT_STORED add file_suffix 0 0 2 js &lt;29 rep file_suffix 0 0 2 16 REP&gt;29 STORED STORED replace命令表示更新key对应的value值，如果key/value不存在，replace操作失败；成功返回STORED，失败返回NOT_STORED； replace first 0 0 7 tianjin STORED replace second 0 0 8 shanghai NOT_STORED append表示在key对应的value值后追加数据，key必须已存在，否则操作失败；成功返回STORED，失败返回NOT_STORED； append second 0 0 8 shanghai NOT_STORED append first 0 0 3 go STORED prepend在key对应的value值的前面追加数据，key必须已存在，否则操作失败；成功返回STORED，失败返回NOT_STORED； prepend second 0 0 2 hi NOT_STORED prepend first 0 0 2 hi STORED get first VALUE first 0 15 hitianjin go go END cas (check and set)：先比较后存储，即原子更新，原理类似于乐观所。每次请求存储某个数据时附带一个cas值，memcached比对这个cas值与当前存储数据的cas值是否相等，如果相等，则更新数据，否则操作失败；当前存储的cas值通过gets命令获取。成功返回STORED，失败返回EXISTS。 gets first VALUE first 0 7 12 // 12表示cas id，可以理解为版本号 chengdu END cas first 0 0 8 10 // 10 != 12，cas失败 shanghai EXISTS cas first 0 0 8 12 // 12表示gets后没有修改key的值，因此可以set shanghai STORED 2.2 读取的命令 get根据key获取value值；可以获取多个key的值；get key | get key1 key2 get first VALUE first 0 8 shanghai END get first fine VALUE first 0 8 shanghai VALUE fine 0 5 yes!! END gets是与cas一起使用的命令，gets会额外返回一个cas值，可以理解为版本；如果最后一次gets后，该cas值改变了，则cas设置的值不会存储；gets key | gets key1 key2 gets first VALUE first 0 8 13 shanghai END set first 0 0 7 chengdu STORED cas first 0 0 7 13 // 因为gets后set了，所以cas id改变了，cas失败 chengdu EXISTS delete命令删除key/value对，一次只能删除一个key/value对；如果要删除的key不存在，操作失败: delete key delete first second CLIENT_ERROR bad command line format. Usage: delete &lt;key&gt; [noreply] delete first DELETED incr/decr: 如果key的value值表示的是一个64位整数，可以通过incr和decr命令进行数值的增减: incr/decr key num set id 0 120 2 10 STORED incr id 10 20 decr id 5 15 2.3 统计的命令 stats显示进程及当前状态等信息。 stats STAT pid 1224 // 进程id STAT uptime 30385 // 系统运行的事件，单位：秒 STAT time 1392199633 // 系统当前事件，Unix时间戳表示的时间：2/12/2014 6:25:40 PM STAT version 1.4.13 // memcached版本 STAT libevent 2.0.21-stable // libevent版本 STAT pointer_size 64 // 操作系统字大小（64位） STAT rusage_user 1.892712 // 进程累计用户时间 STAT rusage_system 0.996848 // 进程累计系统时间 STAT curr_connections 8 // 当前打开的连接数 STAT total_connections 9 // 曾打开的连接总数 STAT connection_structures 9 // 服务器分配的连接结构数 STAT reserved_fds 20 STAT cmd_get 29 // 执行get命令的总数 STAT cmd_set 29 // 执行set命令的总数 STAT cmd_flush 2 // 执行flush_all命令的总数 STAT cmd_touch 0 STAT get_hits 14 STAT get_misses 15 STAT delete_misses 3 STAT delete_hits 3 STAT incr_misses 0 STAT incr_hits 4 STAT decr_misses 0 STAT decr_hits 2 STAT cas_misses 1 STAT cas_hits 2 STAT cas_badval 2 STAT touch_hits 0 STAT touch_misses 0 STAT auth_cmds 0 STAT auth_errors 0 STAT bytes_read 1503 STAT bytes_written 4125 STAT limit_maxbytes 134217728 STAT accepting_conns 1 STAT listen_disabled_num 0 STAT threads 4 // 线程数 STAT conn_yields 0 STAT hash_power_level 16 STAT hash_bytes 524288 STAT hash_is_expanding 0 STAT expired_unfetched 0 STAT evicted_unfetched 0 STAT replication MASTER STAT repcached_version 2.3.1 STAT repcached_qi_free 8191 STAT bytes 217 // 存储的item字节数 STAT curr_items 3 // 当前item数量 STAT total_items 25 // item的总数 STAT evictions 0 // 为获取空间删除的item数量 STAT reclaimed 2 END stats items 显示items的相关信息 stats items STAT items:1:number 3 STAT items:1:age 1552 STAT items:1:evicted 0 STAT items:1:evicted_nonzero 0 STAT items:1:evicted_time 0 STAT items:1:outofmemory 0 STAT items:1:tailrepairs 0 STAT items:1:reclaimed 2 STAT items:1:expired_unfetched 0 STAT items:1:evicted_unfetched 0 END stats slabs 显示slab的相关信息 stats slabs STAT 1:chunk_size 96 STAT 1:chunks_per_page 10922 STAT 1:total_pages 1 STAT 1:total_chunks 10922 STAT 1:used_chunks 3 STAT 1:free_chunks 0 STAT 1:free_chunks_end 10919 STAT 1:mem_requested 217 STAT 1:get_hits 14 STAT 1:cmd_set 29 STAT 1:delete_hits 3 STAT 1:incr_hits 4 STAT 1:decr_hits 2 STAT 1:cas_hits 2 STAT 1:cas_badval 2 STAT 1:touch_hits 0 STAT active_slabs 1 STAT total_malloced 1048512 END stats sizes stats sizes STAT 96 3 END stats cachedump 显示slab中items的信息(该命令在将来可能不再支持）：stats cachedump [slab id] [number of items, 0 for all items] set city 0 0 7 tianjin STORED stats cachedump 1 0 ITEM city [7 b; 1392169248 s] END get city VALUE city 0 7 tianjin END flush_all 使cache中的所有items都过期，server不会停止，也不会刷新或者释放内存。 flush_all OK get first END 参考 memcached wiki memcached命令行操作 memcached命令行参数说明 memcached详解","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"http://nkcoder.github.io/tags/memcached/"}]},{"title":"Memcached安装","slug":"memcached-install-libevent-repcached-memcached","date":"2014-02-12T23:58:41.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/02/13/memcached-install-libevent-repcached-memcached/","link":"","permalink":"http://nkcoder.github.io/2014/02/13/memcached-install-libevent-repcached-memcached/","excerpt":"安装顺序： 1. 先安装libevent，因为memcached依赖它； 2. 给memcached打上repcached补丁，支持主从结构，实现高可用； 3. 安装memcached，启用replication；","text":"安装顺序： 1. 先安装libevent，因为memcached依赖它； 2. 给memcached打上repcached补丁，支持主从结构，实现高可用； 3. 安装memcached，启用replication； 1. libevent# wget https://github.com/downloads/libevent/libevent/libevent-2.0.21-stable.tar.gz -P /opt/softs/ # ./configure --prefix=/usr/local/ # make &amp;&amp; make install 2. repcached &amp;&amp; memcached当前补丁最新位1.4.13，memcached的版本应该一直，否则会出现异常； # wget https://memcached.googlecode.com/files/memcached-1.4.13.tar.gz -P /opt/softs/ # wget http://mdounin.ru/files/repcached-2.3.1-1.4.13.patch.gz -P ./ # tar zxvf memcached-1.4.13.tar.gz -C /opt/softs/ # gzip -d repcached-2.3.1-1.4.13.patch.gz # cd /opt/softs/memcached-1.4.13 # patch -p1 -i ../repcached-2.3.1-1.4.13.patch # ./configure --prefix=/usr/local/ --with-libevent=/usr/local/ --enable-replication # make &amp;&amp; make install 3. memcached (with repcached) 主从测试在本机的两个不同的端口上启动两个memcached实例，一主一从，然后测试相互之间的数据同步。memcached主从测试在本机的两个不同的端口上启动两个memcached实例，一主一从，然后测试相互之间的数据同步。 3.1 启动两个memcached实例# memcached -d -m -p 11211 -u nobody -l 127.0.0.1 -x 127.0.0.1 -X 11222 -P /tmp/localhost_master.pid -vv # memcached -d -m -p 11212 -u nobody -l 127.0.0.1 -x 127.0.0.1 -X 11222 -P /tmp/localhost_slave.pid -vv 两个实例的端口分别为11211和11212，共同的监听端口为11222。 3.2 通过telnet测试数据的同步 进入master，set两个值 [root@localhost ~]# telnet 127.0.0.1 11211 Trying 127.0.0.1... &lt;31 new auto-negotiating client connection Connected to 127.0.0.1. Escape character is &apos;^]&apos;. set city 0 0 7 31: Client using the ascii protocol &lt;31 set city 0 0 7 tianjin &gt;31 STORED replication: pop replication: pop STORED set city2 0 0 7 &lt;31 set city2 0 0 7 beijing &gt;31 STORED replication: pop replication: pop STORED 进入slave，get刚才设置的值，并删除其中一个值 [root@localhost ~]# telnet 127.0.0.1 11212 Trying 127.0.0.1... Connected to 127.0.0.1. Escape character is &apos;^]&apos;. get city VALUE city 0 7 tianjin END get city2 VALUE city2 0 7 beijing END delete city2 &lt;26 delete city2 REP&gt;26 DELETED DELETED 再次进入master，获取set的两个值，删除的数据是否可以get [root@localhost ~]# telnet 127.0.0.1 11211 Trying 127.0.0.1... &lt;31 new auto-negotiating client connection Connected to 127.0.0.1. Escape character is &apos;^]&apos;. get city 31: Client using the ascii protocol &lt;31 get city &gt;31 sending key city &gt;31 END VALUE city 0 7 tianjin END get city2 &lt;31 get city2 &gt;31 END END","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"http://nkcoder.github.io/tags/memcached/"}]},{"title":"《SQL COOKBOOK》阅读笔记","slug":"sql-cookbook-reading-note","date":"2014-01-19T11:38:59.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/01/19/sql-cookbook-reading-note/","link":"","permalink":"http://nkcoder.github.io/2014/01/19/sql-cookbook-reading-note/","excerpt":"1. 尽量不要使用select *返回表中的所有field时，使用select *确实很方便，但是不推荐这么做，应该在select后将所有的field都列出来：一是方便自己，清楚地知道返回了哪些field；而是方便他人，在看你的代码时，别人不一定知道表中有哪些列。这两种方式的性能是一样的。","text":"1. 尽量不要使用select *返回表中的所有field时，使用select *确实很方便，但是不推荐这么做，应该在select后将所有的field都列出来：一是方便自己，清楚地知道返回了哪些field；而是方便他人，在看你的代码时，别人不一定知道表中有哪些列。这两种方式的性能是一样的。 2. 尽量使用as创建易读的别名2.1 尽量使用易读的别名mysql&gt; select sal as salary, empName as employName from employ; 使用易读的别名，可以让你的sql语句更容易被理解。 2.2 在where中使用别名：mysql&gt; select sal as salary from employ where salary &gt;= 10000; 该语句报错，因为salary字段未识别，可以将该语句包装成一个临时表： mysql&gt; select * from (select sal as salary from employ) as t where salary &gt;= 10000; 为什么：因为where语句在select之前执行，所以在第一条语句中，salary还没有创建；而from语句在where语句之前执行，所以在第二条语句中，临时表t中有字段salary。 3. 使用concat函数进行字段值的连接将字段值和字符串拼接起来，使用concat()函数： mysql&gt; select concat(startIp, &quot; : &quot;, endIp) as ipSeg from ip_data limit 1; +---------------------+ | ipSeg | +---------------------+ | 17563648 : 17825791 | +---------------------+ 4. 使用case when进行条件判断在select语句中，可以对返回结果进行逻辑处理。使用case表达式，语法格式如下。else可以省略，如果所有的when都不匹配，默认返回null； case when ... then when ... then else ... end 注意，还有一个case表达式，用于存储过程，以end case结尾。 mysql&gt; select netId, areaId, case when netId &gt; 10 then &apos;big&apos; when netId &lt; 5 then &apos;small&apos; else &apos;normal&apos; end as status from ip_data limit 5; +-------+--------+--------+ | netId | areaId | status | +-------+--------+--------+ | 11 | 4019 | big | | 2 | 2813 | small | | 10 | 35 | normal | | 1 | 2110 | small | | 10 | 35 | normal | +-------+--------+--------+ 5. 使用rand返回随机的记录可以使用rand()函数返回一个随机数，通过order by排序即可。rand()对每一条记录返回0.0到1.0之间的浮点数。当然也可以在order by之后跟一个常量浮点数，但此时结果是固定的。 mysql&gt; select ip_data.netId, ip_data.areaId from ip_data order by rand() limit 5; +-------+--------+ | netId | areaId | +-------+--------+ | 7 | 4019 | | 10 | 1806 | | 2 | 4608 | | 13 | 42 | | 3 | 4502 | +-------+--------+ 6. 使用is null判断字段是否为null判断字段null值，不能使用=或者!=，即使null和null也不能通过=比较。应该使用 is null, 或者 is not null。 mysql&gt; select startIp, areaId from ip_data where netId is null; 7. 使用coalesce函数对字段进行默认处理对字段的返回值进行合并，使用函数coalesce(field, default)：如果field不为null，取field的值，如果field值为null，取设置的默认值default。 mysql&gt; select number, coalesce(number, 1000) as new_number from students; +--------+------------+ | number | new_number | +--------+------------+ | 111 | 111 | | NULL | 1000 | | 333 | 333 | | 12345 | 12345 | | NULL | 1000 | | 777 | 777 | | NULL | 1000 | +--------+------------+ 当然，在select里使用case when end也能达到同样的效果，但显然使用coalesce()更简洁。 8. 在order by之后通过数字指定字段进行排序order by一般后跟字段名，表示根据字段名排序；也可以跟一个整数，该整数的值与select中的字段相对应，从1开始，表示根据select中的第几个字段排序，因此，该整数不能大于select中的字段个数。如下两种情况是等价的： mysql&gt; select id, startIp, endIp, netId from ip_data order by id asc limit 5; mysql&gt; select id, startIp, endIp, netId from ip_data order by 1 asc limit 5; +----+----------+----------+-------+ | id | startIp | endIp | netId | +----+----------+----------+-------+ | 1 | 17563648 | 17825791 | 11 | | 2 | 18350080 | 18874367 | 2 | | 3 | 19726336 | 19791871 | 10 | | 4 | 19922944 | 20054015 | 1 | | 5 | 20054016 | 20119551 | 10 | +----+----------+----------+-------+ 另外，可以根据多个字段排序，优先级从左到右，即先根据第一个字段排序，如果第一个字段的值相等，然后根据第二个字段的值排序，依此类推： &gt;&gt; select id, startIp, endIp, netId from ip_data order by netId asc, endIp desc limit 5; +-------+------------+------------+-------+ | id | startIp | endIp | netId | +-------+------------+------------+-------+ | 17351 | 3757047808 | 3757572095 | 1 | | 17348 | 3755737088 | 3755868159 | 1 | | 17347 | 3755343872 | 3755474943 | 1 | | 17341 | 3754295296 | 3754426367 | 1 | | 17337 | 3750756352 | 3751804927 | 1 | +-------+------------+------------+-------+ 9. 通过substring函数对字段子串排序使用函数substring()可以获取字符串的子串，使用order by可以根据该子串进行排序。注意：substring(str, pos)表示从pos开始的子串，其中位移从1开始，所以如果要表示取字符串的最后两个字符，应该是substring(str, length(str)-1). mysql&gt; select file_path, file_name from file order by substr(file_name, length(file_name)-1) desc limit 5; +----------------+-----------------------+ | file_path | file_name | +----------------+-----------------------+ | /opt/rankFile/ | phb_variety_day_issue | | /opt/rankFile/ | catalog_variety | | /opt/rankFile/ | catalog_tv | | /opt/rankFile/ | catalog_cartoon | | /opt/rankFile/ | catalog_doc | +----------------+-----------------------+ 10. 如何对含有null值的字段进行排序mysql中字段的null值在排序时默认时按照最小值排序的，即升序时，null值在最前面，降序时在最后面。通过在select中使用case when对null值进行判断，然后定义排序顺序。如升序时将null值放在最后： mysql&gt; select name, number from (select name, number, case when number is null then 0 else 1 end as is_null from students) as t order by is_null desc, number; +----------+--------+ | name | number | +----------+--------+ | lingguo | 111 | | lisi | 333 | | guanyu | 777 | | wangwu | 12345 | | zhangsan | NULL | | zhangfei | NULL | | lingguo | NULL | +----------+--------+ 11. 字段值不同，排序的字段不同，如何实现？case when语句也可以在order by中，不同的条件，使用不同的字段进行排序。比如：如果number字段的值不为null，根据number排序，否则，根据enable排序： mysql&gt; select name, number, enable from students order by case when number is null then enable else number end; +----------+--------+--------+ | name | number | enable | +----------+--------+--------+ | zhangsan | NULL | 0 | | zhangfei | NULL | 0 | | lingguo | NULL | 1 | | lingguo | 111 | 1 | | lisi | 333 | 1 | | guanyu | 777 | 0 | | wangwu | 12345 | 0 | +----------+--------+--------+ 参考： SQL Cookbook","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://nkcoder.github.io/tags/mysql/"}]},{"title":"使用汇总表优化查询一则","slug":"mysql-optimize-in-project","date":"2014-01-07T13:38:18.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/01/07/mysql-optimize-in-project/","link":"","permalink":"http://nkcoder.github.io/2014/01/07/mysql-optimize-in-project/","excerpt":"1. 提出问题根据视频的播放数实现排行榜。需要关联4张表：专辑表、视频表、专辑和视频的关联表以及视频的播放数日增表，日增表的日数据大概在2万条左右。出排行榜的sql如下： SELECT vts.TV_ALUMN_ID AS albumId, inc.tv_vid, inc.tv_id, vts.TV_NAME AS tvName, sum( inc.tv_count + inc.tv_count_app) AS realCount, sum( inc.tv_display_count + inc.tv_display_count_app) AS showCount FROM vrs2_playlist vp, vrs2_tv_playlist vtp, vrs_tv_set vts, t_vrs_video_day_inc inc WHERE vp.ID = vtp.TV_PLAYLIST_ID AND vtp.TV_ID = inc.tv_id AND vts.ID = vtp.TV_ID AND vts.TV_ALUMN_ID = vp.id AND inc.tv_date BETWEEN ? AND ? AND vp.IS_BD != 0 AND vp.TV_IS_INTREST != 1 AND vp.TV_EFFECTIVE = 1 AND vp.TV_IS_TEST = 0 AND vts.IS_BD != 0 AND ( vts.TV_S_TYPE &lt; 2 OR vts.TV_S_TYPE IS NULL) AND vp.TV_CATEGORY_ID = ? GROUP BY inc.tv_vid ORDER BY showCount DESC LIMIT 100","text":"1. 提出问题根据视频的播放数实现排行榜。需要关联4张表：专辑表、视频表、专辑和视频的关联表以及视频的播放数日增表，日增表的日数据大概在2万条左右。出排行榜的sql如下： SELECT vts.TV_ALUMN_ID AS albumId, inc.tv_vid, inc.tv_id, vts.TV_NAME AS tvName, sum( inc.tv_count + inc.tv_count_app) AS realCount, sum( inc.tv_display_count + inc.tv_display_count_app) AS showCount FROM vrs2_playlist vp, vrs2_tv_playlist vtp, vrs_tv_set vts, t_vrs_video_day_inc inc WHERE vp.ID = vtp.TV_PLAYLIST_ID AND vtp.TV_ID = inc.tv_id AND vts.ID = vtp.TV_ID AND vts.TV_ALUMN_ID = vp.id AND inc.tv_date BETWEEN ? AND ? AND vp.IS_BD != 0 AND vp.TV_IS_INTREST != 1 AND vp.TV_EFFECTIVE = 1 AND vp.TV_IS_TEST = 0 AND vts.IS_BD != 0 AND ( vts.TV_S_TYPE &lt; 2 OR vts.TV_S_TYPE IS NULL) AND vp.TV_CATEGORY_ID = ? GROUP BY inc.tv_vid ORDER BY showCount DESC LIMIT 100 如果是日榜，查询时间大约在8s左右，还可以接受。如果是周榜，大概需要1min，如果是月榜，耗时约2~3分钟。周榜和月榜的时间是令人难以接受的。 2. 优化方案我采取的方案是：在日增数据的基础上，进行一周和一月的数据的汇总，生成周增表和月增表，周榜和月榜直接从周增表和月增表里查询。 2.1 通过insert into select汇总数据创建表结构之后（周增表和月增表的表结构和日增表一致），通过insert into TABLE select 语句可以将查询的结果写入指定的表中，完整的SQL语句为： INSERT INTO t_vrs_video_week_inc (tv_id,tv_vid,tv_count,tv_display_count,tv_count_app,tv_display_count_app,tv_date) SELECT tv_id, tv_vid, sum(tv_count) as tv_count, sum(tv_display_count) as tv_display_count, sum(tv_count_app) as tv_count_app, sum(tv_display_count_app) as tv_display_count_app, SUBDATE(CURDATE(),INTERVAL WEEKDAY(CURDATE())+7 DAY) AS tv_date FROM t_vrs_video_day_inc inc WHERE inc.tv_date BETWEEN SUBDATE(CURDATE(),INTERVAL WEEKDAY(CURDATE())+7 DAY) AND SUBDATE(CURDATE(),INTERVAL WEEKDAY(CURDATE())+1 DAY) GROUP BY inc.tv_vid 但是，如果数据量稍大，这个操作会失败，MySql提示binlog太小了。binlog用来记录数据库发生变化的“事件”，如创建表或修改表中的数据，我们的INSERT INTO语句就会使用binlog，但是select语句和show语句因为仅仅是查询，不会使用binlog。所以得想别的办法。 2.2 通过load data从文件导数据因为select语句不会受binlog的限制，也就是说，数据是可以查出来的，只是没法一下子写入表中。我们可以先将查出来的数据写入文件，然后通过MySql的load data local infile命令从文件中将数据导入表中。查询数据存入文件： selectSql=&quot;SELECT tv_id, tv_vid, sum(tv_count) as tv_count, sum(tv_display_count) as tv_display_count, sum(tv_count_app) as tv_count_app, sum(tv_display_count_app) as tv_display_count_app, SUBDATE(CURDATE(),INTERVAL WEEKDAY(CURDATE())+7 DAY) AS tv_date FROM t_vrs_video_day_inc inc WHERE inc.tv_date BETWEEN SUBDATE(CURDATE(),INTERVAL WEEKDAY(CURDATE())+7 DAY) AND SUBDATE(CURDATE(),INTERVAL WEEKDAY(CURDATE())+1 DAY) GROUP BY inc.tv_vid&quot; mysql -h10.11.12.13 -uuser -ppasswd -Dvideo -s --skip-column-names -e &quot;$selectSql&quot; &gt; $dataFilePath 从文件将数据load进表中： loadDataSql=&quot;load data local infile &apos;$dataFilePath&apos; into table t_vrs_video_week_inc (tv_id,tv_vid,tv_count,tv_display_count, tv_count_app,tv_display_count_app,tv_date)&quot; mysql -h10.11.12.13 -uuser -ppasswd -Dvideo -s --skip-column-names -e &quot;$loadDataSql&quot; 3. 总结优化之后，周榜和月榜的耗时大概在10s左右，与日榜差不多。整个系统的执行时间从30min左右降到10分钟左右，应该说效果还是比较明显的。我的方案只是一种形式的优化，肯定不是最优的，比如多表联查的效率很低，这应该也是个优化点，还会继续探索。 参考 5.2.4. The Binary Log","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://nkcoder.github.io/tags/mysql/"}]},{"title":"使用Github-pages和Octopress搭建静态博客及配置","slug":"github-pages-and-octopress-blog-built","date":"2014-01-05T14:07:11.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/01/05/github-pages-and-octopress-blog-built/","link":"","permalink":"http://nkcoder.github.io/2014/01/05/github-pages-and-octopress-blog-built/","excerpt":"初衷：其实，前不久才购买的域名和主机，使用wordpress搭建的博客，但是wordpress功能完善地有些笨拙，再加上购买的主机速度慢地难以忍受，连写博客的欲望都没有了。后来发现github pages和Octopress可以组合搭建静态博客，托管在github上，既稳定又方便，我只要关注博客就行了。所以这两天都在折腾。","text":"初衷：其实，前不久才购买的域名和主机，使用wordpress搭建的博客，但是wordpress功能完善地有些笨拙，再加上购买的主机速度慢地难以忍受，连写博客的欲望都没有了。后来发现github pages和Octopress可以组合搭建静态博客，托管在github上，既稳定又方便，我只要关注博客就行了。所以这两天都在折腾。 1. 环境准备1.1 下载软件：RubyInstaller，Devkit，git先在下载地址rubyinstaller看说明，当前建议下载1.9.3稳定版RubyInstaller：Ruby 1.9.3-p484和DevKit-tdm-32-4.5.2-20111229-1559-sfx.exe。（我因为下载的时2.0版，后来配置的时候报错了，版本改为1.9.3就ok了）git可以下载最新版的msysgit即可。 1.2 安装ruby安装RubyInstaller，注意不要安装在带有空格的目录中，如C:\\Program Files，否则后面有些命令可能会出错；安装时选择将可执行路径加入到PATH环境变量中。Devkit解压即可，然后进入devkit目录，执行安装命令： $ cd Devkit $ ruby dk.rb init $ ruby dk.rb install 1.3 安装python和pygmentspython2.7: http://www.python.org/ftp/python/2.7.6/python-2.7.6.msi pygments的安装：先下载ez_setup.py脚本：https://pypi.python.org/pypi/setuptools#windows，然后执行： $ python ez_setup.py 会下载setuptools-2.2.tar.gz，解压并进入： $ cd dist\\setuptools-2.2 $ python easy_install.py pygments 1.4 安装Octopress默认是官方的源，在国内比较慢，建议换成国内淘宝的ruby源： $ gem sources -a http://ruby.taobao.org $ gem sources -r http://rubygems.org $ gem sources -l 安装Octopress到指定的目录： git clone git://github.com/imathis/octopress.git /d/yousharp 打开安装目录/d/yousharp的Gemfile文件，将第一行的source改为淘宝的源： source &quot;http://ruby.taobao.org&quot; 1.5 安装bundler$ gem install bundler $ bundle install 1.6 安装主题，生成博客并预览Octopress的主题列表，我使用的是：Bootstrap theme： $ git clone git://github.com/bkutil/bootstrap-theme.git bootstrap-theme $ cp -R bootstrap-theme $MY_OCTOBLOG/.themes/bootstrap $ rake install[&apos;bootstrap&apos;] $ rake generate $ rake preview 访问http://localhost:4000即可预览。 1.7 修改环境变量支持中文新建环境变量：LANG 和 LC_ALL，值均为zh_CN.UTF_8 2. 托管到github pages2.1 新建github repo在github新建一个repo，命名为USERNAME.github.io，使用你github的用户名取代USERNAME，如：nkcoder.github.io。 2.2 github配置$ git config --global user.name &quot;nkcoder&quot; $ git config --global user.email &quot;daniel5hbs@gmail.com&quot; $ git config --global credential.helper cache $ git config --global credential.helper &apos;cache --timeout=3600&apos; 注意：credential helper 只对https有效，对ssh无效。 生成rsa key，并添加到github的SSH keys中： $ ssh-keygen -t rsa -C &quot;daniel5hbs&quot; $ ssh -T git@github.com 2.3 博客发布$ rake setup_github_pages $ rake generate $ rake deploy 访问nkcoder.github.io。 3. 新建博客或页面$ rake new_post[&apos;first-blog&apos;] $ rake new_page[&apos;about&apos;] $ rake generate $ rake preview 说明：使用$ rake preview可以监视文件的变动，可以实时预览修改，但是如果修改了_config.yml或者该配置文件引用的文件发生了变化，则需要重新$ rake generate。 4. 主题优化4.1 添加disqus评论先去disqus注册一个帐号，然后修改根目录/d/yousharp下的_config.yml文件，修改为： # Disqus Comments disqus_short_name: nkcoder disqus_show_comment_count: yes 注意：将nkcoder替换为你的disqus帐号。 4.2 代码高亮首先下载安装python 2.x版本，然后修改_config.yml，修改为： pygments: true 注意冒号后有个空格。 使用三个反引号（左Alt键右边的键)： 1your code 4.3. 在页面右侧添加分类等模块系统自带的模块放在了目录：source/_include/asides下，我们自定义的模块放在目录：source/_include/custom/asides下。模块一：分类及标签使用插件octopress-category-list下载下来后，将category_list.rb放在plugins目录下，将category_list.html和category_cloud.html放在source/_include/custom/asides目录下； 模块二：最新博文这是系统默认的模块，在source/_include/asides目录下的recent_posts.html，样式可以自己修改； 模块三：博客链接在source/_include/custom/asides目录下新建blogroll.html，比如内容可以为： &lt;section&gt; &lt;h4&gt; Blogroll &lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://ifeve.com/&quot; target=&quot;_blank&quot;&gt;并发编程网&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://tengine.taobao.org/book/index.html#&quot; target=&quot;_blank&quot;&gt;Nginx开发从入门到精通&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://timyang.net/&quot; target=&quot;_blank&quot;&gt;Tim Yang&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/section&gt; 模块四：豆瓣阅读列表新建douban.html，进入豆瓣收藏秀生成js代码，将内容写入douban.html中，如： &lt;section&gt; &lt;h4&gt; Want To Read&lt;/h4&gt; &lt;div&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://www.douban.com/service/badge/daniel5hbs/?selection=random&amp;amp;picsize=small&amp;amp;hideself=on&amp;amp;show=wishlist&amp;amp;n=9&amp;amp;cat=book&amp;amp;columns=3&quot;&gt;&lt;/script&gt; &lt;/div&gt; &lt;/section&gt; 模块五：disqus最新评论新建模块文件：recent_comments.html，写入disqus的js代码，如： &lt;section&gt; &lt;h4&gt; Recent Comments &lt;/h4&gt; &lt;div id=&quot;dsq-recentcomments&quot; class=&quot;dsq-widget&quot;&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;http://disqus.com/forums/{{ site.disqus_short_name }}/recent_comments_widget.js?hide_avatars=0&quot;&gt;&lt;/script&gt;&lt;/div&gt; &lt;/section&gt; 然后修改_config.yml文件，将需要在侧边显示的模块包含在default_asides里，如： default_asides: [custom/asides/category_list.html, asides/recent_posts.html, custom/asides/blogroll.html, custom/asides/douban.html, custom/asides/recent-comment.html] 根据包含的顺序，自上向下地展示。 4.4 显示摘要在文章中的某一行添加如下代码即可： &lt;!--more--&gt; 页面显示“Read on-&gt;”， 剩下的内容会隐藏。 5. 不同环境下博客的同步如果电脑中的文件丢失了，或者换电脑了，或者需要在另一个环境下写博客，则需要将博客拷贝到本地，并配置环境。 5.1 分支的概念Octopress默认有两个分支，一个是source，包含生成博客的所有文件，另一个是master，即博客本身。当我们本地配置完后，master分支在目录_deploy中，因为以下划线开头，当我们向source分支提交时$ git push -u origin source时，master分支被忽略，而当我们使用rake部署时$ rake deploy，master分支则更新。 5.2 将博客拷贝到本地的步骤如果当前没有python和ruby的环境，则需要执行本博客1.1到1.3中的步骤，配置环境； 首先将source分支拷贝到本地的博客目录： $ git clone -b source git@github.com:nkcoder/nkcoder.github.io.git yousharp 将其中的nkcoder替换为你自己的用户名，将yousharp替换为你的博客目录。 然后，将master分支拷贝到_deploy目录(github路径是相同的)： $ cd mybolg $ git clone git@github.com:nkcoder/nkcoder.github.io.git _deploy 如果是新环境（比如新的电脑），则需要配置环境，安装Ruby21-x64/DevKit-mingw64：(注意：将source修改为国内淘宝的) $ gem install bundler $ bundle install $ rake setup_github_pages 可能需要输入github repo的地址， Enter the read/write url for your repository (For example, &apos;git@github.com:nkcoder/nkcoder.github.io.git) 5.3 不同环境的同步在切换环境之前，确保对所做的修改都提交了： $ ranke generate $ git add . $ git commit -am &apos;your comment&apos; $ git push origin source $ rake deploy 切换环境之后，将所有的更新拷贝下来： $ cd myblog $ git pull origin source $ cd ./_deploy $ git pull origin master ———————————————-2014-12-16补充—————————————– 在新环境下配置blog时，不要忘了先安装ruby/devkit/bundler；然后，如果执行rake generate报类似如下的错误： $ rake generate ## Generating Site with Jekyll unchanged sass/screen.scss Configuration from c:/Sites/my_project/_config.yml Building site: source -&gt; public c:/Ruby193/lib/ruby/gems/1.9.1/gems/jekyll-0.11.0/lib/jekyll/convertible.rb:29:i n `read_yaml&apos;: invalid byte sequence in GBK (ArgumentError) from c:/Ruby193/lib/ruby/gems/1.9.1/gems/jekyll-0.11.0/lib/jekyll/post.r b:39:in `initialize&apos; from c:/Sites/my_project/plugins/preview_unpublished.rb:23:in `new&apos; from c:/Sites/my_project/plugins/preview_unpublished.rb:23:in `block in read_posts&apos; from c:/Sites/my_project/plugins/preview_unpublished.rb:21:in `each&apos; from c:/Sites/my_project/plugins/preview_unpublished.rb:21:in `read_ posts&apos; from c:/Ruby193/lib/ruby/gems/1.9.1/gems/jekyll-0.11.0/lib/jekyll/site.r b:128:in `read_directories&apos; from c:/Ruby193/lib/ruby/gems/1.9.1/gems/jekyll-0.11.0/lib/jekyll/site.r b:98:in `read&apos; from c:/Ruby193/lib/ruby/gems/1.9.1/gems/jekyll-0.11.0/lib/jekyll/site.r b:38:in `process&apos; from c:/Ruby193/lib/ruby/gems/1.9.1/gems/jekyll-0.11.0/bin/jekyll:250:in `&lt;top (required)&gt;&apos; from c:/Ruby193/bin/jekyll:19:in `load&apos; from c:/Ruby193/bin/jekyll:19:in `&lt;main&gt;&apos; 说明encoding有问题，在需要设置LC_ALL和LANG两个环境变量，Linux下为： $ set LC_ALL=en_US.UTF-8 $ set LANG=en_US.UTF-8 windows下在*Computer-&gt;Properties-&gt;Advanced system settings新建两个环境变量LC_ALL和LANG，值都为en_US.UTF-8即可。 参考博文：（非常感谢） Clone Your Octopress to Blog From Two Places Recent Comments in Octopress 用Github和Octopress搭建博客 使用github + Octopress 搭建免费博客","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"octopress","slug":"octopress","permalink":"http://nkcoder.github.io/tags/octopress/"},{"name":"github-page","slug":"github-page","permalink":"http://nkcoder.github.io/tags/github-page/"}]},{"title":"Vim常用的基本操作","slug":"vim-basic-usage","date":"2014-01-05T06:52:26.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/01/05/vim-basic-usage/","link":"","permalink":"http://nkcoder.github.io/2014/01/05/vim-basic-usage/","excerpt":"1. vim的三种模式 vim有三种模式，不同的模式对应着不同的操作，完成不同的功能；这三种模式分别是：一般模式，编辑模式和命令模式。 一般模式：当我们使用vim打开文件时，就处于一般模式，一般模式是三种模式中的桥梁，也就是说可以从一般模式切换到命令模式或者编辑模式，但命令模式和编辑模式之间的切换必须经过一般模式，即先切换到一般模式，然后再进入另一种模式，编辑模式和命令模式之间是无法直接切换的。如何从另两种模式进入一般模式呢？按ESC即可。 编辑模式：可以对文件的内容进行编辑操作，比如增加、删除和修改，就像使用notepad编辑文件一样；从一般模式进入编辑模式的命令主要有：i, I, a, A, o, O, s, S;（后面会详解）。 命令模式：该模式下主要是用于文件的保存退出，从一般模式进入命令模式的方法是：冒号(:)。","text":"1. vim的三种模式 vim有三种模式，不同的模式对应着不同的操作，完成不同的功能；这三种模式分别是：一般模式，编辑模式和命令模式。 一般模式：当我们使用vim打开文件时，就处于一般模式，一般模式是三种模式中的桥梁，也就是说可以从一般模式切换到命令模式或者编辑模式，但命令模式和编辑模式之间的切换必须经过一般模式，即先切换到一般模式，然后再进入另一种模式，编辑模式和命令模式之间是无法直接切换的。如何从另两种模式进入一般模式呢？按ESC即可。 编辑模式：可以对文件的内容进行编辑操作，比如增加、删除和修改，就像使用notepad编辑文件一样；从一般模式进入编辑模式的命令主要有：i, I, a, A, o, O, s, S;（后面会详解）。 命令模式：该模式下主要是用于文件的保存退出，从一般模式进入命令模式的方法是：冒号(:)。 2. 一般模式下的常用操作：2.1 光标移动h, j, k, l: 向左、下、上、右移动一个字符；前面加上数字n，表示移动n个字符，如3j表示向下移动三个字符，即三行； b, B: 表示begin，从字符所在的单词开始，b表示移动到前一个短单词的第一个字符处；B表示移动到前一个长单词的第一个字符处，两个空白之间的单词为一个长单词，比如hello how&apos;re you, how,&apos;和re分别为三个短单词，但只是一个长单词； e, E: 与b, B类似，从字符所在的单词开始，分别向后移动一个短单词、长单词； (, ): 从光标所在的语句块开始，(表示移动到前一个语句块，)表示移动到后一个语句块； {, }: 从光标所在的段落开始，{表示向前移动一个段落，}表示向后移动一个段落；可以理解为从一个空行移动到前一个或者后一个空行； %: 跳转到匹配的(, ), {, }, [, ] CTRL+f: 屏幕向下翻页，相当于PgUp; CTRL+b: 屏幕向上翻页，相当于PgDn； 0: 移动到光标所在行的第一个字符处； $: 移动到光标所在行的最后一个字符处； ^: 移动到光标所在行的第一个非空字符处； gg: 移动到文件的第一行（且是第一个字符处）； G: 移动到文件的最后一行（且是第一个字符处）； 数字n+G: 表示移动到第n行，如5G，表示移动到文件的第5行； 说明：以上这些命令非常常用，熟记的同时要熟练； 数字n+空格space: 表示向右移动n个字符，即n+space，等价于nl； 数字n+回车Enter: 表示向下移动n行，等价于nj； 2.2 查找替换/word: 从光标所在处向下搜索关键字word； ?word: 从光标所在处向上搜索关键字word； n: 向下搜索，查找下一个匹配项； N: 向上搜索，查找上一个匹配项； *：对光标所在的单词，向下查找； #: 对光标所在的单词，向上查找； 说明：一般情况下，/word和n、N配合完成查找，而?word用得较少； :n1,n2s/src/dest/g: 在n1行到n2行之间（包括n1行和n2行），查找src，并将所有的src替换成dest； :1,$s/src/dest/g: 查查找范围为第一行到最后一行，将所有的src替换为dest，只是在替换时会有确认； :.,$s/src/dest/g: 查找范围为当前行到最后一行，将所有的src替换为dest；(因为在命令模式下，.表示当前行) 2.3 删除、复制与粘贴x, X: x表示删除光标后的一个字符，X表示删除光标前的一个字符，前面可以加上数字n，表示删除n个字符， 即nx, nX； dd: 表示删除光标所在的行，前面带上数字n，表示删除当前行往下的n行（包括当前行）； d$: 表示删除光标所在的位置到该行的最后一个字符； d0: 表示删除光标所在的位置到该行的第一个字符处； d^: 表示删除光标所在的位置到该行的第一个非空字符处； D: 删除光标所在字符到该行最后一个字符，等价于d$； dt+字符c: dt后接一个字符c，表示从光标所在字符开始，删除该行的字符知道遇到第一个为c的字符为止，如果在该行，光标所在字符后不存在字符c，则什么也不做； dG: 表示删除当前行到最后一行； dgg: 表示删除当前行与第一行； dnG: 表示删除当前行与第n行； yy: 复制光标所在那一行的内容； yG: 复制光标所在行与最后一行之间的内容； ygg: 复制光标所在行与第一行之间的内容； ynG: 复制光标所在行与第n行之间的内容； y$: 复制光标所在的字符到该行的最后一个字符； y0: 复制光标所在的字符到该行的第一个字符； y^: 复制光标所在的字符到该行的第一个非空字符； p, P: p表示将复制的数据粘贴在光标所在行的下一行；P表示将复制的数据粘贴在光标所在行的上一行； 说明：如果复制的内容是以行为单位，则p和P就粘贴在当前行的下一行或上一行，如果复制的内容是以字符为单位，即针对y$, y0或者y^，p和P会粘贴在光标的后面。 u: 复原上一个操作，类似于word中的还原； CTRL+r: 重做上一个操作； .: 重复前一个动作；相当于重新执行一遍前一个操作，比如前一个操作时dd，则.命令会再执行一遍dd； r,R: r表示替换光标所在字符，R表示替换光标所在字符及其后的字符，直到按ESC为止； 2.4 块选择v: 字符选择，从光标所在字符开始，配合h, j, k, l进行字符的选择； V: 行选择，将光标经过的行选中； CTRL+v: 块选择，从光标所在字符开始，选中块； y: 复制选中的内容； p: 粘贴选中的内容； 3. 编辑模式下的常用操作在编辑模式下，就是对文件的内容进行增删改，没有特殊的操作，光标的移动还是得进入到一般模式； 从一般模式进入编辑模式的命令有： i, I: i表示在光标所在字符之前插入，I表示在光标所在行的第一个非空字符前插入； a, A: a表示在光标所在字符之后插入，A表示在光标所在行的最后一个非空字符后插入； o, O: o表示在光标所在行的下一行插入，O表示在光标所在行的上一行插入； s, S: s表示删除光标所在字符并进入插入模式，S表示删除光标所在行并进入插入模式； 说明：s和S使用地稍微较少一些； CTR+n或者CTRL+p: 在编辑模式下，输入一些文字后，按CTRL+n或者CTRL+p可以出现提示文字； 4. 命令行模式下的常用操作4.1 保存退出:w 保存文件内容，但不退出； :q 退出vim；如果文件内容没有被修改，则直接退出，如果文件内容修改了但没有保存，则会提示先保存后退出； :q! 强制退出vim；如果修改过文件但是没有保存，则不保存且强制退出； :e! 将文件恢复到最原始的状态，即上次保存的状态； :wq 保存后退出； :x 保存后退出； 说明：其实最常用的命令当属:w, :q, :x，如果是有意的修改，则应该随时注意保存，即:w，离开时尽量使用:q，如果是有意修改，应该已经保存了，如果作了无意修改，则会提示，比较安全，只有在修改完之后，确定保存并离开，可以使用:x，不过还是尽量少用； 4.2 文件读写:w filename 将文件的内容另存为另一个文件 :n1,n2 w filename 将文件的n1和n2行之间的内容另存为另一个文件； :w&gt;&gt;filename 将文件内容追加到另一个文件中； :r filename 将另一个文件的内容读入到光标所在的下一行； :f 显示当前文件的文件名 4.3 其它常用:set nu 显示行号 :set nonu 隐藏行号 :set hlsearch 查找时高亮显示匹配项 :set nohlsearch 查找时不高亮显示匹配项 说明：以上命令一般写入vim的配置文件里，这样就不需要每次都调这些命令了。 5. 多文件及分屏编辑:r filename命令可以读入整个文件的内容，但如果指向复制或者重用另一个文件的部分内容，怎么办呢？可以使用多文件编辑或者分屏编辑； 多文件：vim后接多个文件名，则可以在一个vim里打开多个文件，可以进行文件间的编辑； :n 显示下一个文件； :N 显示上一个文件； :f 显示当前文件的文件名； :files 显示所有的文件名； 说明：这样，可以从一个文件里复制部分内容，然后切换到另一个文件，将文件内容粘贴，实现多文件的编辑； 分屏：在一个屏幕上同时显示多个文件的内容，可以对照着编辑，很方便； 分屏显示有两种方式： [1] 先使用vim打开一个文件，然后在命令行模式下可输入的命令有： :sp filename 读入另一个文件，两个文件分水平的上下两屏来显示；如果不跟文件名，则新屏里也显示当前文件的内容； :vsp filename 或者 :vsplit filename 读入另一个文件，两个文件分垂直的左右两屏来显示；如果不跟文件名，则新屏里也显示当前文件的内容； 分屏之后的常用命令有： CTRL+w,[h,j,k,l]: 如果是横屏，则CTRL+w,j表示光标移入下方的屏，CTRL+w,k表示光标移入上方的屏；CTRL+w,h表示光标移入左侧的屏，CTRL+w,l表示光标移入右侧的屏； CTRL+w,w: 表示在两个屏之间来回切换； 说明：CTRL+w,h的按键方式为：同时按下CTRL和w，放开或不放开都行，然后按下h； CTRL+w,=: 表示将两个屏幕的宽度调整为相等； [2] vim启动的时候同时打开多个文件： $ vim -On file01 file02 分垂直两屏显示两个文件； $ vim -on file01 file02 分水平两屏显示两个文件； 说明：以上这些命令都是基础且常用的，不要死记硬背，应该在vim下多练习，直到输入这些命令成为一种无意识的行为；vim的功能很强大，而且支持丰富的插件，比如ctags等，有兴趣的可以找相关的文章参考；","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://nkcoder.github.io/tags/linux/"},{"name":"vim","slug":"vim","permalink":"http://nkcoder.github.io/tags/vim/"}]},{"title":"String.java源码学习","slug":"java-string-source","date":"2014-01-03T16:27:12.000Z","updated":"2017-09-17T03:02:20.000Z","comments":true,"path":"2014/01/04/java-string-source/","link":"","permalink":"http://nkcoder.github.io/2014/01/04/java-string-source/","excerpt":"1. 存储结构private final char value[]; 备注：String的底层是通过字符数组实现的，而且是常量，所以我们不能修改String类型的变量的值。","text":"1. 存储结构private final char value[]; 备注：String的底层是通过字符数组实现的，而且是常量，所以我们不能修改String类型的变量的值。 2. 构造函数 String的构造函数有很多：不带参数，参数可以为String, char[], byte[], StringBuffer, StringBuilder等。 2.1 第一类public String() { this.value = new char[0]; } public String(String original) { this.value = original.value; this.hash = original.hash; } 备注：因为String是常量，不能被修改，因此这两个构造函数，如无必要一般不用。 2.2 第二类public String(char value[]) { this.value = Arrays.copyOf(value, value.length); } public String(StringBuffer buffer) { synchronized(buffer) { this.value = Arrays.copyOf(buffer.getValue(), buffer.length()); } } public String(StringBuilder builder) { this.value = Arrays.copyOf(builder.getValue(), builder.builder.length()); } 备注：char[]，StringBuffer，StringBuilder的内在存储结构都是char[]，所以这几个构造函数都是通过Arrays的copyOf函数执行复制操作，Arrays.copyOf函数是通过System.arraycopy实现的，这个函数在看到Arrays的源码时再解释。 2.3 第三类public String(byte bytes[]) { this(bytes, 0, bytes.length); } public String(byte bytes[], int offset, int length) { checkBounds(bytes, offset, length); this.value = StringCoding.decode(bytes, offset, length); } public String(byte bytes[], int offset, int length, Charset charset) { if (charset == null) throw new NullPointerException(&quot;charset&quot;); checkBounds(bytes, offset, length); this.value = StringCoding.decode(charset, bytes, offset, length); } 备注：使用byte[]时，需要解码，如果没有指定字符编码，会使用系统默认的编码。 3. 常用函数3.1 第一类public int length() { return value.length; } public boolean isEmpty() { return value.length == 0; } public char charAt(int index) { if ((index &lt; 0) || (index &gt;= value.length)) { throw new StringIndexOutOfBoundsException(index); } return value[index]; } 备注：length在String，StringBuilder，StringBuffer里面都是函数length()，在char[]是成员变量。 3.2 第二类public void getChars(int srcBegin, int srcEnd, char dst[], int dstBegin) { if (srcBegin &lt; 0) { throw new StringIndexOutOfBoundsException(srcBegin); } if (srcEnd &gt; value.length) { throw new StringIndexOutOfBoundsException(srcEnd); } if (srcBegin &gt; srcEnd) { throw new StringIndexOutOfBoundsException(srcEnd - srcBegin); } System.arraycopy(value, 0, dst, dstBegin, value.length); } public byte[] getBytes(){ return StringCoding.encode(value, 0, value.length); } 备注：getChars就是在字符级别进行复制，getBytes需要指定字符集解码。 3.3 第三类public boolean equals(Object anObject) { if (this == anObject) { return true; } if (anObject instanceOf String) { String anotherString = (String) anObject; int n = value.length; if (n == anotherString.value.length) { char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- &gt; 0) {} if (v1[i] != v2[i]) return false; i++; } return true; } } return false; } public boolean equalsIgnoreCase(String anotherString) { return (this == anotherString) ? true : (anotherString != null) &amp;&amp; (anotherString.value.length == value.length) &amp;&amp; regionMatches(true, 0, anotherString, 0, value.length); } 备注：字符串的比较，都是针对String类型的，一个是大小写敏感，一个忽略大小写，内部实现差不多的。 3.4 第四类public int compareToIgnoreCase(String str) { return CASE_INSENSITIVE_ORDER.compare(this, str); } 备注：CASE_INSENSITIVE_ORDER是内部静态类的一个实例，所以直接调用的效果是一样的：String.CASE_INSENSITIVE_ORDER.compare(str1, str2);","categories":[{"name":"Backend","slug":"Backend","permalink":"http://nkcoder.github.io/categories/Backend/"}],"tags":[{"name":"java","slug":"java","permalink":"http://nkcoder.github.io/tags/java/"}]}]}